{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|(?!\\b)(?=[A-Z][A-Z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"NetApp ActiveIQ API Documentation","text":"<p>Welcome to NetApp ActiveIQ API Documentation</p> <p>Welcome to the comprehensive documentation for the NetApp ActiveIQ Unified Manager REST API. This documentation is based on analysis of external data sources and provides practical guidance for working with the API.</p> \ud83d\ude80 Quick Start Guide <p>Get started with NetApp ActiveIQ API in minutes</p>","tags":["NetApp","ActiveIQ","API","REST","Documentation"]},{"location":"#overview","title":"Overview","text":"<p>NetApp ActiveIQ provides a robust REST API suite through Active IQ Unified Manager and Digital Advisor to programmatically manage, monitor, and extract data from NetApp storage environments.</p>","tags":["NetApp","ActiveIQ","API","REST","Documentation"]},{"location":"#key-features","title":"Key Features","text":"<ul> <li>100+ API endpoints across 20+ service categories</li> <li>RESTful design with full CRUD operations</li> <li>JSON-based request/response format</li> <li>HTTP Basic Authentication</li> <li>Comprehensive coverage of storage operations</li> </ul>","tags":["NetApp","ActiveIQ","API","REST","Documentation"]},{"location":"#documentation-structure","title":"Documentation Structure","text":"","tags":["NetApp","ActiveIQ","API","REST","Documentation"]},{"location":"#api-overview","title":"\ud83d\udcd6 API Overview","text":"<p>General introduction to the NetApp ActiveIQ API, including: - API capabilities and features - Authentication and access requirements - Getting started guide - Common use cases</p>","tags":["NetApp","ActiveIQ","API","REST","Documentation"]},{"location":"#api-endpoints-reference","title":"\ud83d\udd17 API Endpoints Reference","text":"<p>Complete reference of available API endpoints: - Administration endpoints - Datacenter management - Storage provider operations - Management server functions - Gateway APIs</p>","tags":["NetApp","ActiveIQ","API","REST","Documentation"]},{"location":"#data-models","title":"\ud83d\udcca Data Models","text":"<p>Detailed documentation of API data structures: - Core data models - Performance metrics - Event management objects - Storage and cluster information - Request/response formats</p>","tags":["NetApp","ActiveIQ","API","REST","Documentation"]},{"location":"#examples-and-use-cases","title":"\ud83d\udcbb Examples and Use Cases","text":"<p>Practical examples and automation scripts: - Infrastructure discovery - Storage monitoring - Performance tracking - Event management - Backup automation - Health check scripts</p>","tags":["NetApp","ActiveIQ","API","REST","Documentation"]},{"location":"#advanced-use-cases","title":"\ud83d\ude80 Advanced Use Cases","text":"<p>Advanced workflows with sequence diagrams: - SVM creation automation - NFS share management - Credential updates and security - Complete environment deployment - Sequence diagrams for complex workflows - Production-ready automation scripts</p>","tags":["NetApp","ActiveIQ","API","REST","Documentation"]},{"location":"#quick-start","title":"Quick Start","text":"","tags":["NetApp","ActiveIQ","API","REST","Documentation"]},{"location":"#1-access-requirements","title":"1. Access Requirements","text":"<p>To use the ActiveIQ API, you need: - Active IQ Unified Manager instance - User account with appropriate role:   - Operator   - Storage Administrator   - Application Administrator</p>","tags":["NetApp","ActiveIQ","API","REST","Documentation"]},{"location":"#2-base-url","title":"2. Base URL","text":"<pre><code>https://&lt;your-unified-manager-host&gt;/api/v2\n</code></pre>","tags":["NetApp","ActiveIQ","API","REST","Documentation"]},{"location":"#3-authentication","title":"3. Authentication","text":"<p>All API calls use HTTP Basic Authentication: <pre><code>curl -u \"username:password\" -X GET \"https://&lt;host&gt;/api/v2/datacenter/cluster/clusters\"\n</code></pre></p>","tags":["NetApp","ActiveIQ","API","REST","Documentation"]},{"location":"#4-interactive-documentation","title":"4. Interactive Documentation","text":"<p>Access the Swagger UI for live API testing: <pre><code>https://&lt;your-unified-manager-host&gt;/apidocs/\n</code></pre></p>","tags":["NetApp","ActiveIQ","API","REST","Documentation"]},{"location":"#common-operations","title":"Common Operations","text":"","tags":["NetApp","ActiveIQ","API","REST","Documentation"]},{"location":"#get-cluster-information","title":"Get Cluster Information","text":"<pre><code>GET /api/v2/datacenter/cluster/clusters\n</code></pre>","tags":["NetApp","ActiveIQ","API","REST","Documentation"]},{"location":"#monitor-storage-capacity","title":"Monitor Storage Capacity","text":"<pre><code>GET /api/v2/datacenter/storage/volumes?fields=name,size,svm,cluster\n</code></pre>","tags":["NetApp","ActiveIQ","API","REST","Documentation"]},{"location":"#check-critical-events","title":"Check Critical Events","text":"<pre><code>GET /api/v2/management-server/events?query=severity:critical\n</code></pre>","tags":["NetApp","ActiveIQ","API","REST","Documentation"]},{"location":"#create-backup","title":"Create Backup","text":"<pre><code>POST /api/v2/admin/backup\n</code></pre>","tags":["NetApp","ActiveIQ","API","REST","Documentation"]},{"location":"#data-sources","title":"Data Sources","text":"<p>This documentation is derived from analysis of: - NetApp ActiveIQ API research documents - OpenAPI specification (16,674 lines) - Official NetApp documentation references - Practical implementation examples</p>","tags":["NetApp","ActiveIQ","API","REST","Documentation"]},{"location":"#external-resources","title":"External Resources","text":"<ul> <li>Official NetApp ActiveIQ Documentation</li> <li>Active IQ Unified Manager API Guide</li> <li>Digital Advisor API Services</li> </ul> <p>Last updated: Based on analysis of external data sources in donnees-externes/ folder</p>","tags":["NetApp","ActiveIQ","API","REST","Documentation"]},{"location":"DEVELOPMENT/","title":"NetApp ActiveIQ API Documentation - Development Guide","text":"<p>This guide provides instructions for developing and maintaining the NetApp ActiveIQ API documentation using MkDocs Material.</p>"},{"location":"DEVELOPMENT/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"DEVELOPMENT/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+ (recommended: Python 3.12)</li> <li>pip (Python package installer)</li> <li>Git</li> </ul>"},{"location":"DEVELOPMENT/#local-development-setup","title":"Local Development Setup","text":"<ol> <li> <p>Clone the repository: <pre><code>git clone &lt;repository-url&gt;\ncd netapp-activeiq-docs\n</code></pre></p> </li> <li> <p>Create virtual environment: <pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n</code></pre></p> </li> <li> <p>Install dependencies: <pre><code>pip install -r requirements.txt\n</code></pre></p> </li> <li> <p>Start development server: <pre><code>mkdocs serve\n</code></pre></p> </li> <li> <p>Open in browser:    Navigate to <code>http://localhost:8000</code></p> </li> </ol>"},{"location":"DEVELOPMENT/#project-structure","title":"\ud83d\udcc1 Project Structure","text":"<pre><code>\u251c\u2500\u2500 .github/\n\u2502   \u2514\u2500\u2500 workflows/\n\u2502       \u2514\u2500\u2500 deploy-docs.yml      # GitHub Actions workflow\n\u251c\u2500\u2500 docs/\n\u2502   \u251c\u2500\u2500 stylesheets/\n\u2502   \u2502   \u251c\u2500\u2500 extra.css           # Custom CSS styling\n\u2502   \u2502   \u2514\u2500\u2500 termynal.css        # Terminal animation styles\n\u2502   \u251c\u2500\u2500 javascripts/\n\u2502   \u2502   \u251c\u2500\u2500 extra.js            # Enhanced JavaScript features\n\u2502   \u2502   \u2514\u2500\u2500 termynal.js         # Terminal animation script\n\u2502   \u251c\u2500\u2500 index.md                # Homepage\n\u2502   \u251c\u2500\u2500 api-endpoints.md        # API endpoints reference\n\u2502   \u251c\u2500\u2500 data-models.md          # Data models documentation\n\u2502   \u251c\u2500\u2500 examples.md             # Basic examples and use cases\n\u2502   \u251c\u2500\u2500 advanced-use-cases.md   # Advanced workflows with diagrams\n\u2502   \u251c\u2500\u2500 navigation.md           # Navigation guide\n\u2502   \u2514\u2500\u2500 README.md               # Documentation summary\n\u251c\u2500\u2500 donnees-externes/           # Source data (analyzed)\n\u251c\u2500\u2500 mkdocs.yml                  # MkDocs configuration\n\u251c\u2500\u2500 requirements.txt            # Python dependencies\n\u2514\u2500\u2500 DEVELOPMENT.md              # This file\n</code></pre>"},{"location":"DEVELOPMENT/#development-workflow","title":"\ud83d\udee0\ufe0f Development Workflow","text":""},{"location":"DEVELOPMENT/#making-changes","title":"Making Changes","text":"<ol> <li>Edit documentation files:</li> <li>Markdown files are in the <code>docs/</code> directory</li> <li> <p>Follow the existing structure and formatting</p> </li> <li> <p>Preview changes locally: <pre><code>mkdocs serve --livereload\n</code></pre>    Changes will auto-reload in your browser</p> </li> <li> <p>Test the build: <pre><code>mkdocs build --clean --strict\n</code></pre></p> </li> <li> <p>Commit and push: <pre><code>git add .\ngit commit -m \"Update documentation\"\ngit push\n</code></pre></p> </li> </ol>"},{"location":"DEVELOPMENT/#adding-new-content","title":"Adding New Content","text":""},{"location":"DEVELOPMENT/#new-documentation-page","title":"New Documentation Page","text":"<ol> <li>Create a new <code>.md</code> file in the <code>docs/</code> directory</li> <li>Add front matter with tags:    <pre><code>---\ntags:\n  - API\n  - NetApp\n  - Examples\n---\n</code></pre></li> <li>Update <code>mkdocs.yml</code> navigation section</li> <li>Add links in relevant existing pages</li> </ol>"},{"location":"DEVELOPMENT/#new-advanced-use-case","title":"New Advanced Use Case","text":"<ol> <li>Add content to <code>docs/advanced-use-cases.md</code></li> <li>Include sequence diagrams using Mermaid syntax:    <pre><code>sequenceDiagram\n    participant Client\n    participant API\n    Client-&gt;&gt;API: Request\n    API--&gt;&gt;Client: Response</code></pre></li> <li>Add Python code examples with proper syntax highlighting</li> </ol>"},{"location":"DEVELOPMENT/#new-api-endpoint-documentation","title":"New API Endpoint Documentation","text":"<ol> <li>Update <code>docs/api-endpoints.md</code></li> <li>Use the consistent format:    <pre><code>### Endpoint Name\n- **Method**: `GET|POST|PATCH|DELETE`\n- **Path**: `/api/v2/path/to/endpoint`\n- **Description**: Brief description\n</code></pre></li> </ol>"},{"location":"DEVELOPMENT/#styling-and-customization","title":"\ud83c\udfa8 Styling and Customization","text":""},{"location":"DEVELOPMENT/#custom-css","title":"Custom CSS","text":"<p>Edit <code>docs/stylesheets/extra.css</code> to customize: - NetApp brand colors - Component styling - Responsive design - Dark/light theme variations</p>"},{"location":"DEVELOPMENT/#enhanced-javascript","title":"Enhanced JavaScript","text":"<p>Edit <code>docs/javascripts/extra.js</code> to add: - Interactive features - API call demonstrations - Enhanced search functionality - Progress indicators</p>"},{"location":"DEVELOPMENT/#mermaid-diagrams","title":"Mermaid Diagrams","text":"<p>Use Mermaid syntax for sequence diagrams: <pre><code>sequenceDiagram\n    participant User\n    participant API\n    participant Storage\n\n    User-&gt;&gt;API: Create SVM Request\n    API-&gt;&gt;Storage: Initialize SVM\n    Storage--&gt;&gt;API: SVM Created\n    API--&gt;&gt;User: Success Response</code></pre></p>"},{"location":"DEVELOPMENT/#configuration","title":"\ud83d\udd27 Configuration","text":""},{"location":"DEVELOPMENT/#mkdocs-configuration-mkdocsyml","title":"MkDocs Configuration (<code>mkdocs.yml</code>)","text":"<p>Key sections: - nav: Site navigation structure - theme: Material theme configuration - plugins: Enabled plugins and settings - markdown_extensions: Enhanced Markdown features</p>"},{"location":"DEVELOPMENT/#theme-features","title":"Theme Features","text":"<p>Current enabled features: - Navigation tabs and sections - Code copy buttons - Search with highlighting - Dark/light mode toggle - Instant navigation - Content tabs - Mermaid diagram support</p>"},{"location":"DEVELOPMENT/#plugins-and-extensions","title":"\ud83d\udce6 Plugins and Extensions","text":""},{"location":"DEVELOPMENT/#essential-plugins","title":"Essential Plugins","text":"<ul> <li>mkdocs-material: Material Design theme</li> <li>mkdocs-mermaid2-plugin: Sequence diagrams</li> <li>mkdocs-git-revision-date-localized-plugin: Last updated dates</li> <li>mkdocs-awesome-pages-plugin: Advanced navigation</li> <li>mkdocs-macros-plugin: Variables and macros</li> </ul>"},{"location":"DEVELOPMENT/#markdown-extensions","title":"Markdown Extensions","text":"<ul> <li>pymdownx.superfences: Enhanced code blocks</li> <li>pymdownx.tabbed: Content tabs</li> <li>pymdownx.details: Collapsible sections</li> <li>admonition: Info/warning/tip boxes</li> <li>toc: Table of contents generation</li> </ul>"},{"location":"DEVELOPMENT/#deployment","title":"\ud83d\ude80 Deployment","text":""},{"location":"DEVELOPMENT/#github-pages-automatic","title":"GitHub Pages (Automatic)","text":"<p>Documentation automatically deploys via GitHub Actions when: - Changes are pushed to <code>main</code> or <code>master</code> branch - Files in <code>docs/</code>, <code>mkdocs.yml</code>, or <code>requirements.txt</code> are modified</p>"},{"location":"DEVELOPMENT/#manual-deployment","title":"Manual Deployment","text":"<pre><code># Build the site\nmkdocs build\n\n# Deploy to GitHub Pages\nmkdocs gh-deploy --force\n</code></pre>"},{"location":"DEVELOPMENT/#local-preview","title":"Local Preview","text":"<pre><code># Serve locally with live reload\nmkdocs serve\n\n# Serve on specific port\nmkdocs serve --dev-addr localhost:8080\n\n# Serve with strict mode (catch warnings)\nmkdocs serve --strict\n</code></pre>"},{"location":"DEVELOPMENT/#troubleshooting","title":"\ud83d\udd0d Troubleshooting","text":""},{"location":"DEVELOPMENT/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Plugin installation errors: <pre><code>pip install --upgrade pip\npip install -r requirements.txt --force-reinstall\n</code></pre></p> </li> <li> <p>Mermaid diagrams not rendering:</p> </li> <li>Check syntax with Mermaid Live Editor</li> <li> <p>Ensure proper indentation in YAML front matter</p> </li> <li> <p>Navigation not updating:</p> </li> <li>Check <code>mkdocs.yml</code> nav section syntax</li> <li> <p>Ensure file paths are correct and files exist</p> </li> <li> <p>CSS/JS changes not appearing:</p> </li> <li>Clear browser cache</li> <li>Restart development server</li> <li>Check console for JavaScript errors</li> </ol>"},{"location":"DEVELOPMENT/#debug-mode","title":"Debug Mode","text":"<p>Run with verbose output: <pre><code>mkdocs serve --verbose\nmkdocs build --verbose --clean --strict\n</code></pre></p>"},{"location":"DEVELOPMENT/#content-guidelines","title":"\ud83d\udccb Content Guidelines","text":""},{"location":"DEVELOPMENT/#writing-style","title":"Writing Style","text":"<ul> <li>Use clear, concise language</li> <li>Include code examples for all concepts</li> <li>Add practical, real-world examples</li> <li>Use consistent terminology throughout</li> </ul>"},{"location":"DEVELOPMENT/#code-examples","title":"Code Examples","text":"<ul> <li>Always include complete, working examples</li> <li>Use syntax highlighting with language specification</li> <li>Add comments to explain complex operations</li> <li>Include both cURL and Python examples where applicable</li> </ul>"},{"location":"DEVELOPMENT/#sequence-diagrams","title":"Sequence Diagrams","text":"<ul> <li>Keep diagrams simple and focused</li> <li>Use consistent participant naming</li> <li>Include error scenarios where relevant</li> <li>Add notes for complex interactions</li> </ul>"},{"location":"DEVELOPMENT/#contributing","title":"\ud83e\udd1d Contributing","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch: <code>git checkout -b feature/new-content</code></li> <li>Make your changes following these guidelines</li> <li>Test locally with <code>mkdocs serve</code></li> <li>Submit a pull request with description of changes</li> </ol>"},{"location":"DEVELOPMENT/#support","title":"\ud83d\udcde Support","text":"<p>For questions or issues: - Check existing GitHub issues - Review this development guide - Test changes locally before submitting - Include error messages in issue reports</p> <p>This development guide ensures consistent, high-quality documentation for the NetApp ActiveIQ API.</p>"},{"location":"MCP_SERVER_README/","title":"NetApp ActiveIQ Unified Manager MCP Server","text":"<p>This Model Context Protocol (MCP) server provides tools to interact with NetApp ActiveIQ Unified Manager REST API, enabling AI assistants to query and analyze NetApp storage infrastructure.</p>"},{"location":"MCP_SERVER_README/#features","title":"Features","text":"<p>The MCP server provides comprehensive access to NetApp ActiveIQ Unified Manager including:</p>"},{"location":"MCP_SERVER_README/#core-infrastructure","title":"Core Infrastructure","text":"<ul> <li>Clusters: Query cluster information, configuration, and health status</li> <li>Nodes: Retrieve node details, performance metrics, and hardware information</li> <li>Storage Virtual Machines (SVMs): Access SVM configuration and status</li> <li>Volumes: Get volume information, space usage, and performance analytics</li> <li>Aggregates: Monitor aggregate health and capacity</li> </ul>"},{"location":"MCP_SERVER_README/#performance-monitoring","title":"Performance Monitoring","text":"<ul> <li>Cluster Performance: Real-time and historical performance metrics</li> <li>Volume Analytics: IOPS, latency, and throughput analysis</li> <li>Node Analytics: Performance capacity and utilization tracking</li> <li>Workload Monitoring: Application workload performance and conformance</li> </ul>"},{"location":"MCP_SERVER_README/#storage-management","title":"Storage Management","text":"<ul> <li>Performance Service Levels: Query and manage PSL policies</li> <li>Storage Efficiency Policies: Monitor compression and deduplication settings</li> <li>QoS Policies: Access Quality of Service configurations</li> <li>Workloads: Track storage workload performance and recommendations</li> </ul>"},{"location":"MCP_SERVER_README/#operations","title":"Operations","text":"<ul> <li>Events: Query system events and alerts</li> <li>Jobs: Monitor background job status</li> <li>System Information: Get system health and version details</li> </ul>"},{"location":"MCP_SERVER_README/#installation","title":"Installation","text":"<ol> <li> <p>Install Dependencies:    <pre><code>pip install -r mcp_requirements.txt\n</code></pre></p> </li> <li> <p>Configure Environment (optional):    <pre><code>export NETAPP_BASE_URL=\"https://your-netapp-aiqum.example.com/api\"\nexport NETAPP_USERNAME=\"your-username\"\nexport NETAPP_PASSWORD=\"your-password\"\n</code></pre></p> </li> <li> <p>Add to MCP Client Configuration:    Update your MCP client configuration (e.g., Claude Desktop config):    <pre><code>{\n  \"mcpServers\": {\n    \"netapp-activeiq\": {\n      \"command\": \"python\",\n      \"args\": [\"/path/to/mcp_server.py\"],\n      \"env\": {\n        \"NETAPP_BASE_URL\": \"https://your-netapp-aiqum.example.com/api\",\n        \"NETAPP_USERNAME\": \"your-username\",\n        \"NETAPP_PASSWORD\": \"your-password\"\n      }\n    }\n  }\n}\n</code></pre></p> </li> </ol>"},{"location":"MCP_SERVER_README/#usage","title":"Usage","text":""},{"location":"MCP_SERVER_README/#initial-setup","title":"Initial Setup","text":"<p>First, configure the connection to your NetApp ActiveIQ Unified Manager:</p> <pre><code>configure_netapp_connection(\n    base_url=\"https://your-netapp-aiqum.example.com/api\",\n    username=\"your-username\",\n    password=\"your-password\",\n    verify_ssl=True,\n    timeout=30\n)\n</code></pre>"},{"location":"MCP_SERVER_README/#common-use-cases","title":"Common Use Cases","text":""},{"location":"MCP_SERVER_README/#1-infrastructure-overview","title":"1. Infrastructure Overview","text":"<pre><code># Get all clusters\nget_clusters()\n\n# Get cluster details\nget_cluster_details(\"cluster-key\")\n\n# Get cluster performance\nget_cluster_performance(\"cluster-key\", interval=\"1d\")\n</code></pre>"},{"location":"MCP_SERVER_README/#2-storage-analysis","title":"2. Storage Analysis","text":"<pre><code># Get all volumes\nget_volumes()\n\n# Get volumes for specific SVM\nget_volumes(svm_name=\"svm1\")\n\n# Get volume performance analytics\nget_volume_analytics(cluster_name=\"cluster1\", order_by=\"iops desc\")\n</code></pre>"},{"location":"MCP_SERVER_README/#3-performance-monitoring","title":"3. Performance Monitoring","text":"<pre><code># Get nodes with performance metrics\nget_nodes(cluster_name=\"cluster1\")\n\n# Get workload performance\nget_workloads(conformance_status=\"non_conforming\")\n\n# Get performance service levels\nget_performance_service_levels()\n</code></pre>"},{"location":"MCP_SERVER_README/#4-event-and-alert-monitoring","title":"4. Event and Alert Monitoring","text":"<pre><code># Get critical events\nget_events(severity=\"critical\", state=\"new\")\n\n# Get system jobs\nget_jobs(state=\"running\")\n\n# Get system information\nget_system_info()\n</code></pre>"},{"location":"MCP_SERVER_README/#available-tools","title":"Available Tools","text":""},{"location":"MCP_SERVER_README/#connection-management","title":"Connection Management","text":"<ul> <li><code>configure_netapp_connection()</code> - Configure API connection</li> </ul>"},{"location":"MCP_SERVER_README/#infrastructure","title":"Infrastructure","text":"<ul> <li><code>get_clusters()</code> - List all clusters</li> <li><code>get_cluster_details()</code> - Get specific cluster details</li> <li><code>get_cluster_performance()</code> - Get cluster performance metrics</li> <li><code>get_nodes()</code> - List cluster nodes</li> <li><code>get_svms()</code> - List Storage Virtual Machines</li> <li><code>get_aggregates()</code> - List aggregates</li> </ul>"},{"location":"MCP_SERVER_README/#storage","title":"Storage","text":"<ul> <li><code>get_volumes()</code> - List volumes</li> <li><code>get_volume_analytics()</code> - Get volume performance analytics</li> </ul>"},{"location":"MCP_SERVER_README/#performance-workloads","title":"Performance &amp; Workloads","text":"<ul> <li><code>get_performance_service_levels()</code> - List Performance Service Levels</li> <li><code>get_storage_efficiency_policies()</code> - List Storage Efficiency Policies</li> <li><code>get_workloads()</code> - List storage workloads</li> </ul>"},{"location":"MCP_SERVER_README/#operations_1","title":"Operations","text":"<ul> <li><code>get_events()</code> - List system events</li> <li><code>get_jobs()</code> - List system jobs</li> <li><code>get_system_info()</code> - Get system information</li> </ul>"},{"location":"MCP_SERVER_README/#api-coverage","title":"API Coverage","text":"<p>This MCP server implements key endpoints from the NetApp ActiveIQ Unified Manager API v2:</p>"},{"location":"MCP_SERVER_README/#datacenter-apis","title":"Datacenter APIs","text":"<ul> <li><code>/datacenter/cluster/clusters</code> - Cluster management</li> <li><code>/datacenter/cluster/nodes</code> - Node management</li> <li><code>/datacenter/svm/svms</code> - SVM management</li> <li><code>/datacenter/storage/volumes</code> - Volume management</li> <li><code>/datacenter/storage/aggregates</code> - Aggregate management</li> </ul>"},{"location":"MCP_SERVER_README/#storage-provider-apis","title":"Storage Provider APIs","text":"<ul> <li><code>/storage-provider/performance-service-levels</code> - PSL management</li> <li><code>/storage-provider/storage-efficiency-policies</code> - SEP management</li> <li><code>/storage-provider/workloads</code> - Workload management</li> </ul>"},{"location":"MCP_SERVER_README/#management-server-apis","title":"Management Server APIs","text":"<ul> <li><code>/management-server/events</code> - Event management</li> <li><code>/management-server/jobs</code> - Job management</li> </ul>"},{"location":"MCP_SERVER_README/#admin-apis","title":"Admin APIs","text":"<ul> <li><code>/admin/system</code> - System information</li> </ul>"},{"location":"MCP_SERVER_README/#performance-metrics","title":"Performance Metrics","text":"<p>The server provides access to various performance metrics:</p>"},{"location":"MCP_SERVER_README/#time-intervals","title":"Time Intervals","text":"<ul> <li><code>1h</code> - Last hour (5-minute samples)</li> <li><code>12h</code> - Last 12 hours (5-minute samples)</li> <li><code>1d</code> - Last day (5-minute samples)</li> <li><code>2d</code> - Last 2 days (5-minute samples)</li> <li><code>3d</code> - Last 3 days (5-minute samples)</li> <li><code>15d</code> - Last 15 days (1-hour samples)</li> <li><code>1w</code> - Last week (1-hour samples)</li> <li><code>1m</code> - Last month (1-hour samples)</li> <li><code>2m</code> - Last 2 months (1-hour samples)</li> <li><code>3m</code> - Last 3 months (1-hour samples)</li> <li><code>6m</code> - Last 6 months (1-hour samples)</li> </ul>"},{"location":"MCP_SERVER_README/#metrics-types","title":"Metrics Types","text":"<ul> <li>IOPS: Input/Output Operations Per Second</li> <li>Latency: Response time in milliseconds</li> <li>Throughput: Data transfer rate</li> <li>Utilization: Resource usage percentage</li> <li>Performance Capacity: Available vs. used performance</li> </ul>"},{"location":"MCP_SERVER_README/#error-handling","title":"Error Handling","text":"<p>The server includes comprehensive error handling:</p> <ul> <li>Connection Errors: Clear messages for authentication and network issues</li> <li>API Errors: Detailed HTTP status and error descriptions</li> <li>Validation Errors: Parameter validation with helpful messages</li> <li>Timeout Handling: Configurable request timeouts</li> </ul>"},{"location":"MCP_SERVER_README/#security-considerations","title":"Security Considerations","text":"<ul> <li>Authentication: Uses HTTP Basic Authentication</li> <li>SSL/TLS: Supports SSL certificate verification (configurable)</li> <li>Credentials: Supports environment variables for sensitive data</li> <li>Timeouts: Configurable request timeouts to prevent hanging</li> </ul>"},{"location":"MCP_SERVER_README/#example-queries","title":"Example Queries","text":""},{"location":"MCP_SERVER_README/#find-storage-issues","title":"Find Storage Issues","text":"<pre><code># Get volumes with low space\nget_volumes(order_by=\"space.available asc\")\n\n# Get non-conforming workloads\nget_workloads(conformance_status=\"non_conforming\")\n\n# Get critical events\nget_events(severity=\"critical\", state=\"new\")\n</code></pre>"},{"location":"MCP_SERVER_README/#performance-analysis","title":"Performance Analysis","text":"<pre><code># Get top performing volumes\nget_volume_analytics(order_by=\"iops desc\", max_records=10)\n\n# Get cluster performance over last week\nget_cluster_performance(\"cluster-key\", interval=\"1w\")\n\n# Get node utilization\nget_nodes(order_by=\"performance_capacity.used desc\")\n</code></pre>"},{"location":"MCP_SERVER_README/#capacity-planning","title":"Capacity Planning","text":"<pre><code># Get aggregate space usage\nget_aggregates(order_by=\"space.used desc\")\n\n# Get volume space utilization\nget_volumes(order_by=\"space.used desc\")\n\n# Get workload space usage\nget_workloads(order_by=\"space.used desc\")\n</code></pre>"},{"location":"MCP_SERVER_README/#troubleshooting","title":"Troubleshooting","text":""},{"location":"MCP_SERVER_README/#common-issues","title":"Common Issues","text":"<ol> <li>Connection Failed</li> <li>Verify base_url is correct and includes <code>/api</code></li> <li>Check username/password credentials</li> <li> <p>Ensure network connectivity to ActiveIQ server</p> </li> <li> <p>SSL Certificate Errors</p> </li> <li>Set <code>verify_ssl=False</code> for self-signed certificates</li> <li> <p>Or add certificate to trusted store</p> </li> <li> <p>Timeout Errors</p> </li> <li>Increase timeout value</li> <li> <p>Check network latency to ActiveIQ server</p> </li> <li> <p>Permission Denied</p> </li> <li>Verify user has required roles in ActiveIQ</li> <li>Check API access permissions</li> </ol>"},{"location":"MCP_SERVER_README/#debug-mode","title":"Debug Mode","text":"<p>Enable debug logging: <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n</code></pre></p>"},{"location":"MCP_SERVER_README/#integration-with-temporal-workflows","title":"Integration with Temporal Workflows","text":"<p>This MCP server complements the existing Temporal workflows in your NetApp project:</p> <ul> <li>Data Source: Provides real-time data for workflow decisions</li> <li>Monitoring: Enables workflow health checks and performance validation</li> <li>Event Driven: Can trigger workflows based on ActiveIQ events</li> <li>Validation: Supports prerequisite validation for SVM/volume creation</li> </ul>"},{"location":"MCP_SERVER_README/#api-reference","title":"API Reference","text":"<p>For complete API documentation, refer to the NetApp ActiveIQ Unified Manager API Documentation at your ActiveIQ instance: <code>https://your-netapp-aiqum.example.com/docs/api</code></p>"},{"location":"MCP_SERVER_README/#license","title":"License","text":"<p>This MCP server is designed for use with NetApp ActiveIQ Unified Manager and follows NetApp's API terms of service.</p>"},{"location":"MKDOCS_VERSION_SUMMARY/","title":"NetApp ActiveIQ MCP Server Documentation - Version Summary","text":""},{"location":"MKDOCS_VERSION_SUMMARY/#last-updated-2025-06-28","title":"Last Updated: 2025-06-28","text":"<p>This document tracks the current versions of all components used in the documentation system.</p>"},{"location":"MKDOCS_VERSION_SUMMARY/#core-documentation-framework","title":"Core Documentation Framework","text":"Component Version Description MkDocs 1.6.1 Static site generator for project documentation MkDocs Material 9.6.14 Material Design theme for MkDocs Python 3.10+ (Recommended: 3.12) Runtime environment"},{"location":"MKDOCS_VERSION_SUMMARY/#diagram-visualization","title":"Diagram &amp; Visualization","text":"Component Version Description Mermaid.js 11.4.0 JavaScript diagramming library mkdocs-mermaid2-plugin 1.2.1 MkDocs plugin for Mermaid integration"},{"location":"MKDOCS_VERSION_SUMMARY/#essential-plugins","title":"Essential Plugins","text":"Plugin Version Purpose mkdocs-material 9.6.14 Material Design theme mkdocs-minify-plugin 0.8.0 HTML/CSS/JS minification mkdocs-git-revision-date-localized-plugin 1.4.7 Git-based page dates pymdown-extensions 10.16 Enhanced Markdown features"},{"location":"MKDOCS_VERSION_SUMMARY/#additional-features","title":"Additional Features","text":"Plugin Version Purpose mkdocs-redirects 1.2.2 URL redirect management mkdocs-awesome-pages-plugin 2.10.1 Advanced page organization mkdocs-include-markdown-plugin 7.1.6 Markdown file inclusion mkdocs-exclude-search 0.6.6 Search result filtering mkdocs-git-authors-plugin 0.10.0 Git-based author attribution mkdocs-macros-plugin 1.3.7 Template macros and variables mkdocs-with-pdf 0.9.3 PDF export capability mkdocs-static-i18n 1.3.0 Internationalization support mkdocs-swagger-ui-tag 0.7.1 OpenAPI/Swagger UI integration"},{"location":"MKDOCS_VERSION_SUMMARY/#key-features-enabled","title":"Key Features Enabled","text":""},{"location":"MKDOCS_VERSION_SUMMARY/#documentation-features","title":"\u2705 Documentation Features","text":"<ul> <li>Material Design: Modern, responsive UI with dark/light themes</li> <li>Advanced Search: Enhanced search with result filtering</li> <li>Git Integration: Automatic date tracking and author attribution</li> <li>PDF Export: Generate PDF versions of documentation</li> <li>Internationalization: Multi-language support framework</li> </ul>"},{"location":"MKDOCS_VERSION_SUMMARY/#diagram-capabilities","title":"\u2705 Diagram Capabilities","text":"<ul> <li>Mermaid v11.4.0: Latest diagramming with 42+ diagrams</li> <li>Theme Awareness: Automatic dark/light mode switching</li> <li>Responsive Design: Mobile-friendly diagram scaling</li> <li>Advanced Types: Flowcharts, sequences, Gantt, class diagrams</li> </ul>"},{"location":"MKDOCS_VERSION_SUMMARY/#api-documentation","title":"\u2705 API Documentation","text":"<ul> <li>OpenAPI Integration: Swagger UI embedded documentation</li> <li>Code Examples: Syntax-highlighted code blocks</li> <li>Interactive Demos: Curl command examples</li> <li>API Use Cases: Comprehensive NetApp API coverage</li> </ul>"},{"location":"MKDOCS_VERSION_SUMMARY/#devops-integration","title":"\u2705 DevOps Integration","text":"<ul> <li>Live Reload: Instant preview of changes</li> <li>Minification: Optimized builds for production</li> <li>Git Workflow: Revision tracking and collaboration</li> <li>Docker Ready: Containerized documentation builds</li> </ul>"},{"location":"MKDOCS_VERSION_SUMMARY/#upgrade-history","title":"Upgrade History","text":""},{"location":"MKDOCS_VERSION_SUMMARY/#2025-06-28","title":"2025-06-28","text":"<ul> <li>\u2705 Mermaid.js: Upgraded from v10.4.0 \u2192 v11.4.0</li> <li>\u2705 MkDocs: Confirmed latest stable v1.6.1</li> <li>\u2705 MkDocs Material: Confirmed latest stable v9.6.14</li> <li>\u2705 All Plugins: Updated to latest compatible versions</li> <li>\u2705 Fixed: Mermaid diagram syntax issues resolved</li> </ul>"},{"location":"MKDOCS_VERSION_SUMMARY/#quick-start-commands","title":"Quick Start Commands","text":"<pre><code># Install dependencies\npip install -r requirements-docs.txt\n\n# Start development server\n./start-docs.sh\n\n# Build static site\nvenv-docs/bin/mkdocs build\n\n# Deploy to GitHub Pages\nvenv-docs/bin/mkdocs gh-deploy\n</code></pre>"},{"location":"MKDOCS_VERSION_SUMMARY/#status-all-systems-operational","title":"Status: \u2705 All Systems Operational","text":"<p>The documentation system is running on the latest stable versions of all components with full feature compatibility and optimal performance.</p>"},{"location":"PYTHON_VERSION_UPDATE_SUMMARY/","title":"Python Version Update Summary","text":"<p>This document summarizes the updates made to align the entire NetApp ActiveIQ MCP Server project with Python 3.10+ requirements.</p>"},{"location":"PYTHON_VERSION_UPDATE_SUMMARY/#overview","title":"Overview","text":"<p>Previous Requirement: Python 3.8+ New Requirement: Python 3.10+ (Recommended: Python 3.12) Reason: Remove Node.js dependency and modernize to current Python standards</p>"},{"location":"PYTHON_VERSION_UPDATE_SUMMARY/#files-updated","title":"Files Updated","text":""},{"location":"PYTHON_VERSION_UPDATE_SUMMARY/#1-core-infrastructure","title":"1. Core Infrastructure","text":""},{"location":"PYTHON_VERSION_UPDATE_SUMMARY/#dockerfile","title":"Dockerfile","text":"<ul> <li><code>FROM python:3.11-slim</code> \u2192 <code>FROM python:3.10-slim</code> (both builder and production stages)</li> <li>Ensures consistent base image aligned with minimum requirement</li> </ul>"},{"location":"PYTHON_VERSION_UPDATE_SUMMARY/#mcp_requirementstxt","title":"mcp_requirements.txt","text":"<ul> <li>Added comment: <code># Requires Python 3.10+</code></li> <li>Updated header documentation</li> </ul>"},{"location":"PYTHON_VERSION_UPDATE_SUMMARY/#2-documentation-files","title":"2. Documentation Files","text":""},{"location":"PYTHON_VERSION_UPDATE_SUMMARY/#docsdevelopmentmd","title":"docs/DEVELOPMENT.md","text":"<ul> <li><code>Python 3.8+ (recommended: Python 3.11)</code> \u2192 <code>Python 3.10+ (recommended: Python 3.12)</code></li> </ul>"},{"location":"PYTHON_VERSION_UPDATE_SUMMARY/#docsgetting-startedquick-startmd","title":"docs/getting-started/quick-start.md","text":"<ul> <li><code>**Python 3.8+** (for development setup)</code> \u2192 <code>**Python 3.10+** (for development setup)</code></li> </ul>"},{"location":"PYTHON_VERSION_UPDATE_SUMMARY/#docsgetting-startedinstallationmd","title":"docs/getting-started/installation.md","text":"<ul> <li><code>**Docker 20.10+** or **Python 3.8+**</code> \u2192 <code>**Docker 20.10+** or **Python 3.10+**</code></li> </ul>"},{"location":"PYTHON_VERSION_UPDATE_SUMMARY/#docstemporal-integrationmd","title":"docs/temporal-integration.md","text":"<ul> <li><code>Python 3.8+ (recommended: Python 3.11)</code> \u2192 <code>Python 3.10+ (recommended: Python 3.12)</code></li> </ul>"},{"location":"PYTHON_VERSION_UPDATE_SUMMARY/#3-cicd-and-deployment","title":"3. CI/CD and Deployment","text":""},{"location":"PYTHON_VERSION_UPDATE_SUMMARY/#githubworkflowsdeploy-docsyml","title":".github/workflows/deploy-docs.yml","text":"<ul> <li><code>python-version: '3.11'</code> \u2192 <code>python-version: '3.10'</code> (both build and validate jobs)</li> <li>Ensures CI/CD uses the minimum supported version for testing</li> </ul>"},{"location":"PYTHON_VERSION_UPDATE_SUMMARY/#version_summarymd","title":"VERSION_SUMMARY.md","text":"<ul> <li><code>**Python** | 3.13 | Runtime environment</code> \u2192 <code>**Python** | 3.10+ (Recommended: 3.12) | Runtime environment</code></li> </ul>"},{"location":"PYTHON_VERSION_UPDATE_SUMMARY/#4-configuration-cleanup","title":"4. Configuration Cleanup","text":""},{"location":"PYTHON_VERSION_UPDATE_SUMMARY/#mkdocsyml","title":"mkdocs.yml","text":"<ul> <li>Removed custom JavaScript references that are no longer needed:</li> <li><code>javascripts/extra.js</code></li> <li><code>javascripts/termynal.js</code></li> <li><code>javascripts/mermaid-config.js</code></li> <li>Kept only essential Mermaid.js CDN reference</li> </ul>"},{"location":"PYTHON_VERSION_UPDATE_SUMMARY/#python-310-features-utilized","title":"Python 3.10+ Features Utilized","text":"<p>The codebase already uses modern Python features compatible with 3.10+:</p>"},{"location":"PYTHON_VERSION_UPDATE_SUMMARY/#type-hints","title":"Type Hints","text":"<pre><code>from typing import Dict, List, Optional, Any, Union\n</code></pre>"},{"location":"PYTHON_VERSION_UPDATE_SUMMARY/#asyncawait-patterns","title":"Async/Await Patterns","text":"<pre><code>async def _make_request(\n    self,\n    method: str,\n    endpoint: str,\n    params: Optional[Dict[str, Any]] = None\n) -&gt; Dict[str, Any]:\n</code></pre>"},{"location":"PYTHON_VERSION_UPDATE_SUMMARY/#dataclasses","title":"Dataclasses","text":"<pre><code>from dataclasses import dataclass\n\n@dataclass\nclass NetAppConfig:\n    base_url: str = Field(..., description=\"Base URL\")\n</code></pre>"},{"location":"PYTHON_VERSION_UPDATE_SUMMARY/#union-types-python-310","title":"Union Types (Python 3.10+)","text":"<p>The codebase is ready for <code>X | Y</code> syntax when fully adopting 3.10+ only.</p>"},{"location":"PYTHON_VERSION_UPDATE_SUMMARY/#deployment-impact","title":"Deployment Impact","text":""},{"location":"PYTHON_VERSION_UPDATE_SUMMARY/#docker","title":"Docker","text":"<ul> <li>Production containers now use Python 3.10 as base</li> <li>Smaller image size and better security posture</li> <li>Consistent with minimum requirement</li> </ul>"},{"location":"PYTHON_VERSION_UPDATE_SUMMARY/#kubernetesknative","title":"Kubernetes/Knative","text":"<ul> <li>All Helm charts and Knative functions inherit Python 3.10</li> <li>No additional changes needed</li> </ul>"},{"location":"PYTHON_VERSION_UPDATE_SUMMARY/#development","title":"Development","text":"<ul> <li>Virtual environments should use Python 3.10+</li> <li>CI/CD validates against minimum version</li> <li>Documentation build process uses Python 3.10</li> </ul>"},{"location":"PYTHON_VERSION_UPDATE_SUMMARY/#benefits-of-python-310","title":"Benefits of Python 3.10+","text":"<ol> <li>Performance: Improved performance over 3.8/3.9</li> <li>Pattern Matching: <code>match</code>/<code>case</code> statements available</li> <li>Union Types: <code>X | Y</code> syntax for type hints</li> <li>Better Error Messages: More descriptive error reporting</li> <li>Security: Latest security patches and improvements</li> <li>Libraries: Better compatibility with modern Python packages</li> </ol>"},{"location":"PYTHON_VERSION_UPDATE_SUMMARY/#compatibility-notes","title":"Compatibility Notes","text":""},{"location":"PYTHON_VERSION_UPDATE_SUMMARY/#removed-dependencies","title":"Removed Dependencies","text":"<ul> <li>No Node.js requirement</li> <li>No JavaScript build tools needed</li> <li>Simplified development environment</li> </ul>"},{"location":"PYTHON_VERSION_UPDATE_SUMMARY/#python-version-support","title":"Python Version Support","text":"<ul> <li>Minimum: Python 3.10.0</li> <li>Recommended: Python 3.12.x</li> <li>Maximum: Python 3.13.x (when available)</li> </ul>"},{"location":"PYTHON_VERSION_UPDATE_SUMMARY/#testing-requirements","title":"Testing Requirements","text":"<p>Ensure testing covers:</p> <ul> <li>Python 3.10.0 (minimum)</li> <li>Python 3.11.x (LTS)</li> <li>Python 3.12.x (recommended)</li> </ul>"},{"location":"PYTHON_VERSION_UPDATE_SUMMARY/#migration-guide-for-developers","title":"Migration Guide for Developers","text":""},{"location":"PYTHON_VERSION_UPDATE_SUMMARY/#for-local-development","title":"For Local Development","text":"<pre><code># Check current Python version\npython3 --version\n\n# Install Python 3.10+ if needed (example for Ubuntu)\nsudo apt update\nsudo apt install python3.10 python3.10-venv python3.10-dev\n\n# Recreate virtual environment\nrm -rf venv\npython3.10 -m venv venv\nsource venv/bin/activate\npip install -r mcp_requirements.txt\n</code></pre>"},{"location":"PYTHON_VERSION_UPDATE_SUMMARY/#for-production-deployment","title":"For Production Deployment","text":"<pre><code># Docker builds automatically use Python 3.10\ndocker build -t netapp-mcp-server .\n\n# Kubernetes deployments inherit from Docker image\nkubectl apply -f k8s/\n</code></pre>"},{"location":"PYTHON_VERSION_UPDATE_SUMMARY/#verification-checklist","title":"Verification Checklist","text":"<ul> <li> All documentation references updated</li> <li> Docker images build successfully</li> <li> CI/CD pipelines pass with Python 3.10</li> <li> Local development works with Python 3.10+</li> <li> No Node.js dependencies remain</li> <li> All deployment methods tested</li> </ul>"},{"location":"PYTHON_VERSION_UPDATE_SUMMARY/#status-complete","title":"Status: \u2705 Complete","text":"<p>All files have been successfully updated to require Python 3.10+ and remove Node.js dependencies. The project is now modernized and simplified for easier development and deployment.</p>"},{"location":"advanced-use-cases/","title":"NetApp ActiveIQ API - Advanced Use Cases","text":"<p>This document provides advanced use cases for NetApp ActiveIQ API, focusing on SVM management, NFS operations, and workflow automation with sequence diagrams.</p>"},{"location":"advanced-use-cases/#table-of-contents","title":"Table of Contents","text":"<ol> <li>SVM Creation Workflow</li> <li>NFS Share Management</li> <li>NFS Credential Updates</li> <li>Sequence Diagrams</li> <li>Complete Automation Scripts</li> </ol>"},{"location":"advanced-use-cases/#svm-creation-workflow","title":"SVM Creation Workflow","text":""},{"location":"advanced-use-cases/#overview","title":"Overview","text":"<p>Storage Virtual Machines (SVMs) provide secure, multi-tenant environments for data access. This workflow demonstrates creating SVMs with proper configuration.</p>"},{"location":"advanced-use-cases/#prerequisites","title":"Prerequisites","text":"<ul> <li>Cluster must be accessible and healthy</li> <li>Sufficient storage resources available</li> <li>Administrative privileges required</li> </ul>"},{"location":"advanced-use-cases/#step-by-step-svm-creation","title":"Step-by-Step SVM Creation","text":""},{"location":"advanced-use-cases/#1-validate-cluster-prerequisites","title":"1. Validate Cluster Prerequisites","text":"<pre><code># Check cluster health and capacity\ncurl -u \"admin:password\" -X GET \\\n  \"https://um-server.example.com/api/v2/datacenter/cluster/clusters/{cluster_key}\" \\\n  -H \"Accept: application/json\"\n</code></pre>"},{"location":"advanced-use-cases/#2-create-svm","title":"2. Create SVM","text":"<pre><code>curl -u \"admin:password\" -X POST \\\n  \"https://um-server.example.com/api/v2/datacenter/svm/svms\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"svm_nfs_prod\",\n    \"cluster\": {\n      \"key\": \"cluster-uuid-here\"\n    },\n    \"state\": \"running\",\n    \"subtype\": \"default\",\n    \"language\": \"c.utf_8\",\n    \"allowed_protocols\": [\"nfs\", \"cifs\"],\n    \"aggregates\": [\n      {\n        \"key\": \"aggregate-uuid-here\"\n      }\n    ],\n    \"dns\": {\n      \"domains\": [\"company.local\"],\n      \"servers\": [\"10.1.1.10\", \"10.1.1.11\"]\n    },\n    \"ldap\": {\n      \"enabled\": false\n    },\n    \"nis\": {\n      \"enabled\": false\n    }\n  }'\n</code></pre>"},{"location":"advanced-use-cases/#3-configure-network-interfaces","title":"3. Configure Network Interfaces","text":"<pre><code># Create management LIF\ncurl -u \"admin:password\" -X POST \\\n  \"https://um-server.example.com/api/v2/datacenter/svm/svms/{svm_key}/network/ip/interfaces\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"svm_nfs_prod_mgmt\",\n    \"ip\": {\n      \"address\": \"10.1.100.50\",\n      \"netmask\": \"255.255.255.0\"\n    },\n    \"location\": {\n      \"home_node\": {\n        \"uuid\": \"node-uuid-here\"\n      },\n      \"home_port\": {\n        \"name\": \"e0c\"\n      }\n    },\n    \"service_policy\": \"default-management\",\n    \"enabled\": true\n  }'\n\n# Create data LIF for NFS\ncurl -u \"admin:password\" -X POST \\\n  \"https://um-server.example.com/api/v2/datacenter/svm/svms/{svm_key}/network/ip/interfaces\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"svm_nfs_prod_data\",\n    \"ip\": {\n      \"address\": \"10.1.100.51\",\n      \"netmask\": \"255.255.255.0\"\n    },\n    \"location\": {\n      \"home_node\": {\n        \"uuid\": \"node-uuid-here\"\n      },\n      \"home_port\": {\n        \"name\": \"e0d\"\n      }\n    },\n    \"service_policy\": \"default-data-files\",\n    \"enabled\": true\n  }'\n</code></pre>"},{"location":"advanced-use-cases/#python-implementation","title":"Python Implementation","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nNetApp SVM Creation Automation\n\"\"\"\n\nimport requests\nfrom requests.auth import HTTPBasicAuth\nimport json\nimport time\n\nclass SVMManager:\n    def __init__(self, um_host, username, password):\n        self.um_host = um_host\n        self.auth = HTTPBasicAuth(username, password)\n        self.base_url = f\"https://{um_host}/api/v2\"\n\n    def validate_cluster(self, cluster_key):\n        \"\"\"Validate cluster is healthy and ready for SVM creation\"\"\"\n        url = f\"{self.base_url}/datacenter/cluster/clusters/{cluster_key}\"\n        params = {\"fields\": \"name,state,health,version\"}\n\n        response = requests.get(url, auth=self.auth, params=params)\n\n        if response.status_code == 200:\n            cluster = response.json()\n            if cluster.get('state') == 'up' and cluster.get('health', {}).get('overall_status') == 'healthy':\n                print(f\"\u2713 Cluster {cluster['name']} is healthy and ready\")\n                return True\n            else:\n                print(f\"\u2717 Cluster {cluster['name']} is not ready: {cluster.get('state')}\")\n                return False\n        else:\n            print(f\"\u2717 Failed to validate cluster: {response.status_code}\")\n            return False\n\n    def get_available_aggregates(self, cluster_key):\n        \"\"\"Get available aggregates for the cluster\"\"\"\n        url = f\"{self.base_url}/datacenter/storage/aggregates\"\n        params = {\n            \"query\": f\"cluster.key:{cluster_key}\",\n            \"fields\": \"name,key,space.size,space.available,state\"\n        }\n\n        response = requests.get(url, auth=self.auth, params=params)\n\n        if response.status_code == 200:\n            aggregates = response.json().get('records', [])\n            available_aggs = []\n\n            for agg in aggregates:\n                if (agg.get('state') == 'online' and\n                    agg.get('space', {}).get('available', 0) &gt; 10737418240):  # 10GB minimum\n                    available_aggs.append(agg)\n\n            return available_aggs\n        return []\n\n    def create_svm(self, svm_config):\n        \"\"\"Create SVM with specified configuration\"\"\"\n        url = f\"{self.base_url}/datacenter/svm/svms\"\n\n        response = requests.post(url, auth=self.auth, json=svm_config)\n\n        if response.status_code in [201, 202]:\n            result = response.json()\n            if 'job' in result:\n                job_uuid = result['job']['uuid']\n                print(f\"\u2713 SVM creation job started: {job_uuid}\")\n                return self.wait_for_job(job_uuid)\n            else:\n                print(\"\u2713 SVM created successfully\")\n                return result\n        else:\n            print(f\"\u2717 Failed to create SVM: {response.status_code}\")\n            print(f\"Response: {response.text}\")\n            return None\n\n    def wait_for_job(self, job_uuid, timeout=300):\n        \"\"\"Wait for job completion\"\"\"\n        url = f\"{self.base_url}/management-server/jobs/{job_uuid}\"\n        start_time = time.time()\n\n        while time.time() - start_time &lt; timeout:\n            response = requests.get(url, auth=self.auth)\n\n            if response.status_code == 200:\n                job = response.json()\n                state = job.get('state')\n\n                if state == 'success':\n                    print(\"\u2713 Job completed successfully\")\n                    return job\n                elif state in ['failure', 'partial_failures']:\n                    print(f\"\u2717 Job failed: {job.get('message', 'Unknown error')}\")\n                    return None\n                else:\n                    print(f\"\u23f3 Job in progress: {state} ({job.get('progress', 0)}%)\")\n                    time.sleep(10)\n            else:\n                print(f\"\u2717 Failed to check job status: {response.status_code}\")\n                return None\n\n        print(\"\u2717 Job timeout\")\n        return None\n\n    def create_network_interface(self, svm_key, lif_config):\n        \"\"\"Create network interface for SVM\"\"\"\n        url = f\"{self.base_url}/datacenter/svm/svms/{svm_key}/network/ip/interfaces\"\n\n        response = requests.post(url, auth=self.auth, json=lif_config)\n\n        if response.status_code in [201, 202]:\n            print(f\"\u2713 Network interface {lif_config['name']} created\")\n            return response.json()\n        else:\n            print(f\"\u2717 Failed to create interface: {response.status_code}\")\n            return None\n\n# Usage example\ndef create_complete_svm():\n    svm_mgr = SVMManager(\"um-server.example.com\", \"admin\", \"password\")\n\n    cluster_key = \"cluster-uuid-here\"\n\n    # Step 1: Validate cluster\n    if not svm_mgr.validate_cluster(cluster_key):\n        return False\n\n    # Step 2: Get available aggregates\n    aggregates = svm_mgr.get_available_aggregates(cluster_key)\n    if not aggregates:\n        print(\"\u2717 No suitable aggregates found\")\n        return False\n\n    # Step 3: Create SVM\n    svm_config = {\n        \"name\": \"svm_nfs_prod\",\n        \"cluster\": {\"key\": cluster_key},\n        \"state\": \"running\",\n        \"subtype\": \"default\",\n        \"language\": \"c.utf_8\",\n        \"allowed_protocols\": [\"nfs\"],\n        \"aggregates\": [{\"key\": aggregates[0]['key']}]\n    }\n\n    svm_result = svm_mgr.create_svm(svm_config)\n    if not svm_result:\n        return False\n\n    svm_key = svm_result.get('key') or svm_result.get('records', [{}])[0].get('key')\n\n    # Step 4: Create network interfaces\n    mgmt_lif = {\n        \"name\": \"svm_nfs_prod_mgmt\",\n        \"ip\": {\n            \"address\": \"10.1.100.50\",\n            \"netmask\": \"255.255.255.0\"\n        },\n        \"service_policy\": \"default-management\",\n        \"enabled\": True\n    }\n\n    data_lif = {\n        \"name\": \"svm_nfs_prod_data\",\n        \"ip\": {\n            \"address\": \"10.1.100.51\",\n            \"netmask\": \"255.255.255.0\"\n        },\n        \"service_policy\": \"default-data-files\",\n        \"enabled\": True\n    }\n\n    svm_mgr.create_network_interface(svm_key, mgmt_lif)\n    svm_mgr.create_network_interface(svm_key, data_lif)\n\n    print(\"\u2713 SVM creation workflow completed successfully\")\n    return True\n\nif __name__ == \"__main__\":\n    create_complete_svm()\n</code></pre>"},{"location":"advanced-use-cases/#nfs-share-management","title":"NFS Share Management","text":""},{"location":"advanced-use-cases/#creating-nfs-shares","title":"Creating NFS Shares","text":""},{"location":"advanced-use-cases/#1-create-volume-for-nfs-share","title":"1. Create Volume for NFS Share","text":"<pre><code>curl -u \"admin:password\" -X POST \\\n  \"https://um-server.example.com/api/v2/storage-provider/file-shares\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"shared_documents\",\n    \"size\": \"500GB\",\n    \"svm\": {\n      \"key\": \"svm-key-here\"\n    },\n    \"aggregate\": {\n      \"key\": \"aggregate-key-here\"\n    },\n    \"performance_service_level\": {\n      \"key\": \"psl-key-here\"\n    },\n    \"export_policy\": {\n      \"name\": \"default\"\n    },\n    \"unix_permissions\": \"755\",\n    \"security_style\": \"unix\",\n    \"space_guarantee\": \"none\",\n    \"snapshot_policy\": {\n      \"name\": \"default\"\n    }\n  }'\n</code></pre>"},{"location":"advanced-use-cases/#2-configure-nfs-export-policy","title":"2. Configure NFS Export Policy","text":"<pre><code># Create export policy\ncurl -u \"admin:password\" -X POST \\\n  \"https://um-server.example.com/api/v2/datacenter/svm/svms/{svm_key}/export-policies\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"nfs_production_policy\",\n    \"rules\": [\n      {\n        \"clients\": [\"10.1.0.0/16\"],\n        \"protocols\": [\"nfs3\", \"nfs4\"],\n        \"ro_rule\": [\"sys\"],\n        \"rw_rule\": [\"sys\"],\n        \"superuser\": [\"sys\"],\n        \"allow_suid\": false\n      },\n      {\n        \"clients\": [\"192.168.1.0/24\"],\n        \"protocols\": [\"nfs3\", \"nfs4\"],\n        \"ro_rule\": [\"sys\"],\n        \"rw_rule\": [\"none\"],\n        \"superuser\": [\"none\"],\n        \"allow_suid\": false\n      }\n    ]\n  }'\n</code></pre>"},{"location":"advanced-use-cases/#3-update-volume-export-policy","title":"3. Update Volume Export Policy","text":"<pre><code>curl -u \"admin:password\" -X PATCH \\\n  \"https://um-server.example.com/api/v2/storage-provider/file-shares/{file_share_key}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"export_policy\": {\n      \"name\": \"nfs_production_policy\"\n    }\n  }'\n</code></pre>"},{"location":"advanced-use-cases/#python-nfs-share-manager","title":"Python NFS Share Manager","text":"<pre><code>class NFSShareManager:\n    def __init__(self, um_host, username, password):\n        self.um_host = um_host\n        self.auth = HTTPBasicAuth(username, password)\n        self.base_url = f\"https://{um_host}/api/v2\"\n\n    def create_export_policy(self, svm_key, policy_name, rules):\n        \"\"\"Create NFS export policy with specified rules\"\"\"\n        url = f\"{self.base_url}/datacenter/svm/svms/{svm_key}/export-policies\"\n\n        policy_config = {\n            \"name\": policy_name,\n            \"rules\": rules\n        }\n\n        response = requests.post(url, auth=self.auth, json=policy_config)\n\n        if response.status_code in [201, 202]:\n            print(f\"\u2713 Export policy {policy_name} created\")\n            return response.json()\n        else:\n            print(f\"\u2717 Failed to create export policy: {response.status_code}\")\n            return None\n\n    def create_nfs_share(self, share_config):\n        \"\"\"Create NFS file share\"\"\"\n        url = f\"{self.base_url}/storage-provider/file-shares\"\n\n        response = requests.post(url, auth=self.auth, json=share_config)\n\n        if response.status_code in [201, 202]:\n            result = response.json()\n            if 'job' in result:\n                return self.wait_for_job(result['job']['uuid'])\n            return result\n        else:\n            print(f\"\u2717 Failed to create NFS share: {response.status_code}\")\n            return None\n\n    def update_share_export_policy(self, share_key, policy_name):\n        \"\"\"Update export policy for existing share\"\"\"\n        url = f\"{self.base_url}/storage-provider/file-shares/{share_key}\"\n\n        update_config = {\n            \"export_policy\": {\n                \"name\": policy_name\n            }\n        }\n\n        response = requests.patch(url, auth=self.auth, json=update_config)\n\n        if response.status_code == 200:\n            print(f\"\u2713 Share export policy updated to {policy_name}\")\n            return response.json()\n        else:\n            print(f\"\u2717 Failed to update export policy: {response.status_code}\")\n            return None\n\n# Complete NFS setup workflow\ndef setup_nfs_environment():\n    nfs_mgr = NFSShareManager(\"um-server.example.com\", \"admin\", \"password\")\n\n    svm_key = \"svm-key-here\"\n\n    # Create export policy with multiple rules\n    export_rules = [\n        {\n            \"clients\": [\"10.1.0.0/16\"],\n            \"protocols\": [\"nfs3\", \"nfs4\"],\n            \"ro_rule\": [\"sys\"],\n            \"rw_rule\": [\"sys\"],\n            \"superuser\": [\"sys\"],\n            \"allow_suid\": False\n        },\n        {\n            \"clients\": [\"192.168.1.0/24\"],\n            \"protocols\": [\"nfs3\", \"nfs4\"],\n            \"ro_rule\": [\"sys\"],\n            \"rw_rule\": [\"none\"],\n            \"superuser\": [\"none\"],\n            \"allow_suid\": False\n        }\n    ]\n\n    policy_result = nfs_mgr.create_export_policy(\n        svm_key,\n        \"production_nfs_policy\",\n        export_rules\n    )\n\n    if policy_result:\n        # Create NFS share\n        share_config = {\n            \"name\": \"shared_documents\",\n            \"size\": \"500GB\",\n            \"svm\": {\"key\": svm_key},\n            \"export_policy\": {\"name\": \"production_nfs_policy\"},\n            \"unix_permissions\": \"755\",\n            \"security_style\": \"unix\"\n        }\n\n        share_result = nfs_mgr.create_nfs_share(share_config)\n\n        if share_result:\n            print(\"\u2713 NFS environment setup completed\")\n            return True\n\n    return False\n</code></pre>"},{"location":"advanced-use-cases/#nfs-credential-updates","title":"NFS Credential Updates","text":""},{"location":"advanced-use-cases/#updating-nfs-authentication-and-authorization","title":"Updating NFS Authentication and Authorization","text":""},{"location":"advanced-use-cases/#1-update-svm-ldap-configuration","title":"1. Update SVM LDAP Configuration","text":"<pre><code>curl -u \"admin:password\" -X PATCH \\\n  \"https://um-server.example.com/api/v2/datacenter/svm/svms/{svm_key}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"ldap\": {\n      \"enabled\": true,\n      \"servers\": [\"ldap1.company.local\", \"ldap2.company.local\"],\n      \"base_dn\": \"dc=company,dc=local\",\n      \"bind_dn\": \"cn=netapp,ou=service,dc=company,dc=local\",\n      \"bind_password\": \"secure_password\",\n      \"schema\": \"RFC-2307\",\n      \"port\": 389,\n      \"use_start_tls\": true\n    }\n  }'\n</code></pre>"},{"location":"advanced-use-cases/#2-update-export-policy-rules-with-new-authentication","title":"2. Update Export Policy Rules with New Authentication","text":"<pre><code>curl -u \"admin:password\" -X PATCH \\\n  \"https://um-server.example.com/api/v2/datacenter/svm/svms/{svm_key}/export-policies/{policy_name}/rules/{rule_index}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"clients\": [\"10.1.0.0/16\"],\n    \"protocols\": [\"nfs4\"],\n    \"ro_rule\": [\"krb5\", \"sys\"],\n    \"rw_rule\": [\"krb5\", \"sys\"],\n    \"superuser\": [\"krb5\"],\n    \"allow_suid\": false,\n    \"allow_dev\": false,\n    \"ntfs_unix_security\": \"fail\"\n  }'\n</code></pre>"},{"location":"advanced-use-cases/#3-configure-kerberos-authentication","title":"3. Configure Kerberos Authentication","text":"<pre><code>curl -u \"admin:password\" -X POST \\\n  \"https://um-server.example.com/api/v2/datacenter/svm/svms/{svm_key}/kerberos/realms\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"COMPANY.LOCAL\",\n    \"kdc_ip\": \"10.1.1.20\",\n    \"kdc_port\": 88,\n    \"admin_server_ip\": \"10.1.1.20\",\n    \"admin_server_port\": 749,\n    \"password_server_ip\": \"10.1.1.20\",\n    \"password_server_port\": 464,\n    \"clock_skew\": 5,\n    \"comment\": \"Production Kerberos realm\"\n  }'\n</code></pre>"},{"location":"advanced-use-cases/#python-credential-management","title":"Python Credential Management","text":"<pre><code>class NFSCredentialManager:\n    def __init__(self, um_host, username, password):\n        self.um_host = um_host\n        self.auth = HTTPBasicAuth(username, password)\n        self.base_url = f\"https://{um_host}/api/v2\"\n\n    def update_ldap_config(self, svm_key, ldap_config):\n        \"\"\"Update LDAP configuration for SVM\"\"\"\n        url = f\"{self.base_url}/datacenter/svm/svms/{svm_key}\"\n\n        update_data = {\"ldap\": ldap_config}\n\n        response = requests.patch(url, auth=self.auth, json=update_data)\n\n        if response.status_code == 200:\n            print(\"\u2713 LDAP configuration updated\")\n            return response.json()\n        else:\n            print(f\"\u2717 Failed to update LDAP config: {response.status_code}\")\n            return None\n\n    def update_export_rule_auth(self, svm_key, policy_name, rule_index, auth_config):\n        \"\"\"Update authentication rules for export policy\"\"\"\n        url = f\"{self.base_url}/datacenter/svm/svms/{svm_key}/export-policies/{policy_name}/rules/{rule_index}\"\n\n        response = requests.patch(url, auth=self.auth, json=auth_config)\n\n        if response.status_code == 200:\n            print(\"\u2713 Export rule authentication updated\")\n            return response.json()\n        else:\n            print(f\"\u2717 Failed to update export rule: {response.status_code}\")\n            return None\n\n    def configure_kerberos(self, svm_key, kerberos_config):\n        \"\"\"Configure Kerberos realm for SVM\"\"\"\n        url = f\"{self.base_url}/datacenter/svm/svms/{svm_key}/kerberos/realms\"\n\n        response = requests.post(url, auth=self.auth, json=kerberos_config)\n\n        if response.status_code in [201, 202]:\n            print(\"\u2713 Kerberos realm configured\")\n            return response.json()\n        else:\n            print(f\"\u2717 Failed to configure Kerberos: {response.status_code}\")\n            return None\n\n# Complete credential update workflow\ndef update_nfs_security():\n    cred_mgr = NFSCredentialManager(\"um-server.example.com\", \"admin\", \"password\")\n\n    svm_key = \"svm-key-here\"\n\n    # Update LDAP configuration\n    ldap_config = {\n        \"enabled\": True,\n        \"servers\": [\"ldap1.company.local\", \"ldap2.company.local\"],\n        \"base_dn\": \"dc=company,dc=local\",\n        \"bind_dn\": \"cn=netapp,ou=service,dc=company,dc=local\",\n        \"bind_password\": \"secure_password\",\n        \"schema\": \"RFC-2307\",\n        \"use_start_tls\": True\n    }\n\n    ldap_result = cred_mgr.update_ldap_config(svm_key, ldap_config)\n\n    if ldap_result:\n        # Update export policy authentication\n        auth_config = {\n            \"clients\": [\"10.1.0.0/16\"],\n            \"protocols\": [\"nfs4\"],\n            \"ro_rule\": [\"krb5\", \"sys\"],\n            \"rw_rule\": [\"krb5\", \"sys\"],\n            \"superuser\": [\"krb5\"],\n            \"allow_suid\": False\n        }\n\n        auth_result = cred_mgr.update_export_rule_auth(\n            svm_key,\n            \"production_nfs_policy\",\n            0,\n            auth_config\n        )\n\n        if auth_result:\n            # Configure Kerberos\n            kerberos_config = {\n                \"name\": \"COMPANY.LOCAL\",\n                \"kdc_ip\": \"10.1.1.20\",\n                \"admin_server_ip\": \"10.1.1.20\",\n                \"clock_skew\": 5\n            }\n\n            krb_result = cred_mgr.configure_kerberos(svm_key, kerberos_config)\n\n            if krb_result:\n                print(\"\u2713 NFS security configuration completed\")\n                return True\n\n    return False\n</code></pre>"},{"location":"advanced-use-cases/#sequence-diagrams","title":"Sequence Diagrams","text":""},{"location":"advanced-use-cases/#svm-creation-sequence","title":"SVM Creation Sequence","text":"<pre><code>sequenceDiagram\n    participant Client\n    participant UM as Unified Manager\n    participant Cluster\n    participant SVM\n    participant Network\n\n    Client-&gt;&gt;UM: 1. Validate Cluster Health\n    UM-&gt;&gt;Cluster: Check cluster status\n    Cluster--&gt;&gt;UM: Return health status\n    UM--&gt;&gt;Client: Cluster validation result\n\n    Client-&gt;&gt;UM: 2. Get Available Aggregates\n    UM-&gt;&gt;Cluster: Query aggregates\n    Cluster--&gt;&gt;UM: Return aggregate list\n    UM--&gt;&gt;Client: Available aggregates\n\n    Client-&gt;&gt;UM: 3. Create SVM Request\n    UM-&gt;&gt;Cluster: Create SVM command\n    Cluster-&gt;&gt;SVM: Initialize SVM\n    SVM--&gt;&gt;Cluster: SVM created\n    Cluster--&gt;&gt;UM: Job started\n    UM--&gt;&gt;Client: Job UUID\n\n    Client-&gt;&gt;UM: 4. Monitor Job Status\n    loop Job Monitoring\n        UM-&gt;&gt;Cluster: Check job progress\n        Cluster--&gt;&gt;UM: Job status\n        UM--&gt;&gt;Client: Progress update\n    end\n\n    Client-&gt;&gt;UM: 5. Create Management LIF\n    UM-&gt;&gt;SVM: Configure mgmt interface\n    SVM-&gt;&gt;Network: Setup network interface\n    Network--&gt;&gt;SVM: Interface ready\n    SVM--&gt;&gt;UM: LIF created\n    UM--&gt;&gt;Client: Management LIF ready\n\n    Client-&gt;&gt;UM: 6. Create Data LIF\n    UM-&gt;&gt;SVM: Configure data interface\n    SVM-&gt;&gt;Network: Setup data interface\n    Network--&gt;&gt;SVM: Interface ready\n    SVM--&gt;&gt;UM: Data LIF created\n    UM--&gt;&gt;Client: SVM fully configured</code></pre>"},{"location":"advanced-use-cases/#nfs-share-creation-sequence","title":"NFS Share Creation Sequence","text":"<pre><code>sequenceDiagram\n    participant Client\n    participant UM as Unified Manager\n    participant SVM\n    participant Volume\n    participant Export as Export Policy\n\n    Client-&gt;&gt;UM: 1. Create Export Policy\n    UM-&gt;&gt;SVM: Configure export policy\n    SVM-&gt;&gt;Export: Create policy rules\n    Export--&gt;&gt;SVM: Policy created\n    SVM--&gt;&gt;UM: Export policy ready\n    UM--&gt;&gt;Client: Policy creation success\n\n    Client-&gt;&gt;UM: 2. Create File Share\n    UM-&gt;&gt;SVM: Create volume request\n    SVM-&gt;&gt;Volume: Initialize volume\n    Volume--&gt;&gt;SVM: Volume created\n    SVM--&gt;&gt;UM: File share job started\n    UM--&gt;&gt;Client: Job UUID\n\n    loop Job Monitoring\n        Client-&gt;&gt;UM: Check job status\n        UM-&gt;&gt;SVM: Query job progress\n        SVM--&gt;&gt;UM: Job status\n        UM--&gt;&gt;Client: Progress update\n    end\n\n    Client-&gt;&gt;UM: 3. Apply Export Policy\n    UM-&gt;&gt;Volume: Attach export policy\n    Volume-&gt;&gt;Export: Link to policy\n    Export--&gt;&gt;Volume: Policy attached\n    Volume--&gt;&gt;UM: Export policy applied\n    UM--&gt;&gt;Client: NFS share ready\n\n    Client-&gt;&gt;UM: 4. Verify NFS Access\n    UM-&gt;&gt;SVM: Check NFS service\n    SVM--&gt;&gt;UM: NFS status\n    UM--&gt;&gt;Client: NFS share accessible</code></pre>"},{"location":"advanced-use-cases/#nfs-credential-update-sequence","title":"NFS Credential Update Sequence","text":"<pre><code>sequenceDiagram\n    participant Client\n    participant UM as Unified Manager\n    participant SVM\n    participant LDAP\n    participant Kerberos as KDC\n    participant Export as Export Policy\n\n    Client-&gt;&gt;UM: 1. Update LDAP Config\n    UM-&gt;&gt;SVM: Configure LDAP settings\n    SVM-&gt;&gt;LDAP: Test LDAP connection\n    LDAP--&gt;&gt;SVM: Connection verified\n    SVM--&gt;&gt;UM: LDAP configured\n    UM--&gt;&gt;Client: LDAP update success\n\n    Client-&gt;&gt;UM: 2. Configure Kerberos\n    UM-&gt;&gt;SVM: Setup Kerberos realm\n    SVM-&gt;&gt;Kerberos: Register with KDC\n    Kerberos--&gt;&gt;SVM: Realm configured\n    SVM--&gt;&gt;UM: Kerberos ready\n    UM--&gt;&gt;Client: Kerberos configuration success\n\n    Client-&gt;&gt;UM: 3. Update Export Rules\n    UM-&gt;&gt;Export: Modify authentication rules\n    Export-&gt;&gt;SVM: Apply new auth methods\n    SVM--&gt;&gt;Export: Rules updated\n    Export--&gt;&gt;UM: Export policy updated\n    UM--&gt;&gt;Client: Authentication updated\n\n    Client-&gt;&gt;UM: 4. Verify Access\n    UM-&gt;&gt;SVM: Test NFS with new auth\n    SVM-&gt;&gt;LDAP: Validate user credentials\n    LDAP--&gt;&gt;SVM: User validated\n    SVM-&gt;&gt;Kerberos: Check Kerberos ticket\n    Kerberos--&gt;&gt;SVM: Ticket valid\n    SVM--&gt;&gt;UM: Access verified\n    UM--&gt;&gt;Client: Credential update complete</code></pre>"},{"location":"advanced-use-cases/#complete-workflow-integration","title":"Complete Workflow Integration","text":"<pre><code>sequenceDiagram\n    participant Admin\n    participant Script\n    participant UM as Unified Manager\n    participant Cluster\n    participant SVM\n    participant NFS\n\n    Admin-&gt;&gt;Script: 1. Execute Full Setup\n\n    rect rgb(240, 248, 255)\n        Note over Script, Cluster: SVM Creation Phase\n        Script-&gt;&gt;UM: Validate cluster\n        Script-&gt;&gt;UM: Create SVM\n        Script-&gt;&gt;UM: Configure network\n    end\n\n    rect rgb(240, 255, 240)\n        Note over Script, NFS: NFS Configuration Phase\n        Script-&gt;&gt;UM: Create export policy\n        Script-&gt;&gt;UM: Create NFS share\n        Script-&gt;&gt;UM: Apply export policy\n    end\n\n    rect rgb(255, 248, 240)\n        Note over Script, NFS: Security Configuration Phase\n        Script-&gt;&gt;UM: Configure LDAP\n        Script-&gt;&gt;UM: Setup Kerberos\n        Script-&gt;&gt;UM: Update export rules\n    end\n\n    Script-&gt;&gt;UM: 2. Validate Complete Setup\n    UM-&gt;&gt;SVM: Check SVM status\n    SVM-&gt;&gt;NFS: Verify NFS services\n    NFS--&gt;&gt;SVM: Services operational\n    SVM--&gt;&gt;UM: All services ready\n    UM--&gt;&gt;Script: Setup verification complete\n    Script--&gt;&gt;Admin: Full environment ready</code></pre>"},{"location":"advanced-use-cases/#api-sequence-action-tables","title":"API Sequence Action Tables","text":""},{"location":"advanced-use-cases/#svm-creation-sequence_1","title":"SVM Creation Sequence","text":"Step Description API Endpoint HTTP Method 1 Validate Cluster Health <code>/datacenter/cluster/clusters/{cluster_key}</code> GET 2 Get Available Aggregates <code>/datacenter/storage/aggregates</code> GET 3 Create SVM <code>/datacenter/svm/svms</code> POST 4 Monitor Job Status <code>/management-server/jobs/{uuid}</code> GET 5 Create Management LIF <code>/datacenter/svm/svms/{svm_key}/network/ip/interfaces</code> POST 6 Create Data LIF <code>/datacenter/svm/svms/{svm_key}/network/ip/interfaces</code> POST"},{"location":"advanced-use-cases/#nfs-share-creation-sequence_1","title":"NFS Share Creation Sequence","text":"Step Description API Endpoint HTTP Method 1 Create Export Policy <code>/datacenter/svm/svms/{svm_key}/export-policies</code> POST 2 Create File Share <code>/storage-provider/file-shares</code> POST 3 Apply Export Policy <code>/storage-provider/file-shares/{file_share_key}</code> PATCH 4 Verify NFS Access <code>/datacenter/svm/svms/{svm_key}</code> GET"},{"location":"advanced-use-cases/#nfs-credential-update-sequence_1","title":"NFS Credential Update Sequence","text":"Step Description API Endpoint HTTP Method 1 Update LDAP Config <code>/datacenter/svm/svms/{svm_key}</code> PATCH 2 Configure Kerberos <code>/datacenter/svm/svms/{svm_key}</code> PATCH 3 Update Export Rules <code>/datacenter/svm/svms/{svm_key}/export-policies</code> PATCH 4 Verify Access <code>/datacenter/svm/svms/{svm_key}</code> GET"},{"location":"advanced-use-cases/#complete-workflow-integration_1","title":"Complete Workflow Integration","text":"<p>This workflow involves executing combined steps from SVM Creation, NFS Share Creation, and NFS Credential Update sequences. It leverages the same endpoints listed above in a phased manner.</p>"},{"location":"advanced-use-cases/#complete-automation-scripts","title":"Complete Automation Scripts","text":""},{"location":"advanced-use-cases/#master-deployment-script","title":"Master Deployment Script","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nNetApp Complete NFS Environment Deployment\nAutomates SVM creation, NFS share setup, and security configuration\n\"\"\"\n\nimport requests\nfrom requests.auth import HTTPBasicAuth\nimport json\nimport time\nimport logging\nfrom datetime import datetime\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler('netapp_deployment.log'),\n        logging.StreamHandler()\n    ]\n)\n\nclass NetAppEnvironmentDeployer:\n    def __init__(self, um_host, username, password):\n        self.um_host = um_host\n        self.auth = HTTPBasicAuth(username, password)\n        self.base_url = f\"https://{um_host}/api/v2\"\n        self.logger = logging.getLogger(__name__)\n\n    def deploy_complete_environment(self, config):\n        \"\"\"Deploy complete NFS environment\"\"\"\n        self.logger.info(\"Starting NetApp NFS environment deployment\")\n\n        try:\n            # Phase 1: SVM Creation\n            self.logger.info(\"Phase 1: Creating SVM\")\n            svm_result = self._create_svm_environment(config['cluster'], config['svm'])\n            if not svm_result:\n                return False\n\n            svm_key = svm_result['key']\n\n            # Phase 2: NFS Share Setup\n            self.logger.info(\"Phase 2: Setting up NFS shares\")\n            share_result = self._setup_nfs_shares(svm_key, config['nfs_shares'])\n            if not share_result:\n                return False\n\n            # Phase 3: Security Configuration\n            self.logger.info(\"Phase 3: Configuring security\")\n            security_result = self._configure_security(svm_key, config['security'])\n            if not security_result:\n                return False\n\n            # Phase 4: Validation\n            self.logger.info(\"Phase 4: Validating deployment\")\n            validation_result = self._validate_deployment(svm_key)\n\n            if validation_result:\n                self.logger.info(\"\u2713 Complete NFS environment deployment successful\")\n                return True\n            else:\n                self.logger.error(\"\u2717 Deployment validation failed\")\n                return False\n\n        except Exception as e:\n            self.logger.error(f\"Deployment failed: {str(e)}\")\n            return False\n\n    def _create_svm_environment(self, cluster_config, svm_config):\n        \"\"\"Create SVM with network configuration\"\"\"\n        # Implementation similar to previous examples\n        # ... (SVM creation logic)\n        pass\n\n    def _setup_nfs_shares(self, svm_key, shares_config):\n        \"\"\"Setup NFS shares with export policies\"\"\"\n        # Implementation for NFS share creation\n        # ... (NFS setup logic)\n        pass\n\n    def _configure_security(self, svm_key, security_config):\n        \"\"\"Configure LDAP and Kerberos security\"\"\"\n        # Implementation for security configuration\n        # ... (Security setup logic)\n        pass\n\n    def _validate_deployment(self, svm_key):\n        \"\"\"Validate complete deployment\"\"\"\n        # Implementation for deployment validation\n        # ... (Validation logic)\n        pass\n\n# Example deployment configuration\ndeployment_config = {\n    \"cluster\": {\n        \"key\": \"cluster-uuid-here\"\n    },\n    \"svm\": {\n        \"name\": \"prod_nfs_svm\",\n        \"protocols\": [\"nfs\"],\n        \"dns_domain\": \"company.local\",\n        \"dns_servers\": [\"10.1.1.10\", \"10.1.1.11\"],\n        \"network_interfaces\": [\n            {\n                \"name\": \"mgmt_lif\",\n                \"ip\": \"10.1.100.50\",\n                \"netmask\": \"255.255.255.0\",\n                \"role\": \"management\"\n            },\n            {\n                \"name\": \"data_lif\",\n                \"ip\": \"10.1.100.51\",\n                \"netmask\": \"255.255.255.0\",\n                \"role\": \"data\"\n            }\n        ]\n    },\n    \"nfs_shares\": [\n        {\n            \"name\": \"shared_documents\",\n            \"size\": \"500GB\",\n            \"export_policy\": \"production_policy\",\n            \"unix_permissions\": \"755\"\n        },\n        {\n            \"name\": \"user_homes\",\n            \"size\": \"1TB\",\n            \"export_policy\": \"home_directories\",\n            \"unix_permissions\": \"700\"\n        }\n    ],\n    \"security\": {\n        \"ldap\": {\n            \"enabled\": True,\n            \"servers\": [\"ldap1.company.local\", \"ldap2.company.local\"],\n            \"base_dn\": \"dc=company,dc=local\"\n        },\n        \"kerberos\": {\n            \"realm\": \"COMPANY.LOCAL\",\n            \"kdc_server\": \"10.1.1.20\"\n        }\n    }\n}\n\n# Usage\nif __name__ == \"__main__\":\n    deployer = NetAppEnvironmentDeployer(\"um-server.example.com\", \"admin\", \"password\")\n    success = deployer.deploy_complete_environment(deployment_config)\n\n    if success:\n        print(\"Deployment completed successfully!\")\n    else:\n        print(\"Deployment failed. Check logs for details.\")\n</code></pre>"},{"location":"advanced-use-cases/#best-practices-and-considerations","title":"Best Practices and Considerations","text":""},{"location":"advanced-use-cases/#security-best-practices","title":"Security Best Practices","text":"<ol> <li>Use strong authentication - Implement Kerberos for production environments</li> <li>Principle of least privilege - Grant minimal required access</li> <li>Regular credential rotation - Update service accounts periodically</li> <li>Network segmentation - Use appropriate subnet restrictions</li> <li>Audit logging - Monitor all API calls and changes</li> </ol>"},{"location":"advanced-use-cases/#performance-considerations","title":"Performance Considerations","text":"<ol> <li>Aggregate selection - Choose high-performance aggregates for critical workloads</li> <li>Network optimization - Use dedicated networks for NFS traffic</li> <li>Export policy optimization - Minimize rule complexity</li> <li>Load balancing - Distribute NFS clients across multiple LIFs</li> </ol>"},{"location":"advanced-use-cases/#monitoring-and-maintenance","title":"Monitoring and Maintenance","text":"<ol> <li>Automated health checks - Regular validation of services</li> <li>Capacity monitoring - Track volume usage and growth</li> <li>Performance monitoring - Monitor NFS latency and throughput</li> <li>Backup validation - Ensure backup policies are effective</li> </ol> <p>This comprehensive guide provides the foundation for advanced NetApp ActiveIQ API operations, complete with sequence diagrams and production-ready automation scripts.</p>"},{"location":"api-endpoints/","title":"NetApp ActiveIQ API - Endpoints Reference","text":"<p>This document provides a comprehensive list of available API endpoints in the NetApp ActiveIQ Unified Manager REST API.</p>"},{"location":"api-endpoints/#base-url-structure","title":"Base URL Structure","text":"<pre><code>https://&lt;hostname&gt;:&lt;port&gt;/api/v2\n</code></pre>"},{"location":"api-endpoints/#endpoint-categories","title":"Endpoint Categories","text":""},{"location":"api-endpoints/#1-administration-admin","title":"1. Administration (<code>/admin/*</code>)","text":""},{"location":"api-endpoints/#backup-management","title":"Backup Management","text":"<ul> <li>POST <code>/admin/backup</code> - Create a backup request</li> <li>GET <code>/admin/backup-file-info</code> - Retrieve information on all backup files</li> <li>GET <code>/admin/backup-settings</code> - Get scheduled backup settings</li> <li>PATCH <code>/admin/backup-settings</code> - Update scheduled backup settings</li> </ul>"},{"location":"api-endpoints/#datasource-management","title":"Datasource Management","text":"<ul> <li>GET <code>/admin/datasource-certificate</code> - Retrieve datasource certificate details</li> <li>GET <code>/admin/datasources/clusters</code> - Get all datasources</li> <li>POST <code>/admin/datasources/clusters</code> - Add a new datasource</li> <li>DELETE <code>/admin/datasources/clusters/{key}</code> - Remove a datasource</li> <li>GET <code>/admin/datasources/clusters/{key}</code> - Get specific datasource details</li> <li>PATCH <code>/admin/datasources/clusters/{key}</code> - Update datasource configuration</li> </ul>"},{"location":"api-endpoints/#2-datacenter-datacenter","title":"2. Datacenter (<code>/datacenter/*</code>)","text":""},{"location":"api-endpoints/#cluster-management","title":"Cluster Management","text":"<ul> <li>GET <code>/datacenter/cluster/clusters</code> - Get cluster information</li> <li>GET <code>/datacenter/cluster/clusters/{key}</code> - Get specific cluster details</li> <li>GET <code>/datacenter/cluster/clusters/{key}/nodes</code> - Get cluster nodes</li> <li>GET <code>/datacenter/cluster/clusters/{key}/nodes/{uuid}</code> - Get specific node details</li> </ul>"},{"location":"api-endpoints/#storage-virtual-machines-svms","title":"Storage Virtual Machines (SVMs)","text":"<ul> <li>GET <code>/datacenter/svm/svms</code> - Get all SVMs</li> <li>GET <code>/datacenter/svm/svms/{key}</code> - Get specific SVM details</li> </ul>"},{"location":"api-endpoints/#storage-management","title":"Storage Management","text":"<ul> <li>GET <code>/datacenter/storage/aggregates</code> - Get storage aggregates</li> <li>GET <code>/datacenter/storage/aggregates/{key}</code> - Get specific aggregate details</li> <li>GET <code>/datacenter/storage/disks</code> - Get disk information</li> <li>GET <code>/datacenter/storage/disks/{key}</code> - Get specific disk details</li> <li>GET <code>/datacenter/storage/volumes</code> - Get volume information</li> <li>GET <code>/datacenter/storage/volumes/{key}</code> - Get specific volume details</li> <li>GET <code>/datacenter/storage/luns</code> - Get LUN information</li> <li>GET <code>/datacenter/storage/luns/{key}</code> - Get specific LUN details</li> <li>GET <code>/datacenter/storage/qtrees</code> - Get qtree information</li> <li>GET <code>/datacenter/storage/qtrees/{key}</code> - Get specific qtree details</li> </ul>"},{"location":"api-endpoints/#access-endpoints","title":"Access Endpoints","text":"<ul> <li>GET <code>/datacenter/svm/svms/{svm.key}/access-endpoints</code> - Get SVM access endpoints</li> <li>POST <code>/datacenter/svm/svms/{svm.key}/access-endpoints</code> - Create SVM access endpoint</li> <li>DELETE <code>/datacenter/svm/svms/{svm.key}/access-endpoints/{uuid}</code> - Delete access endpoint</li> <li>GET <code>/datacenter/svm/svms/{svm.key}/access-endpoints/{uuid}</code> - Get specific access endpoint</li> <li>PATCH <code>/datacenter/svm/svms/{svm.key}/access-endpoints/{uuid}</code> - Update access endpoint</li> </ul>"},{"location":"api-endpoints/#3-storage-provider-storage-provider","title":"3. Storage Provider (<code>/storage-provider/*</code>)","text":""},{"location":"api-endpoints/#performance-service-levels","title":"Performance Service Levels","text":"<ul> <li>GET <code>/storage-provider/performance-service-levels</code> - Get performance service levels</li> <li>POST <code>/storage-provider/performance-service-levels</code> - Create performance service level</li> <li>DELETE <code>/storage-provider/performance-service-levels/{key}</code> - Delete performance service level</li> <li>GET <code>/storage-provider/performance-service-levels/{key}</code> - Get specific performance service level</li> <li>PATCH <code>/storage-provider/performance-service-levels/{key}</code> - Update performance service level</li> </ul>"},{"location":"api-endpoints/#file-shares","title":"File Shares","text":"<ul> <li>GET <code>/storage-provider/file-shares</code> - Get file shares</li> <li>POST <code>/storage-provider/file-shares</code> - Create file share</li> <li>DELETE <code>/storage-provider/file-shares/{key}</code> - Delete file share</li> <li>GET <code>/storage-provider/file-shares/{key}</code> - Get specific file share</li> <li>PATCH <code>/storage-provider/file-shares/{key}</code> - Update file share</li> </ul>"},{"location":"api-endpoints/#luns","title":"LUNs","text":"<ul> <li>GET <code>/storage-provider/luns</code> - Get LUNs</li> <li>POST <code>/storage-provider/luns</code> - Create LUN</li> <li>DELETE <code>/storage-provider/luns/{key}</code> - Delete LUN</li> <li>GET <code>/storage-provider/luns/{key}</code> - Get specific LUN</li> <li>PATCH <code>/storage-provider/luns/{key}</code> - Update LUN</li> </ul>"},{"location":"api-endpoints/#4-management-server-management-server","title":"4. Management Server (<code>/management-server/*</code>)","text":""},{"location":"api-endpoints/#events-and-alerts","title":"Events and Alerts","text":"<ul> <li>GET <code>/management-server/events</code> - Get events</li> <li>POST <code>/management-server/events/{key}/acknowledge</code> - Acknowledge event</li> <li>DELETE <code>/management-server/events/{key}/acknowledge</code> - Un-acknowledge event</li> <li>POST <code>/management-server/events/{key}/assign-to</code> - Assign event to user</li> <li>POST <code>/management-server/events/{key}/resolve</code> - Resolve event</li> </ul>"},{"location":"api-endpoints/#jobs","title":"Jobs","text":"<ul> <li>GET <code>/management-server/jobs</code> - Get jobs</li> <li>GET <code>/management-server/jobs/{uuid}</code> - Get specific job details</li> </ul>"},{"location":"api-endpoints/#system-information","title":"System Information","text":"<ul> <li>GET <code>/management-server/system</code> - Get system information</li> <li>GET <code>/management-server/version</code> - Get version information</li> </ul>"},{"location":"api-endpoints/#5-gateways-gateways","title":"5. Gateways (<code>/gateways/*</code>)","text":""},{"location":"api-endpoints/#gateway-apis","title":"Gateway APIs","text":"<ul> <li>GET <code>/gateways/clusters/{cluster_uuid}/events</code> - Get cluster events via gateway</li> <li>GET <code>/gateways/clusters/{cluster_uuid}/metrics/aggregates/perf</code> - Get aggregate performance metrics</li> <li>GET <code>/gateways/clusters/{cluster_uuid}/metrics/clusters/perf</code> - Get cluster performance metrics</li> <li>GET <code>/gateways/clusters/{cluster_uuid}/metrics/volumes/perf</code> - Get volume performance metrics</li> </ul>"},{"location":"api-endpoints/#common-query-parameters","title":"Common Query Parameters","text":"<p>All GET endpoints support these common query parameters:</p> <ul> <li>fields (array): Specify which fields to return</li> <li>max_records (integer): Limit the number of records returned (default: 20)</li> <li>offset (integer): Start index for pagination (default: 0)</li> <li>order_by (string): Sort results by field [asc|desc] (default: asc)</li> <li>query (string): Search using 'contains' relationship</li> <li>return_records (boolean): Control whether to return record data or just counts</li> </ul>"},{"location":"api-endpoints/#http-methods-and-response-codes","title":"HTTP Methods and Response Codes","text":""},{"location":"api-endpoints/#supported-http-methods","title":"Supported HTTP Methods","text":"<ul> <li>GET: Retrieve resources</li> <li>POST: Create new resources</li> <li>PATCH: Update existing resources</li> <li>DELETE: Remove resources</li> </ul>"},{"location":"api-endpoints/#common-response-codes","title":"Common Response Codes","text":"<ul> <li>200: OK - Request successful</li> <li>201: Created - Resource created successfully</li> <li>202: Accepted - Request accepted for processing</li> <li>400: Bad Request - Invalid request parameters</li> <li>401: Unauthorized - Authentication required</li> <li>403: Forbidden - Access denied</li> <li>404: Not Found - Resource not found</li> <li>500: Internal Server Error - Server error</li> </ul>"},{"location":"api-endpoints/#authentication","title":"Authentication","text":"<p>All endpoints require HTTP Basic Authentication with one of these roles: - Operator - Storage Administrator - Application Administrator</p>"},{"location":"api-endpoints/#content-type","title":"Content Type","text":"<p>All requests and responses use: <pre><code>Content-Type: application/json\n</code></pre></p>"},{"location":"api-endpoints/#example-usage","title":"Example Usage","text":""},{"location":"api-endpoints/#get-all-clusters","title":"Get All Clusters","text":"<pre><code>GET /api/v2/datacenter/cluster/clusters\n</code></pre>"},{"location":"api-endpoints/#get-cluster-with-specific-fields","title":"Get Cluster with Specific Fields","text":"<pre><code>GET /api/v2/datacenter/cluster/clusters?fields=name,uuid,version\n</code></pre>"},{"location":"api-endpoints/#create-a-backup","title":"Create a Backup","text":"<pre><code>POST /api/v2/admin/backup\nContent-Type: application/json\n</code></pre>"},{"location":"api-endpoints/#get-events-with-pagination","title":"Get Events with Pagination","text":"<pre><code>GET /api/v2/management-server/events?max_records=50&amp;offset=100\n</code></pre> <p>For detailed parameter information and examples for each endpoint, refer to the interactive Swagger documentation available at: <pre><code>https://&lt;your-unified-manager&gt;/apidocs/\n</code></pre></p>"},{"location":"api-use-cases-table/","title":"NetApp ActiveIQ API - Use Cases Reference Table","text":"<p>This document provides a comprehensive table of NetApp ActiveIQ API endpoints organized by use cases and scenarios.</p>"},{"location":"api-use-cases-table/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Infrastructure Discovery &amp; Monitoring</li> <li>Storage Management &amp; Provisioning</li> <li>Performance Monitoring &amp; Analytics</li> <li>Event &amp; Alert Management</li> <li>Backup &amp; Administration</li> <li>Security &amp; Access Management</li> <li>Workload &amp; Service Level Management</li> <li>Automation &amp; Integration</li> </ol>"},{"location":"api-use-cases-table/#infrastructure-discovery-monitoring","title":"Infrastructure Discovery &amp; Monitoring","text":"Use Case API Endpoint HTTP Method Description Common Parameters Example Scenario Cluster Discovery <code>/datacenter/cluster/clusters</code> GET Get all cluster information <code>fields=name,uuid,version,management_ip</code> Initial environment discovery, inventory management Cluster Details <code>/datacenter/cluster/clusters/{key}</code> GET Get specific cluster details <code>fields=name,state,health,version</code> Health checks, cluster validation Node Information <code>/datacenter/cluster/clusters/{key}/nodes</code> GET Get cluster nodes <code>fields=name,uuid,model,serial_number</code> Hardware inventory, capacity planning Node Details <code>/datacenter/cluster/clusters/{key}/nodes/{uuid}</code> GET Get specific node details <code>fields=name,model,uptime,health</code> Node-specific monitoring, troubleshooting SVM Discovery <code>/datacenter/svm/svms</code> GET Get all Storage Virtual Machines <code>fields=name,uuid,cluster,state</code> Multi-tenant environment mapping SVM Details <code>/datacenter/svm/svms/{key}</code> GET Get specific SVM details <code>fields=name,state,protocols,dns</code> SVM configuration validation System Information <code>/management-server/system</code> GET Get system information N/A Environment documentation, compliance Version Information <code>/management-server/version</code> GET Get version information N/A Version tracking, upgrade planning"},{"location":"api-use-cases-table/#storage-management-provisioning","title":"Storage Management &amp; Provisioning","text":"Use Case API Endpoint HTTP Method Description Common Parameters Example Scenario Volume Management <code>/datacenter/storage/volumes</code> GET Get all volumes <code>fields=name,size,svm,cluster</code> Capacity reporting, storage inventory Volume Details <code>/datacenter/storage/volumes/{key}</code> GET Get specific volume details <code>fields=name,size,space,snapshot_policy</code> Volume analysis, capacity planning Aggregate Management <code>/datacenter/storage/aggregates</code> GET Get storage aggregates <code>fields=name,space,state,raid_type</code> Storage pool management Aggregate Details <code>/datacenter/storage/aggregates/{key}</code> GET Get specific aggregate details <code>fields=space.size,space.available,state</code> Capacity monitoring, performance analysis LUN Management <code>/datacenter/storage/luns</code> GET Get LUN information <code>fields=name,size,os_type,serial_number</code> SAN storage management LUN Details <code>/datacenter/storage/luns/{key}</code> GET Get specific LUN details <code>fields=name,size,space,serial_number</code> LUN monitoring, space management Qtree Management <code>/datacenter/storage/qtrees</code> GET Get qtree information <code>fields=name,volume,svm,quota</code> File system organization Qtree Details <code>/datacenter/storage/qtrees/{key}</code> GET Get specific qtree details <code>fields=name,path,security_style</code> Qtree configuration validation Disk Information <code>/datacenter/storage/disks</code> GET Get disk information <code>fields=name,type,size,container</code> Hardware inventory, disk utilization Disk Details <code>/datacenter/storage/disks/{key}</code> GET Get specific disk details <code>fields=name,state,type,rpm</code> Disk health monitoring File Share Creation <code>/storage-provider/file-shares</code> POST Create file share N/A NFS/CIFS share provisioning File Share Management <code>/storage-provider/file-shares</code> GET Get file shares <code>fields=name,size,svm,export_policy</code> Share inventory, access management File Share Details <code>/storage-provider/file-shares/{key}</code> GET Get specific file share <code>fields=name,path,protocols,access</code> Share configuration review File Share Updates <code>/storage-provider/file-shares/{key}</code> PATCH Update file share N/A Share modification, policy updates File Share Deletion <code>/storage-provider/file-shares/{key}</code> DELETE Delete file share N/A Share cleanup, decommissioning LUN Provisioning <code>/storage-provider/luns</code> POST Create LUN N/A SAN storage provisioning LUN Management <code>/storage-provider/luns</code> GET Get LUNs <code>fields=name,size,os_type,location</code> SAN storage inventory LUN Updates <code>/storage-provider/luns/{key}</code> PATCH Update LUN N/A LUN modification, size changes LUN Deletion <code>/storage-provider/luns/{key}</code> DELETE Delete LUN N/A LUN cleanup, storage reclamation"},{"location":"api-use-cases-table/#performance-monitoring-analytics","title":"Performance Monitoring &amp; Analytics","text":"Use Case API Endpoint HTTP Method Description Common Parameters Example Scenario Cluster Performance <code>/gateways/clusters/{cluster_uuid}/metrics/clusters/perf</code> GET Get cluster performance metrics <code>duration=1h&amp;interval=5m</code> Performance monitoring, trend analysis Volume Performance <code>/gateways/clusters/{cluster_uuid}/metrics/volumes/perf</code> GET Get volume performance data <code>duration=24h&amp;interval=1h</code> Volume performance analysis Aggregate Performance <code>/gateways/clusters/{cluster_uuid}/metrics/aggregates/perf</code> GET Get aggregate performance metrics <code>duration=1d&amp;interval=5m</code> Storage pool performance monitoring Cluster Events <code>/gateways/clusters/{cluster_uuid}/events</code> GET Get cluster events via gateway <code>severity=critical&amp;max_records=100</code> Performance issue correlation"},{"location":"api-use-cases-table/#event-alert-management","title":"Event &amp; Alert Management","text":"Use Case API Endpoint HTTP Method Description Common Parameters Example Scenario Event Monitoring <code>/management-server/events</code> GET Get all events <code>query=severity:critical&amp;fields=name,message,time</code> Critical event monitoring Event Acknowledgment <code>/management-server/events/{key}/acknowledge</code> POST Acknowledge event N/A Event management workflow Event Un-acknowledgment <code>/management-server/events/{key}/acknowledge</code> DELETE Un-acknowledge event N/A Event management correction Event Assignment <code>/management-server/events/{key}/assign-to</code> POST Assign event to user N/A Incident management workflow Event Resolution <code>/management-server/events/{key}/resolve</code> POST Resolve event N/A Incident closure, problem resolution"},{"location":"api-use-cases-table/#backup-administration","title":"Backup &amp; Administration","text":"Use Case API Endpoint HTTP Method Description Common Parameters Example Scenario Backup Creation <code>/admin/backup</code> POST Create a backup request N/A Scheduled backup automation Backup Information <code>/admin/backup-file-info</code> GET Get backup file information N/A Backup inventory, space management Backup Settings <code>/admin/backup-settings</code> GET Get scheduled backup settings N/A Backup configuration review Backup Configuration <code>/admin/backup-settings</code> PATCH Update backup settings N/A Backup policy management Job Monitoring <code>/management-server/jobs</code> GET Get all jobs <code>fields=state,progress,message</code> Operation monitoring Job Details <code>/management-server/jobs/{uuid}</code> GET Get specific job details <code>fields=state,start_time,end_time</code> Job tracking, troubleshooting"},{"location":"api-use-cases-table/#security-access-management","title":"Security &amp; Access Management","text":"Use Case API Endpoint HTTP Method Description Common Parameters Example Scenario Datasource Certificate <code>/admin/datasource-certificate</code> GET Get datasource certificate details <code>address=cluster_ip&amp;port=443</code> SSL certificate validation Datasource Management <code>/admin/datasources/clusters</code> GET Get all datasources <code>fields=address,port,protocol</code> Cluster connection inventory Datasource Addition <code>/admin/datasources/clusters</code> POST Add new datasource N/A Cluster onboarding automation Datasource Removal <code>/admin/datasources/clusters/{key}</code> DELETE Remove datasource N/A Cluster decommissioning Datasource Details <code>/admin/datasources/clusters/{key}</code> GET Get specific datasource details <code>fields=address,state,last_update</code> Connection status monitoring Datasource Updates <code>/admin/datasources/clusters/{key}</code> PATCH Update datasource configuration N/A Credential updates, configuration changes Access Endpoints <code>/datacenter/svm/svms/{svm.key}/access-endpoints</code> GET Get SVM access endpoints <code>fields=ip,gateway,vlan</code> Network access management Access Endpoint Creation <code>/datacenter/svm/svms/{svm.key}/access-endpoints</code> POST Create SVM access endpoint N/A Network provisioning Access Endpoint Deletion <code>/datacenter/svm/svms/{svm.key}/access-endpoints/{uuid}</code> DELETE Delete access endpoint N/A Network cleanup Access Endpoint Details <code>/datacenter/svm/svms/{svm.key}/access-endpoints/{uuid}</code> GET Get specific access endpoint <code>fields=ip,protocols,services</code> Network configuration review Access Endpoint Updates <code>/datacenter/svm/svms/{svm.key}/access-endpoints/{uuid}</code> PATCH Update access endpoint N/A Network reconfiguration"},{"location":"api-use-cases-table/#workload-service-level-management","title":"Workload &amp; Service Level Management","text":"Use Case API Endpoint HTTP Method Description Common Parameters Example Scenario Performance Service Levels <code>/storage-provider/performance-service-levels</code> GET Get performance service levels <code>fields=name,expected_iops,peak_iops</code> Service level inventory PSL Creation <code>/storage-provider/performance-service-levels</code> POST Create performance service level N/A Custom service level definition PSL Deletion <code>/storage-provider/performance-service-levels/{key}</code> DELETE Delete performance service level N/A Service level cleanup PSL Details <code>/storage-provider/performance-service-levels/{key}</code> GET Get specific performance service level <code>fields=name,iops,latency,allocation</code> Service level configuration review PSL Updates <code>/storage-provider/performance-service-levels/{key}</code> PATCH Update performance service level N/A Service level modification"},{"location":"api-use-cases-table/#automation-integration","title":"Automation &amp; Integration","text":"Use Case API Endpoint HTTP Method Description Common Parameters Example Scenario Health Check Automation Multiple endpoints GET Automated health checks <code>fields=health,state,status</code> Automated monitoring scripts Capacity Reporting Volume/Aggregate endpoints GET Automated capacity reporting <code>fields=size,available,used_percent</code> Capacity management automation Event Dashboard <code>/management-server/events</code> GET Automated event monitoring <code>query=state:new&amp;severity=critical</code> Dashboard integration Backup Automation <code>/admin/backup*</code> GET/POST/PATCH Automated backup management N/A Backup workflow automation Provisioning Automation Storage Provider endpoints POST/PATCH Automated resource provisioning N/A Self-service portals Compliance Reporting Multiple endpoints GET Automated compliance checks Various field selections Regulatory compliance automation"},{"location":"api-use-cases-table/#use-case-categories-summary","title":"Use Case Categories Summary","text":""},{"location":"api-use-cases-table/#monitoring-observability","title":"Monitoring &amp; Observability","text":"<ul> <li>Infrastructure Discovery: Cluster, node, and SVM discovery</li> <li>Performance Monitoring: Real-time and historical performance data</li> <li>Event Management: Critical event monitoring and alerting</li> <li>Health Checks: Automated health status validation</li> </ul>"},{"location":"api-use-cases-table/#storage-operations","title":"Storage Operations","text":"<ul> <li>Provisioning: Volume, LUN, and file share creation</li> <li>Management: Storage resource lifecycle management</li> <li>Capacity Planning: Space utilization and growth tracking</li> <li>Performance Analysis: Storage performance optimization</li> </ul>"},{"location":"api-use-cases-table/#administration-security","title":"Administration &amp; Security","text":"<ul> <li>Backup Management: Automated backup operations</li> <li>Access Control: Network and security configuration</li> <li>Certificate Management: SSL certificate validation</li> <li>User Management: Role-based access control</li> </ul>"},{"location":"api-use-cases-table/#integration-automation","title":"Integration &amp; Automation","text":"<ul> <li>Workflow Automation: End-to-end process automation</li> <li>Custom Dashboards: Integration with monitoring systems</li> <li>Self-Service Portals: User-driven resource provisioning</li> <li>Compliance: Automated compliance and audit reporting</li> </ul>"},{"location":"api-use-cases-table/#common-query-parameters-reference","title":"Common Query Parameters Reference","text":"Parameter Type Description Example Usage <code>fields</code> Array Specify which fields to return <code>fields=name,uuid,state</code> <code>max_records</code> Integer Limit number of records (default: 20) <code>max_records=100</code> <code>offset</code> Integer Start index for pagination (default: 0) <code>offset=50</code> <code>order_by</code> String Sort results by field [asc|desc] <code>order_by=name desc</code> <code>query</code> String Search using 'contains' relationship <code>query=severity:critical</code> <code>return_records</code> Boolean Return record data or just counts <code>return_records=false</code>"},{"location":"api-use-cases-table/#http-response-codes","title":"HTTP Response Codes","text":"Code Status Description Use Case 200 OK Request successful Successful GET operations 201 Created Resource created successfully Successful POST operations 202 Accepted Request accepted for processing Asynchronous operations 400 Bad Request Invalid request parameters Parameter validation errors 401 Unauthorized Authentication required Credential issues 403 Forbidden Access denied Permission issues 404 Not Found Resource not found Invalid resource keys 500 Internal Server Error Server error System issues"},{"location":"api-use-cases-table/#authentication-requirements","title":"Authentication Requirements","text":"<p>All API endpoints require HTTP Basic Authentication with one of these roles: - Operator: Read-only access to most resources - Storage Administrator: Read/write access to storage resources - Application Administrator: Full administrative access</p> <p>Content-Type: All requests and responses use <code>application/json</code></p>"},{"location":"api-use-cases-table/#best-practices-for-api-usage","title":"Best Practices for API Usage","text":"<ol> <li>Use field selection to reduce response size and improve performance</li> <li>Implement pagination for large datasets using <code>max_records</code> and <code>offset</code></li> <li>Handle rate limiting with appropriate delays between requests</li> <li>Use HTTPS for all communications</li> <li>Store credentials securely - never hardcode passwords</li> <li>Implement proper error handling and retry logic</li> <li>Monitor API usage for performance optimization</li> <li>Use specific fields rather than returning all data</li> </ol> <p>For detailed examples and implementation guidance, refer to the Examples and Advanced Use Cases documentation.</p>"},{"location":"data-models/","title":"NetApp ActiveIQ API - Data Models","text":"<p>This document describes the key data models and schemas used in the NetApp ActiveIQ Unified Manager REST API.</p>"},{"location":"data-models/#overview","title":"Overview","text":"<p>The API uses JSON for all request and response payloads. All data models are based on OpenAPI 2.0 specification and include detailed property descriptions, data types, and constraints.</p>"},{"location":"data-models/#schema-overview-diagram","title":"Schema Overview Diagram","text":"<pre><code>classDiagram\n    class NetAppConfig {\n        +string base_url\n        +string username\n        +string password\n        +bool verify_ssl\n        +int timeout\n    }\n\n    class ClusterInfo {\n        +string cluster_id\n        +string name\n        +string version\n        +string health_status\n        +List~string~ nodes\n    }\n    c\n    class SVMConfig {\n        +string name\n        +string cluster_key\n        +string aggregate_name\n        +string root_volume\n        +string language\n        +string security_style\n    }\n\n    class NFSShareConfig {\n        +string name\n        +string svm_key\n        +string path\n        +string export_policy\n        +Dict~string, Any~ access_control\n    }\n\n    class MonitoringConfig {\n        +List~string~ cluster_keys\n        +List~string~ metrics\n        +Dict~string, float~ alert_thresholds\n        +List~string~ notification_channels\n    }\n\n    NetAppConfig --&gt; ClusterInfo : manages\n    ClusterInfo --&gt; SVMConfig : contains\n    SVMConfig --&gt; NFSShareConfig : hosts\n    ClusterInfo --&gt; MonitoringConfig : monitors</code></pre>"},{"location":"data-models/#common-properties","title":"Common Properties","text":"<p>Many objects in the API share common structural elements:</p>"},{"location":"data-models/#links-object-_links","title":"Links Object (<code>_links</code>)","text":"<p><pre><code>{\n  \"_links\": {\n    \"self\": {\n      \"href\": \"string\"\n    },\n    \"next\": {\n      \"href\": \"string\"\n    }\n  }\n}\n</code></pre> - self: Link to the current resource - next: Link to the next page (for paginated responses)</p>"},{"location":"data-models/#response-wrapper","title":"Response Wrapper","text":"<p>Most collection responses follow this pattern: <pre><code>{\n  \"_links\": {},\n  \"num_records\": 0,\n  \"records\": [],\n  \"total_records\": 0\n}\n</code></pre></p>"},{"location":"data-models/#core-data-models","title":"Core Data Models","text":""},{"location":"data-models/#1-access-endpoint","title":"1. Access Endpoint","text":"<p>Represents network access points for storage resources.</p> <pre><code>{\n  \"_links\": {},\n  \"data_protocols\": [\"nfs\", \"cifs\", \"iscsi\", \"fcp\"],\n  \"fileshare\": {\n    \"key\": \"string\"\n  },\n  \"gateway\": \"10.132.72.12\",\n  \"gateways\": [\"10.142.56.12\"],\n  \"ip\": {\n    \"address\": \"10.162.83.26\",\n    \"ha_address\": \"10.142.83.26\",\n    \"netmask\": \"255.255.0.0\"\n  },\n  \"key\": \"string\",\n  \"lun\": {\n    \"key\": \"string\"\n  },\n  \"mtu\": 15000,\n  \"name\": \"aep1\",\n  \"svm\": {\n    \"_links\": {},\n    \"key\": \"string\",\n    \"name\": \"svm1\",\n    \"uuid\": \"uuid\"\n  },\n  \"uuid\": \"uuid\",\n  \"vlan\": 10,\n  \"wwpn\": \"20:00:00:50:56:a7:bc:a2\"\n}\n</code></pre> <p>Key Properties: - data_protocols: Supported protocols (NFS, CIFS, iSCSI, FCP) - ip: Network configuration including address, netmask, and HA address - mtu: Maximum Transmission Unit - vlan: VLAN identifier - wwpn: World Wide Port Name (for FCP)</p>"},{"location":"data-models/#2-performance-metrics","title":"2. Performance Metrics","text":""},{"location":"data-models/#accumulative-metric","title":"Accumulative Metric","text":"<pre><code>{\n  \"other\": {},\n  \"read\": {},\n  \"total\": {},\n  \"write\": {}\n}\n</code></pre>"},{"location":"data-models/#accumulative-submetric","title":"Accumulative Submetric","text":"<pre><code>{\n  \"95th_percentile\": 28.0,\n  \"avg\": 28.0,\n  \"max\": 28.0,\n  \"min\": 28.0\n}\n</code></pre> <p>Performance Metrics Categories: - read: Read operation metrics - write: Write operation metrics - total: Combined read/write metrics - other: Other operation metrics</p>"},{"location":"data-models/#3-cluster-information","title":"3. Cluster Information","text":"<pre><code>{\n  \"_links\": {},\n  \"key\": \"string\",\n  \"name\": \"fas8040-206-21\",\n  \"uuid\": \"4c6bf721-2e3f-11e9-a3e2-00a0985badbb\",\n  \"version\": {\n    \"full\": \"NetApp Release Dayblazer__9.5.0: Thu Jan 17 10:28:33 UTC 2019\"\n  },\n  \"management_ip\": \"10.226.207.25\",\n  \"nodes\": [\n    {\n      \"uuid\": \"12cf06cc-2e3a-11e9-b9b4-00a0985badbb\",\n      \"name\": \"fas8040-206-21-01\",\n      \"model\": \"FAS8040\"\n    }\n  ]\n}\n</code></pre>"},{"location":"data-models/#4-storage-virtual-machine-svm","title":"4. Storage Virtual Machine (SVM)","text":"<pre><code>{\n  \"_links\": {},\n  \"key\": \"string\",\n  \"name\": \"svm1\",\n  \"uuid\": \"1d1c3198-fc57-11a8-99ca-00a078d39e12\"\n}\n</code></pre>"},{"location":"data-models/#5-storage-metrics","title":"5. Storage Metrics","text":""},{"location":"data-models/#space-usage","title":"Space Usage","text":"<pre><code>{\n  \"size\": {\n    \"available\": 1073741824,\n    \"total\": 2147483648,\n    \"used\": 1073741824\n  }\n}\n</code></pre>"},{"location":"data-models/#efficiency-metrics","title":"Efficiency Metrics","text":"<pre><code>{\n  \"efficiency\": {\n    \"compression\": {\n      \"savings\": 1073741824,\n      \"ratio\": \"2:1\"\n    },\n    \"deduplication\": {\n      \"savings\": 536870912,\n      \"ratio\": \"1.5:1\"\n    }\n  }\n}\n</code></pre>"},{"location":"data-models/#6-event-management","title":"6. Event Management","text":""},{"location":"data-models/#event-object","title":"Event Object","text":"<pre><code>{\n  \"_links\": {},\n  \"key\": \"string\",\n  \"name\": \"Event Name\",\n  \"message\": \"Event description\",\n  \"severity\": \"critical\",\n  \"state\": \"new\",\n  \"acknowledged\": false,\n  \"resolved\": false,\n  \"source\": {\n    \"key\": \"string\",\n    \"name\": \"Source Object\"\n  },\n  \"time\": \"2023-01-01T12:00:00Z\"\n}\n</code></pre> <p>Event Severities: - <code>critical</code> - <code>warning</code> - <code>information</code> - <code>error</code></p> <p>Event States: - <code>new</code> - <code>acknowledged</code> - <code>resolved</code> - <code>obsolete</code></p>"},{"location":"data-models/#7-job-management","title":"7. Job Management","text":""},{"location":"data-models/#job-object","title":"Job Object","text":"<pre><code>{\n  \"_links\": {},\n  \"uuid\": \"string\",\n  \"description\": \"Job description\",\n  \"state\": \"success\",\n  \"start_time\": \"2023-01-01T12:00:00Z\",\n  \"end_time\": \"2023-01-01T12:05:00Z\",\n  \"progress\": 100,\n  \"message\": \"Job completed successfully\"\n}\n</code></pre> <p>Job States: - <code>queued</code> - <code>running</code> - <code>paused</code> - <code>success</code> - <code>failure</code> - <code>partial_failures</code></p>"},{"location":"data-models/#8-performance-service-level","title":"8. Performance Service Level","text":"<pre><code>{\n  \"_links\": {},\n  \"key\": \"string\",\n  \"name\": \"Extreme\",\n  \"description\": \"High performance service level\",\n  \"expected_iops\": {\n    \"allocation\": \"allocated_space\",\n    \"peak\": 10000,\n    \"per_tb\": 1000\n  },\n  \"peak_iops\": {\n    \"allocation\": \"allocated_space\",\n    \"absolute\": 50000,\n    \"per_tb\": 5000\n  },\n  \"expected_latency\": 1,\n  \"peak_latency\": 2,\n  \"block_size\": \"any\"\n}\n</code></pre>"},{"location":"data-models/#9-backup-information","title":"9. Backup Information","text":""},{"location":"data-models/#backup-settings","title":"Backup Settings","text":"<pre><code>{\n  \"enabled\": true,\n  \"frequency\": \"daily\",\n  \"hour\": 1,\n  \"minute\": 17,\n  \"day_of_week\": null,\n  \"retention_count\": 10,\n  \"path\": \"/opt/netapp/data/ocum-backup/\"\n}\n</code></pre>"},{"location":"data-models/#backup-file-info","title":"Backup File Info","text":"<pre><code>{\n  \"name\": \"backup_20230101_120000.sql\",\n  \"path\": \"/opt/netapp/data/ocum-backup/backup_20230101_120000.sql\",\n  \"size\": 1073741824,\n  \"creation_time\": \"2023-01-01T12:00:00Z\",\n  \"type\": \"mysql\"\n}\n</code></pre>"},{"location":"data-models/#common-data-types","title":"Common Data Types","text":""},{"location":"data-models/#primitive-types","title":"Primitive Types","text":"<ul> <li>string: Text values</li> <li>integer: Whole numbers</li> <li>number/double: Decimal numbers</li> <li>boolean: true/false values</li> <li>uuid: UUID format strings</li> <li>int64: 64-bit integers</li> </ul>"},{"location":"data-models/#datetime-format","title":"Date/Time Format","text":"<p>All timestamps use ISO 8601 format: <pre><code>YYYY-MM-DDTHH:mm:ssZ\n</code></pre></p>"},{"location":"data-models/#enumerated-values","title":"Enumerated Values","text":"<p>Many fields use predefined enumerated values for consistency:</p> <p>Protocols: - <code>nfs</code> - <code>cifs</code> - <code>iscsi</code> - <code>fcp</code></p> <p>Frequencies: - <code>daily</code> - <code>weekly</code></p> <p>Allocation Types: - <code>allocated_space</code> - <code>used_space</code></p>"},{"location":"data-models/#error-response-format","title":"Error Response Format","text":"<p>All error responses follow this structure: <pre><code>{\n  \"error\": {\n    \"message\": \"Error description\",\n    \"code\": \"ERROR_CODE\",\n    \"target\": \"field_name\"\n  }\n}\n</code></pre></p>"},{"location":"data-models/#pagination","title":"Pagination","text":"<p>Collection responses include pagination metadata: <pre><code>{\n  \"num_records\": 20,\n  \"total_records\": 150,\n  \"_links\": {\n    \"self\": {\"href\": \"/api/v2/endpoint?offset=0&amp;max_records=20\"},\n    \"next\": {\"href\": \"/api/v2/endpoint?offset=20&amp;max_records=20\"}\n  }\n}\n</code></pre></p>"},{"location":"data-models/#field-selection","title":"Field Selection","text":"<p>Use the <code>fields</code> parameter to specify which properties to include in responses: <pre><code>GET /api/v2/datacenter/cluster/clusters?fields=name,uuid,version\n</code></pre></p> <p>This returns only the specified fields, reducing response size and improving performance.</p> <p>For complete schema definitions and additional models, refer to the OpenAPI specification available through your Unified Manager Swagger interface.</p>"},{"location":"examples/","title":"NetApp ActiveIQ API - Examples and Use Cases","text":"<p>This document provides practical examples and common use cases for the NetApp ActiveIQ Unified Manager REST API.</p>"},{"location":"examples/#prerequisites","title":"Prerequisites","text":"<ul> <li>NetApp Active IQ Unified Manager instance running</li> <li>Valid credentials with appropriate roles (Operator, Storage Administrator, or Application Administrator)</li> <li>Basic understanding of REST API concepts</li> </ul>"},{"location":"examples/#authentication","title":"Authentication","text":"<p>All examples use HTTP Basic Authentication. Replace <code>&lt;username&gt;</code>, <code>&lt;password&gt;</code>, and <code>&lt;um-host&gt;</code> with your actual values.</p> <pre><code># Using curl\ncurl -u \"&lt;username&gt;:&lt;password&gt;\" -X GET \"https://&lt;um-host&gt;/api/v2/datacenter/cluster/clusters\"\n\n# Using Python requests\nimport requests\nfrom requests.auth import HTTPBasicAuth\n\nauth = HTTPBasicAuth('&lt;username&gt;', '&lt;password&gt;')\nresponse = requests.get('https://&lt;um-host&gt;/api/v2/datacenter/cluster/clusters', auth=auth)\n</code></pre>"},{"location":"examples/#common-use-cases","title":"Common Use Cases","text":""},{"location":"examples/#1-infrastructure-discovery","title":"1. Infrastructure Discovery","text":""},{"location":"examples/#get-all-clusters","title":"Get All Clusters","text":"<pre><code># Basic cluster information\ncurl -u \"admin:password\" -X GET \\\n  \"https://um-server.example.com/api/v2/datacenter/cluster/clusters\"\n\n# Get specific fields only\ncurl -u \"admin:password\" -X GET \\\n  \"https://um-server.example.com/api/v2/datacenter/cluster/clusters?fields=name,uuid,version,management_ip\"\n</code></pre> <p>Python Example: <pre><code>import requests\nfrom requests.auth import HTTPBasicAuth\n\ndef get_clusters(um_host, username, password):\n    url = f\"https://{um_host}/api/v2/datacenter/cluster/clusters\"\n    params = {\"fields\": \"name,uuid,version,management_ip\"}\n\n    response = requests.get(url, auth=HTTPBasicAuth(username, password), params=params)\n\n    if response.status_code == 200:\n        return response.json()\n    else:\n        print(f\"Error: {response.status_code} - {response.text}\")\n        return None\n\nclusters = get_clusters(\"um-server.example.com\", \"admin\", \"password\")\nfor cluster in clusters.get('records', []):\n    print(f\"Cluster: {cluster['name']} - Version: {cluster['version']['full']}\")\n</code></pre></p>"},{"location":"examples/#get-all-storage-virtual-machines-svms","title":"Get All Storage Virtual Machines (SVMs)","text":"<pre><code>curl -u \"admin:password\" -X GET \\\n  \"https://um-server.example.com/api/v2/datacenter/svm/svms?fields=name,uuid,cluster\"\n</code></pre>"},{"location":"examples/#2-storage-monitoring","title":"2. Storage Monitoring","text":""},{"location":"examples/#get-volume-information","title":"Get Volume Information","text":"<pre><code># Get all volumes with capacity information\ncurl -u \"admin:password\" -X GET \\\n  \"https://um-server.example.com/api/v2/datacenter/storage/volumes?fields=name,size,svm,cluster\"\n\n# Get volumes with low available space (example query)\ncurl -u \"admin:password\" -X GET \\\n  \"https://um-server.example.com/api/v2/datacenter/storage/volumes?query=size.available&lt;1073741824\"\n</code></pre> <p>Python Example - Storage Capacity Report: <pre><code>def generate_storage_report(um_host, username, password):\n    url = f\"https://{um_host}/api/v2/datacenter/storage/volumes\"\n    params = {\n        \"fields\": \"name,size,svm.name,cluster.name\",\n        \"max_records\": 100\n    }\n\n    response = requests.get(url, auth=HTTPBasicAuth(username, password), params=params)\n\n    if response.status_code == 200:\n        volumes = response.json()\n\n        print(\"Storage Capacity Report\")\n        print(\"-\" * 60)\n        print(f\"{'Volume':&lt;20} {'Cluster':&lt;15} {'SVM':&lt;15} {'Size (GB)':&lt;10} {'Available (GB)':&lt;15}\")\n        print(\"-\" * 60)\n\n        for volume in volumes.get('records', []):\n            size_gb = volume['size']['total'] / (1024**3)\n            available_gb = volume['size']['available'] / (1024**3)\n\n            print(f\"{volume['name']:&lt;20} {volume['cluster']['name']:&lt;15} \"\n                  f\"{volume['svm']['name']:&lt;15} {size_gb:&lt;10.2f} {available_gb:&lt;15.2f}\")\n</code></pre></p>"},{"location":"examples/#3-performance-monitoring","title":"3. Performance Monitoring","text":""},{"location":"examples/#get-cluster-performance-metrics","title":"Get Cluster Performance Metrics","text":"<pre><code>curl -u \"admin:password\" -X GET \\\n  \"https://um-server.example.com/api/v2/gateways/clusters/{cluster_uuid}/metrics/clusters/perf\"\n</code></pre>"},{"location":"examples/#get-volume-performance-data","title":"Get Volume Performance Data","text":"<pre><code>curl -u \"admin:password\" -X GET \\\n  \"https://um-server.example.com/api/v2/gateways/clusters/{cluster_uuid}/metrics/volumes/perf?duration=1h&amp;interval=5m\"\n</code></pre>"},{"location":"examples/#4-event-management","title":"4. Event Management","text":""},{"location":"examples/#get-all-critical-events","title":"Get All Critical Events","text":"<pre><code>curl -u \"admin:password\" -X GET \\\n  \"https://um-server.example.com/api/v2/management-server/events?query=severity:critical&amp;fields=name,message,severity,state,time\"\n</code></pre>"},{"location":"examples/#acknowledge-an-event","title":"Acknowledge an Event","text":"<pre><code>curl -u \"admin:password\" -X POST \\\n  \"https://um-server.example.com/api/v2/management-server/events/{event_key}/acknowledge\"\n</code></pre>"},{"location":"examples/#python-example-event-dashboard","title":"Python Example - Event Dashboard","text":"<pre><code>def get_critical_events(um_host, username, password):\n    url = f\"https://{um_host}/api/v2/management-server/events\"\n    params = {\n        \"query\": \"severity:critical AND state:new\",\n        \"fields\": \"name,message,severity,state,time,source\",\n        \"order_by\": \"time desc\",\n        \"max_records\": 50\n    }\n\n    response = requests.get(url, auth=HTTPBasicAuth(username, password), params=params)\n\n    if response.status_code == 200:\n        events = response.json()\n\n        print(\"Critical Events Dashboard\")\n        print(\"=\" * 80)\n\n        for event in events.get('records', []):\n            print(f\"Event: {event['name']}\")\n            print(f\"Source: {event['source']['name']}\")\n            print(f\"Time: {event['time']}\")\n            print(f\"Message: {event['message']}\")\n            print(\"-\" * 40)\n\n    return events\n</code></pre>"},{"location":"examples/#5-backup-management","title":"5. Backup Management","text":""},{"location":"examples/#create-a-backup","title":"Create a Backup","text":"<pre><code>curl -u \"admin:password\" -X POST \\\n  \"https://um-server.example.com/api/v2/admin/backup\"\n</code></pre>"},{"location":"examples/#get-backup-settings","title":"Get Backup Settings","text":"<pre><code>curl -u \"admin:password\" -X GET \\\n  \"https://um-server.example.com/api/v2/admin/backup-settings\"\n</code></pre>"},{"location":"examples/#update-backup-schedule","title":"Update Backup Schedule","text":"<pre><code>curl -u \"admin:password\" -X PATCH \\\n  \"https://um-server.example.com/api/v2/admin/backup-settings\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"enabled\": true,\n    \"frequency\": \"daily\",\n    \"hour\": 2,\n    \"minute\": 30,\n    \"retention_count\": 7\n  }'\n</code></pre>"},{"location":"examples/#6-datasource-management","title":"6. Datasource Management","text":""},{"location":"examples/#add-a-new-cluster","title":"Add a New Cluster","text":"<pre><code>curl -u \"admin:password\" -X POST \\\n  \"https://um-server.example.com/api/v2/admin/datasources/clusters\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"address\": \"10.226.207.154\",\n    \"port\": 443,\n    \"username\": \"admin\",\n    \"password\": \"cluster_password\",\n    \"protocol\": \"HTTPS\"\n  }'\n</code></pre>"},{"location":"examples/#get-datasource-certificate","title":"Get Datasource Certificate","text":"<pre><code>curl -u \"admin:password\" -X GET \\\n  \"https://um-server.example.com/api/v2/admin/datasource-certificate?address=10.226.207.154&amp;port=443\"\n</code></pre>"},{"location":"examples/#7-workload-management","title":"7. Workload Management","text":""},{"location":"examples/#create-a-performance-service-level","title":"Create a Performance Service Level","text":"<pre><code>curl -u \"admin:password\" -X POST \\\n  \"https://um-server.example.com/api/v2/storage-provider/performance-service-levels\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"High Performance SSD\",\n    \"description\": \"High performance for critical workloads\",\n    \"expected_iops\": {\n      \"allocation\": \"allocated_space\",\n      \"per_tb\": 1000\n    },\n    \"peak_iops\": {\n      \"allocation\": \"allocated_space\",\n      \"per_tb\": 5000\n    },\n    \"expected_latency\": 1,\n    \"peak_latency\": 2\n  }'\n</code></pre>"},{"location":"examples/#create-a-file-share","title":"Create a File Share","text":"<pre><code>curl -u \"admin:password\" -X POST \\\n  \"https://um-server.example.com/api/v2/storage-provider/file-shares\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"shared_data\",\n    \"size\": \"100GB\",\n    \"svm\": {\n      \"key\": \"svm-key-here\"\n    },\n    \"performance_service_level\": {\n      \"key\": \"psl-key-here\"\n    }\n  }'\n</code></pre>"},{"location":"examples/#practical-automation-scripts","title":"Practical Automation Scripts","text":""},{"location":"examples/#health-check-script","title":"Health Check Script","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nNetApp UM Health Check Script\nChecks cluster health, critical events, and storage capacity\n\"\"\"\n\nimport requests\nfrom requests.auth import HTTPBasicAuth\nimport json\nfrom datetime import datetime\n\nclass NetAppHealthCheck:\n    def __init__(self, um_host, username, password):\n        self.um_host = um_host\n        self.auth = HTTPBasicAuth(username, password)\n        self.base_url = f\"https://{um_host}/api/v2\"\n\n    def check_cluster_health(self):\n        \"\"\"Check overall cluster health\"\"\"\n        url = f\"{self.base_url}/datacenter/cluster/clusters\"\n        params = {\"fields\": \"name,state,health\"}\n\n        response = requests.get(url, auth=self.auth, params=params)\n        if response.status_code == 200:\n            clusters = response.json()\n\n            healthy_clusters = []\n            unhealthy_clusters = []\n\n            for cluster in clusters.get('records', []):\n                if cluster.get('health', {}).get('overall_status') == 'healthy':\n                    healthy_clusters.append(cluster['name'])\n                else:\n                    unhealthy_clusters.append(cluster['name'])\n\n            return {\n                'healthy': healthy_clusters,\n                'unhealthy': unhealthy_clusters\n            }\n        return None\n\n    def check_critical_events(self):\n        \"\"\"Get all unresolved critical events\"\"\"\n        url = f\"{self.base_url}/management-server/events\"\n        params = {\n            \"query\": \"severity:critical AND state:new\",\n            \"fields\": \"name,source.name,time\"\n        }\n\n        response = requests.get(url, auth=self.auth, params=params)\n        if response.status_code == 200:\n            return response.json().get('records', [])\n        return []\n\n    def check_storage_capacity(self, threshold_percent=90):\n        \"\"\"Check volumes approaching capacity threshold\"\"\"\n        url = f\"{self.base_url}/datacenter/storage/volumes\"\n        params = {\"fields\": \"name,size,svm.name,cluster.name\"}\n\n        response = requests.get(url, auth=self.auth, params=params)\n        if response.status_code == 200:\n            volumes = response.json()\n            high_capacity_volumes = []\n\n            for volume in volumes.get('records', []):\n                size = volume.get('size', {})\n                if size.get('total', 0) &gt; 0:\n                    used_percent = (size.get('used', 0) / size.get('total', 1)) * 100\n                    if used_percent &gt; threshold_percent:\n                        high_capacity_volumes.append({\n                            'name': volume['name'],\n                            'cluster': volume['cluster']['name'],\n                            'svm': volume['svm']['name'],\n                            'used_percent': round(used_percent, 2)\n                        })\n\n            return high_capacity_volumes\n        return []\n\n    def generate_report(self):\n        \"\"\"Generate comprehensive health report\"\"\"\n        print(\"NetApp Unified Manager Health Check Report\")\n        print(\"=\" * 60)\n        print(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n        print()\n\n        # Cluster Health\n        cluster_health = self.check_cluster_health()\n        if cluster_health:\n            print(\"Cluster Health:\")\n            print(f\"  Healthy: {len(cluster_health['healthy'])} clusters\")\n            print(f\"  Unhealthy: {len(cluster_health['unhealthy'])} clusters\")\n            if cluster_health['unhealthy']:\n                print(f\"  Unhealthy clusters: {', '.join(cluster_health['unhealthy'])}\")\n            print()\n\n        # Critical Events\n        critical_events = self.check_critical_events()\n        print(f\"Critical Events: {len(critical_events)} unresolved\")\n        for event in critical_events[:5]:  # Show first 5\n            print(f\"  - {event['name']} on {event['source']['name']}\")\n        print()\n\n        # Storage Capacity\n        high_capacity = self.check_storage_capacity()\n        print(f\"High Capacity Volumes (&gt;90%): {len(high_capacity)}\")\n        for volume in high_capacity[:5]:  # Show first 5\n            print(f\"  - {volume['name']} ({volume['used_percent']}%) on {volume['cluster']}\")\n\n# Usage\nif __name__ == \"__main__\":\n    health_check = NetAppHealthCheck(\"um-server.example.com\", \"admin\", \"password\")\n    health_check.generate_report()\n</code></pre>"},{"location":"examples/#error-handling-best-practices","title":"Error Handling Best Practices","text":"<pre><code>def make_api_request(url, auth, method='GET', data=None, retries=3):\n    \"\"\"Make API request with proper error handling and retries\"\"\"\n\n    for attempt in range(retries):\n        try:\n            if method == 'GET':\n                response = requests.get(url, auth=auth, timeout=30)\n            elif method == 'POST':\n                response = requests.post(url, auth=auth, json=data, timeout=30)\n            elif method == 'PATCH':\n                response = requests.patch(url, auth=auth, json=data, timeout=30)\n\n            # Handle different response codes\n            if response.status_code == 200:\n                return response.json()\n            elif response.status_code == 401:\n                print(\"Authentication failed - check credentials\")\n                return None\n            elif response.status_code == 403:\n                print(\"Access forbidden - check user permissions\")\n                return None\n            elif response.status_code == 404:\n                print(\"Resource not found\")\n                return None\n            elif response.status_code &gt;= 500:\n                print(f\"Server error: {response.status_code}\")\n                if attempt &lt; retries - 1:\n                    print(f\"Retrying... (attempt {attempt + 2}/{retries})\")\n                    time.sleep(2 ** attempt)  # Exponential backoff\n                continue\n            else:\n                print(f\"Unexpected status code: {response.status_code}\")\n                print(f\"Response: {response.text}\")\n                return None\n\n        except requests.exceptions.Timeout:\n            print(f\"Request timeout (attempt {attempt + 1}/{retries})\")\n            if attempt &lt; retries - 1:\n                time.sleep(2 ** attempt)\n                continue\n        except requests.exceptions.ConnectionError:\n            print(f\"Connection error (attempt {attempt + 1}/{retries})\")\n            if attempt &lt; retries - 1:\n                time.sleep(2 ** attempt)\n                continue\n\n    print(f\"Failed after {retries} attempts\")\n    return None\n</code></pre>"},{"location":"examples/#tips-and-best-practices","title":"Tips and Best Practices","text":"<ol> <li>Use field selection to reduce response size and improve performance</li> <li>Implement pagination for large datasets using <code>max_records</code> and <code>offset</code></li> <li>Handle rate limiting with appropriate delays between requests</li> <li>Use HTTPS for all communications</li> <li>Store credentials securely - never hardcode passwords</li> <li>Validate SSL certificates in production environments</li> <li>Implement proper error handling and retry logic</li> <li>Log API interactions for debugging and audit purposes</li> </ol> <p>For more examples and detailed parameter information, refer to the interactive Swagger documentation at: <pre><code>https://&lt;your-unified-manager&gt;/apidocs/\n</code></pre></p>"},{"location":"navigation/","title":"Documentation Navigation Guide","text":""},{"location":"navigation/#reading-path-recommendations","title":"\ud83d\udcda Reading Path Recommendations","text":""},{"location":"navigation/#for-beginners","title":"For Beginners","text":"<ol> <li>Start Here: Main Index - Overview and quick start</li> <li>Learn the Basics: API Overview - Core concepts and authentication</li> <li>Explore APIs: API Endpoints - Available endpoints and operations</li> <li>Try Examples: Basic Examples - Simple automation scripts</li> </ol>"},{"location":"navigation/#for-intermediate-users","title":"For Intermediate Users","text":"<ol> <li>Data Understanding: Data Models - JSON structures and schemas</li> <li>Advanced Examples: Examples and Use Cases - Complete automation scripts</li> <li>Complex Workflows: Advanced Use Cases - SVM and NFS management</li> </ol>"},{"location":"navigation/#for-advanced-users-architects","title":"For Advanced Users / Architects","text":"<ol> <li>Workflow Design: Advanced Use Cases - Sequence diagrams and complex automation</li> <li>Reference Material: API Endpoints + Data Models</li> <li>Production Scripts: Focus on the Python classes and error handling patterns</li> </ol>"},{"location":"navigation/#use-case-navigation","title":"\ud83c\udfaf Use Case Navigation","text":""},{"location":"navigation/#infrastructure-management","title":"Infrastructure Management","text":"<ul> <li>Getting Started: API Overview</li> <li>Cluster Discovery: Examples - Infrastructure Discovery</li> <li>Health Monitoring: Examples - Health Check Script</li> </ul>"},{"location":"navigation/#storage-operations","title":"Storage Operations","text":"<ul> <li>Basic Operations: Examples - Storage Monitoring</li> <li>File Share Creation: Advanced Use Cases - NFS Share Management</li> <li>SVM Management: Advanced Use Cases - SVM Creation</li> </ul>"},{"location":"navigation/#security-credentials","title":"Security &amp; Credentials","text":"<ul> <li>Authentication Setup: API Overview - Authentication</li> <li>Credential Management: Advanced Use Cases - NFS Credential Updates</li> <li>Security Best Practices: Advanced Use Cases - Best Practices</li> </ul>"},{"location":"navigation/#automation-integration","title":"Automation &amp; Integration","text":"<ul> <li>Basic Automation: Examples - Automation Scripts</li> <li>Advanced Workflows: Advanced Use Cases - Complete Automation</li> <li>Error Handling: Examples - Error Handling</li> </ul>"},{"location":"navigation/#quick-reference","title":"\ud83d\udd0d Quick Reference","text":""},{"location":"navigation/#api-endpoints-by-category","title":"API Endpoints by Category","text":"Category Documentation Section Administration API Endpoints - Administration Datacenter API Endpoints - Datacenter Storage Provider API Endpoints - Storage Provider Management Server API Endpoints - Management Server Gateways API Endpoints - Gateways"},{"location":"navigation/#common-data-models","title":"Common Data Models","text":"Model Documentation Section Access Endpoints Data Models - Access Endpoint Performance Metrics Data Models - Performance Metrics Cluster Information Data Models - Cluster Information Storage Metrics Data Models - Storage Metrics Event Management Data Models - Event Management"},{"location":"navigation/#sequence-diagrams","title":"Sequence Diagrams","text":"Workflow Diagram Location SVM Creation Advanced Use Cases - SVM Creation Sequence NFS Share Creation Advanced Use Cases - NFS Share Creation Sequence Credential Updates Advanced Use Cases - NFS Credential Update Sequence Complete Workflow Advanced Use Cases - Complete Workflow Integration"},{"location":"navigation/#implementation-guides","title":"\ud83d\udee0\ufe0f Implementation Guides","text":""},{"location":"navigation/#python-development","title":"Python Development","text":"<ol> <li>Basic Setup: Examples - Authentication</li> <li>Class Examples: Examples - Health Check Script</li> <li>Advanced Classes: Advanced Use Cases - Python Implementation</li> <li>Error Handling: Examples - Error Handling Best Practices</li> </ol>"},{"location":"navigation/#curl-examples","title":"cURL Examples","text":"<ol> <li>Basic Commands: Examples - Common Use Cases</li> <li>Complex Operations: Advanced Use Cases - Step-by-Step</li> </ol>"},{"location":"navigation/#integration-patterns","title":"Integration Patterns","text":"<ol> <li>Health Monitoring: Examples - Health Check Script</li> <li>Automated Deployment: Advanced Use Cases - Master Deployment Script</li> <li>Event Management: Examples - Event Dashboard</li> </ol>"},{"location":"navigation/#learning-progression","title":"\ud83d\udcd6 Learning Progression","text":""},{"location":"navigation/#phase-1-foundation-30-minutes","title":"Phase 1: Foundation (30 minutes)","text":"<ul> <li> Read Main Index</li> <li> Review API Overview</li> <li> Try basic cURL commands from Examples</li> </ul>"},{"location":"navigation/#phase-2-practical-application-1-2-hours","title":"Phase 2: Practical Application (1-2 hours)","text":"<ul> <li> Explore API Endpoints</li> <li> Study Data Models</li> <li> Run Python examples from Examples</li> </ul>"},{"location":"navigation/#phase-3-advanced-implementation-2-4-hours","title":"Phase 3: Advanced Implementation (2-4 hours)","text":"<ul> <li> Review Advanced Use Cases</li> <li> Study sequence diagrams</li> <li> Implement SVM creation workflow</li> </ul>"},{"location":"navigation/#phase-4-production-readiness-4-hours","title":"Phase 4: Production Readiness (4+ hours)","text":"<ul> <li> Implement error handling patterns</li> <li> Create custom automation scripts</li> <li> Design complete workflows with proper validation</li> </ul>"},{"location":"navigation/#getting-help","title":"\ud83d\udcde Getting Help","text":""},{"location":"navigation/#documentation-issues","title":"Documentation Issues","text":"<ul> <li>Check the README for source information</li> <li>Review external links in each section</li> </ul>"},{"location":"navigation/#api-issues","title":"API Issues","text":"<ul> <li>Consult the interactive Swagger documentation at <code>https://&lt;your-um-host&gt;/apidocs/</code></li> <li>Check official NetApp documentation links provided</li> </ul>"},{"location":"navigation/#implementation-issues","title":"Implementation Issues","text":"<ul> <li>Review error handling examples</li> <li>Check authentication configuration</li> <li>Validate endpoint URLs and parameters</li> </ul> <p>This navigation guide helps you find the right documentation section based on your role, use case, and experience level.</p>"},{"location":"netapp-activeiq-api-overview/","title":"NetApp ActiveIQ API - Overview","text":""},{"location":"netapp-activeiq-api-overview/#introduction","title":"Introduction","text":"<p>This documentation provides an overview of the NetApp ActiveIQ REST API based on the analysis of external data sources. The NetApp ActiveIQ API suite provides programmatic access to manage, monitor, and extract data from NetApp storage environments through Active IQ Unified Manager and Digital Advisor.</p>"},{"location":"netapp-activeiq-api-overview/#api-information","title":"API Information","text":"<ul> <li>Title: Active IQ Unified Manager - API Documentation</li> <li>Version: v2</li> <li>Base Path: <code>/api</code></li> <li>Support: NetApp Support - https://mysupport.netapp.com</li> </ul>"},{"location":"netapp-activeiq-api-overview/#key-features","title":"Key Features","text":""},{"location":"netapp-activeiq-api-overview/#restful-design","title":"RESTful Design","text":"<ul> <li>Follows REST principles</li> <li>Supports standard HTTP methods (GET, POST, PATCH, DELETE)</li> <li>Full CRUD operations on storage resources</li> <li>Content-Type: <code>application/json</code></li> </ul>"},{"location":"netapp-activeiq-api-overview/#comprehensive-coverage","title":"Comprehensive Coverage","text":"<ul> <li>Over 100 endpoints grouped into 20+ service areas</li> <li>Coverage includes:</li> <li>System information</li> <li>Storage efficiency</li> <li>Performance monitoring</li> <li>Health status</li> <li>Upgrades management</li> <li>Backup operations</li> <li>Access endpoints</li> <li>Performance service levels</li> <li>Workload management</li> </ul>"},{"location":"netapp-activeiq-api-overview/#use-cases","title":"Use Cases","text":"<ul> <li>Automate monitoring and management</li> <li>Integration with ticketing/reporting systems</li> <li>Build custom dashboards</li> <li>Manage storage resources programmatically</li> <li>Risk detection and health checks</li> <li>Data extraction for analytics</li> </ul>"},{"location":"netapp-activeiq-api-overview/#authentication-access","title":"Authentication &amp; Access","text":""},{"location":"netapp-activeiq-api-overview/#endpoint-structure","title":"Endpoint Structure","text":"<pre><code>https://&lt;hostname&gt;:&lt;port&gt;/api/v2/&lt;service&gt;/&lt;resource&gt;\n</code></pre> <p>Default Configuration: - Port: 443 (HTTPS) - Authentication: HTTP Basic Authentication (username/password) - SSL: Supports self-signed or custom SSL certificates - User types: Both local and LDAP users supported</p>"},{"location":"netapp-activeiq-api-overview/#required-roles","title":"Required Roles","text":"<p>To access the API documentation and endpoints, users must have one of the following roles: - Operator - Storage Administrator - Application Administrator</p>"},{"location":"netapp-activeiq-api-overview/#api-documentation-access","title":"API Documentation Access","text":""},{"location":"netapp-activeiq-api-overview/#interactive-documentation-swagger-ui","title":"Interactive Documentation (Swagger UI)","text":"<p>Access the interactive API documentation at: <pre><code>https://&lt;Unified_Manager_IP_or_FQDN&gt;/apidocs/\n</code></pre> or <pre><code>https://&lt;Unified_Manager_IP_or_FQDN&gt;/docs/api/\n</code></pre></p> <p>The Swagger UI provides: - Code samples - \"Try it out\" browser experience - Input/output parameter details - Live testing capabilities</p>"},{"location":"netapp-activeiq-api-overview/#openapi-specification","title":"OpenAPI Specification","text":"<p>The OpenAPI (Swagger) specification is not available for public download but can be retrieved from your Unified Manager instance: 1. Access the Swagger UI 2. Use browser developer tools to capture the <code>/swagger.json</code> or <code>/openapi.json</code> request 3. Save the JSON content for local use</p>"},{"location":"netapp-activeiq-api-overview/#development-support","title":"Development Support","text":""},{"location":"netapp-activeiq-api-overview/#client-libraries","title":"Client Libraries","text":"<p>Any REST client or programming language can interact with the API: - Python - Perl - Java - cURL - Any HTTP client library</p>"},{"location":"netapp-activeiq-api-overview/#automation-integration","title":"Automation Integration","text":"<p>The APIs are designed for integration into automation workflows, enabling: - Scripted health checks - Automated risk detection - Scheduled data extraction - Workflow automation</p>"},{"location":"netapp-activeiq-api-overview/#common-parameters","title":"Common Parameters","text":"<p>The API supports several common query parameters:</p> <ul> <li><code>fields</code>: Specify which fields to return</li> <li><code>max_records</code>: Limit the number of records returned</li> <li><code>offset</code>: Start index for pagination</li> <li><code>order_by</code>: Sort results by specified field</li> <li><code>query</code>: Search using 'contains' relationship</li> <li><code>return_records</code>: Control whether to return record data or just counts</li> </ul>"},{"location":"netapp-activeiq-api-overview/#data-models","title":"Data Models","text":"<p>The API defines numerous data models for different resources including: - Access endpoints (with IP, gateway, VLAN configurations) - Performance metrics (with accumulative statistics) - Storage metrics - Cluster information - SVM (Storage Virtual Machine) details - Performance service levels - Workload management</p>"},{"location":"netapp-activeiq-api-overview/#next-steps","title":"Next Steps","text":"<ul> <li>API Endpoints Reference - Detailed list of available endpoints</li> <li>Data Models - Complete data model documentation</li> <li>Examples - Code examples and use cases</li> </ul>"},{"location":"netapp-activeiq-api-overview/#resources","title":"Resources","text":"<ul> <li>Official NetApp ActiveIQ Documentation</li> <li>Active IQ Unified Manager API Automation Guide</li> <li>Digital Advisor API Services</li> </ul>"},{"location":"temporal-integration/","title":"Temporal.io Integration for NetApp ActiveIQ API","text":""},{"location":"temporal-integration/#overview","title":"Overview","text":"<p>This document provides comprehensive guidance for integrating Temporal.io workflows with the NetApp ActiveIQ API, based on the sequence diagrams and use cases defined in our advanced documentation. Temporal.io provides durable execution, automatic retries, and state management for complex NetApp infrastructure workflows.</p>"},{"location":"temporal-integration/#why-temporalio-for-netapp-operations","title":"Why Temporal.io for NetApp Operations?","text":"<ul> <li>Durability: Workflows survive process restarts and failures</li> <li>Reliability: Automatic retries and error handling</li> <li>Observability: Built-in monitoring and debugging capabilities</li> <li>Scalability: Handle thousands of concurrent operations</li> <li>State Management: Automatic persistence of workflow state</li> </ul>"},{"location":"temporal-integration/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker installed and running</li> <li>Python 3.10+ (recommended: Python 3.12)</li> <li>NetApp ActiveIQ Unified Manager access</li> <li>Valid API credentials</li> </ul>"},{"location":"temporal-integration/#quick-start-guide","title":"Quick Start Guide","text":""},{"location":"temporal-integration/#step-1-install-temporal-server","title":"Step 1: Install Temporal Server","text":"<pre><code># Pull and run Temporal Server with auto-setup\ndocker run -d --name temporalio -p 7233:7233 -p 8080:8080 temporalio/auto-setup\n\n# Verify installation\ncurl http://localhost:8080\n</code></pre>"},{"location":"temporal-integration/#step-2-install-dependencies","title":"Step 2: Install Dependencies","text":"<pre><code># Create virtual environment\npython3 -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install required packages\npip install temporalio requests dataclasses-json\n</code></pre>"},{"location":"temporal-integration/#step-3-environment-setup","title":"Step 3: Environment Setup","text":"<p>Create a <code>.env</code> file for configuration:</p> <pre><code># NetApp ActiveIQ Configuration\nNETAPP_UM_HOST=your-unified-manager.company.com\nNETAPP_USERNAME=admin\nNETAPP_PASSWORD=your-secure-password\n\n# Temporal Configuration\nTEMPORAL_HOST=localhost:7233\nTEMPORAL_NAMESPACE=default\nTASK_QUEUE=netapp-activeiq\n</code></pre>"},{"location":"temporal-integration/#workflow-implementations-based-on-sequence-diagrams","title":"Workflow Implementations Based on Sequence Diagrams","text":""},{"location":"temporal-integration/#1-svm-creation-workflow","title":"1. SVM Creation Workflow","text":"<p>Based on the SVM Creation Sequence Diagram, here's the complete implementation:</p> <pre><code># netapp_workflows.py\nfrom datetime import timedelta\nfrom typing import Dict, List, Optional\nfrom dataclasses import dataclass\nimport asyncio\nimport logging\nimport requests\nfrom requests.auth import HTTPBasicAuth\n\nfrom temporalio import workflow, activity\nfrom temporalio.common import RetryPolicy\n\n@dataclass\nclass SVMConfig:\n    name: str\n    cluster_key: str\n    aggregate_name: Optional[str] = None\n    root_volume: str = \"root\"\n    language: str = \"c.utf_8\"\n    security_style: str = \"unix\"\n    protocols: List[str] = None\n\n    def __post_init__(self):\n        if self.protocols is None:\n            self.protocols = [\"nfs\"]\n\n@dataclass\nclass NetworkInterface:\n    name: str\n    ip_address: str\n    netmask: str\n    node_uuid: str\n    port_name: str\n    service_policy: str\n\n# Activities for NetApp API Operations\n@activity.defn\nasync def validate_cluster_health(cluster_key: str, um_host: str, auth: tuple) -&gt; Dict:\n    \"\"\"Step 1: Validate cluster health before SVM creation\"\"\"\n    url = f\"https://{um_host}/api/v2/datacenter/cluster/clusters/{cluster_key}\"\n    params = {\"fields\": \"name,state,health,version\"}\n\n    response = requests.get(\n        url,\n        auth=HTTPBasicAuth(*auth),\n        params=params,\n        verify=False,  # For self-signed certificates\n        timeout=30\n    )\n\n    if response.status_code != 200:\n        raise Exception(f\"Failed to validate cluster: {response.status_code} - {response.text}\")\n\n    cluster = response.json()\n    if cluster.get('state') != 'up':\n        raise Exception(f\"Cluster {cluster.get('name')} is not up: {cluster.get('state')}\")\n\n    activity.logger.info(f\"\u2713 Cluster {cluster['name']} is healthy and ready\")\n    return cluster\n\n@activity.defn\nasync def get_available_aggregates(cluster_key: str, um_host: str, auth: tuple) -&gt; List[Dict]:\n    \"\"\"Step 2: Get available aggregates for SVM creation\"\"\"\n    url = f\"https://{um_host}/api/v2/datacenter/storage/aggregates\"\n    params = {\n        \"query\": f\"cluster.key:{cluster_key}\",\n        \"fields\": \"name,key,space.size,space.available,state\"\n    }\n\n    response = requests.get(url, auth=HTTPBasicAuth(*auth), params=params, verify=False, timeout=30)\n\n    if response.status_code != 200:\n        raise Exception(f\"Failed to get aggregates: {response.status_code}\")\n\n    aggregates = response.json().get('records', [])\n    available_aggs = [\n        agg for agg in aggregates\n        if agg.get('state') == 'online' and agg.get('space', {}).get('available', 0) &gt; 10737418240  # 10GB min\n    ]\n\n    activity.logger.info(f\"Found {len(available_aggs)} available aggregates\")\n    return available_aggs\n\n@activity.defn\nasync def create_svm(svm_config: SVMConfig, um_host: str, auth: tuple) -&gt; Dict:\n    \"\"\"Step 3: Create SVM with specified configuration\"\"\"\n    url = f\"https://{um_host}/api/v2/datacenter/svm/svms\"\n\n    payload = {\n        \"name\": svm_config.name,\n        \"cluster\": {\"key\": svm_config.cluster_key},\n        \"state\": \"running\",\n        \"subtype\": \"default\",\n        \"language\": svm_config.language,\n        \"security_style\": svm_config.security_style,\n        \"allowed_protocols\": svm_config.protocols\n    }\n\n    if svm_config.aggregate_name:\n        payload[\"aggregates\"] = [{\"name\": svm_config.aggregate_name}]\n\n    response = requests.post(\n        url,\n        auth=HTTPBasicAuth(*auth),\n        json=payload,\n        verify=False,\n        timeout=60\n    )\n\n    if response.status_code not in [201, 202]:\n        raise Exception(f\"Failed to create SVM: {response.status_code} - {response.text}\")\n\n    result = response.json()\n    activity.logger.info(f\"\u2713 SVM creation initiated: {svm_config.name}\")\n    return result\n\n@activity.defn\nasync def monitor_job_completion(job_uuid: str, um_host: str, auth: tuple, timeout_minutes: int = 30) -&gt; Dict:\n    \"\"\"Step 4: Monitor job completion with polling\"\"\"\n    url = f\"https://{um_host}/api/v2/management-server/jobs/{job_uuid}\"\n    start_time = asyncio.get_event_loop().time()\n    max_wait = timeout_minutes * 60\n\n    while (asyncio.get_event_loop().time() - start_time) &lt; max_wait:\n        response = requests.get(url, auth=HTTPBasicAuth(*auth), verify=False, timeout=30)\n\n        if response.status_code != 200:\n            raise Exception(f\"Failed to check job status: {response.status_code}\")\n\n        job = response.json()\n        state = job.get('state')\n\n        activity.logger.info(f\"Job {job_uuid} status: {state} ({job.get('progress', 0)}%)\")\n\n        if state == 'success':\n            return job\n        elif state in ['failure', 'partial_failures']:\n            raise Exception(f\"Job failed: {job.get('message', 'Unknown error')}\")\n\n        await asyncio.sleep(10)  # Poll every 10 seconds\n\n    raise Exception(f\"Job {job_uuid} timed out after {timeout_minutes} minutes\")\n\n@activity.defn\nasync def create_network_interface(svm_key: str, interface: NetworkInterface, um_host: str, auth: tuple) -&gt; Dict:\n    \"\"\"Step 5 &amp; 6: Create management and data network interfaces\"\"\"\n    url = f\"https://{um_host}/api/v2/datacenter/svm/svms/{svm_key}/network/ip/interfaces\"\n\n    payload = {\n        \"name\": interface.name,\n        \"ip\": {\n            \"address\": interface.ip_address,\n            \"netmask\": interface.netmask\n        },\n        \"location\": {\n            \"home_node\": {\"uuid\": interface.node_uuid},\n            \"home_port\": {\"name\": interface.port_name}\n        },\n        \"service_policy\": interface.service_policy,\n        \"enabled\": True\n    }\n\n    response = requests.post(\n        url,\n        auth=HTTPBasicAuth(*auth),\n        json=payload,\n        verify=False,\n        timeout=30\n    )\n\n    if response.status_code not in [201, 202]:\n        raise Exception(f\"Failed to create interface {interface.name}: {response.status_code}\")\n\n    activity.logger.info(f\"\u2713 Network interface {interface.name} created\")\n    return response.json()\n\n# SVM Creation Workflow\n@workflow.defn\nclass SVMCreationWorkflow:\n    \"\"\"Complete SVM Creation Workflow based on sequence diagram\"\"\"\n\n    @workflow.run\n    async def run(self, svm_config: SVMConfig, um_host: str, auth: tuple) -&gt; Dict:\n        retry_policy = RetryPolicy(\n            initial_interval=timedelta(seconds=1),\n            maximum_interval=timedelta(seconds=60),\n            maximum_attempts=3\n        )\n\n        workflow.logger.info(f\"Starting SVM creation workflow for {svm_config.name}\")\n\n        # Step 1: Validate cluster health\n        cluster_info = await workflow.execute_activity(\n            validate_cluster_health,\n            svm_config.cluster_key,\n            um_host,\n            auth,\n            start_to_close_timeout=timedelta(seconds=30),\n            retry_policy=retry_policy\n        )\n\n        # Step 2: Get available aggregates\n        aggregates = await workflow.execute_activity(\n            get_available_aggregates,\n            svm_config.cluster_key,\n            um_host,\n            auth,\n            start_to_close_timeout=timedelta(seconds=30),\n            retry_policy=retry_policy\n        )\n\n        # Select best aggregate (most available space)\n        if aggregates:\n            best_aggregate = max(aggregates, key=lambda x: x.get('space', {}).get('available', 0))\n            svm_config.aggregate_name = best_aggregate['name']\n            workflow.logger.info(f\"Selected aggregate: {best_aggregate['name']}\")\n\n        # Step 3: Create SVM\n        svm_result = await workflow.execute_activity(\n            create_svm,\n            svm_config,\n            um_host,\n            auth,\n            start_to_close_timeout=timedelta(minutes=2),\n            retry_policy=retry_policy\n        )\n\n        # Step 4: Monitor job completion if async operation\n        if 'job' in svm_result:\n            job_result = await workflow.execute_activity(\n                monitor_job_completion,\n                svm_result['job']['uuid'],\n                um_host,\n                auth,\n                30,  # timeout in minutes\n                start_to_close_timeout=timedelta(minutes=35),\n                retry_policy=retry_policy\n            )\n\n        svm_key = svm_result.get('svm_key') or svm_result.get('key')\n\n        # Steps 5 &amp; 6: Create network interfaces\n        mgmt_interface = NetworkInterface(\n            name=f\"{svm_config.name}_mgmt\",\n            ip_address=\"10.1.100.50\",  # This should be parameterized\n            netmask=\"255.255.255.0\",\n            node_uuid=\"node-uuid-placeholder\",  # This should be retrieved dynamically\n            port_name=\"e0c\",\n            service_policy=\"default-management\"\n        )\n\n        data_interface = NetworkInterface(\n            name=f\"{svm_config.name}_data\",\n            ip_address=\"10.1.100.51\",  # This should be parameterized\n            netmask=\"255.255.255.0\",\n            node_uuid=\"node-uuid-placeholder\",  # This should be retrieved dynamically\n            port_name=\"e0d\",\n            service_policy=\"default-data-files\"\n        )\n\n        # Create management interface\n        mgmt_result = await workflow.execute_activity(\n            create_network_interface,\n            svm_key,\n            mgmt_interface,\n            um_host,\n            auth,\n            start_to_close_timeout=timedelta(seconds=30),\n            retry_policy=retry_policy\n        )\n\n        # Create data interface\n        data_result = await workflow.execute_activity(\n            create_network_interface,\n            svm_key,\n            data_interface,\n            um_host,\n            auth,\n            start_to_close_timeout=timedelta(seconds=30),\n            retry_policy=retry_policy\n        )\n\n        return {\n            \"svm_name\": svm_config.name,\n            \"svm_key\": svm_key,\n            \"cluster_info\": cluster_info,\n            \"aggregate_used\": best_aggregate if aggregates else None,\n            \"management_interface\": mgmt_result,\n            \"data_interface\": data_result,\n            \"status\": \"completed\"\n        }\n</code></pre>"},{"location":"temporal-integration/#2-nfs-share-creation-workflow","title":"2. NFS Share Creation Workflow","text":"<p>Based on the NFS Share Creation Sequence Diagram:</p> <pre><code>@dataclass\nclass NFSShareConfig:\n    name: str\n    svm_key: str\n    path: str\n    size: str\n    export_policy_name: str = \"default\"\n    unix_permissions: str = \"755\"\n    security_style: str = \"unix\"\n\n@dataclass\nclass ExportPolicyRule:\n    clients: List[str]\n    protocols: List[str]\n    ro_rule: List[str]\n    rw_rule: List[str]\n    superuser: List[str]\n    allow_suid: bool = False\n\n# NFS-specific Activities\n@activity.defn\nasync def create_export_policy(svm_key: str, policy_name: str, rules: List[ExportPolicyRule], um_host: str, auth: tuple) -&gt; Dict:\n    \"\"\"Step 1: Create export policy with rules\"\"\"\n    url = f\"https://{um_host}/api/v2/datacenter/svm/svms/{svm_key}/export-policies\"\n\n    payload = {\n        \"name\": policy_name,\n        \"rules\": [\n            {\n                \"clients\": rule.clients,\n                \"protocols\": rule.protocols,\n                \"ro_rule\": rule.ro_rule,\n                \"rw_rule\": rule.rw_rule,\n                \"superuser\": rule.superuser,\n                \"allow_suid\": rule.allow_suid\n            } for rule in rules\n        ]\n    }\n\n    response = requests.post(url, auth=HTTPBasicAuth(*auth), json=payload, verify=False, timeout=30)\n\n    if response.status_code not in [201, 202]:\n        raise Exception(f\"Failed to create export policy: {response.status_code}\")\n\n    activity.logger.info(f\"\u2713 Export policy {policy_name} created\")\n    return response.json()\n\n@activity.defn\nasync def create_file_share(share_config: NFSShareConfig, um_host: str, auth: tuple) -&gt; Dict:\n    \"\"\"Step 2: Create NFS file share\"\"\"\n    url = f\"https://{um_host}/api/v2/storage-provider/file-shares\"\n\n    payload = {\n        \"name\": share_config.name,\n        \"size\": share_config.size,\n        \"svm\": {\"key\": share_config.svm_key},\n        \"path\": share_config.path,\n        \"export_policy\": {\"name\": share_config.export_policy_name},\n        \"unix_permissions\": share_config.unix_permissions,\n        \"security_style\": share_config.security_style\n    }\n\n    response = requests.post(url, auth=HTTPBasicAuth(*auth), json=payload, verify=False, timeout=60)\n\n    if response.status_code not in [201, 202]:\n        raise Exception(f\"Failed to create file share: {response.status_code}\")\n\n    activity.logger.info(f\"\u2713 NFS share {share_config.name} created\")\n    return response.json()\n\n@activity.defn\nasync def apply_export_policy(share_key: str, policy_name: str, um_host: str, auth: tuple) -&gt; Dict:\n    \"\"\"Step 3: Apply export policy to file share\"\"\"\n    url = f\"https://{um_host}/api/v2/storage-provider/file-shares/{share_key}\"\n\n    payload = {\n        \"export_policy\": {\"name\": policy_name}\n    }\n\n    response = requests.patch(url, auth=HTTPBasicAuth(*auth), json=payload, verify=False, timeout=30)\n\n    if response.status_code != 200:\n        raise Exception(f\"Failed to apply export policy: {response.status_code}\")\n\n    activity.logger.info(f\"\u2713 Export policy {policy_name} applied to share\")\n    return response.json()\n\n@activity.defn\nasync def verify_nfs_access(svm_key: str, um_host: str, auth: tuple) -&gt; Dict:\n    \"\"\"Step 4: Verify NFS service is accessible\"\"\"\n    url = f\"https://{um_host}/api/v2/datacenter/svm/svms/{svm_key}\"\n    params = {\"fields\": \"nfs.enabled,state,protocols\"}\n\n    response = requests.get(url, auth=HTTPBasicAuth(*auth), params=params, verify=False, timeout=30)\n\n    if response.status_code != 200:\n        raise Exception(f\"Failed to verify NFS access: {response.status_code}\")\n\n    svm_info = response.json()\n\n    if not svm_info.get('nfs', {}).get('enabled', False):\n        raise Exception(\"NFS is not enabled on SVM\")\n\n    activity.logger.info(\"\u2713 NFS access verified\")\n    return svm_info\n\n# NFS Share Creation Workflow\n@workflow.defn\nclass NFSShareCreationWorkflow:\n    \"\"\"Complete NFS Share Creation Workflow based on sequence diagram\"\"\"\n\n    @workflow.run\n    async def run(self, share_config: NFSShareConfig, export_rules: List[ExportPolicyRule], um_host: str, auth: tuple) -&gt; Dict:\n        retry_policy = RetryPolicy(\n            initial_interval=timedelta(seconds=2),\n            maximum_interval=timedelta(seconds=30),\n            maximum_attempts=3\n        )\n\n        workflow.logger.info(f\"Starting NFS share creation workflow for {share_config.name}\")\n\n        # Step 1: Create export policy\n        export_policy_result = await workflow.execute_activity(\n            create_export_policy,\n            share_config.svm_key,\n            share_config.export_policy_name,\n            export_rules,\n            um_host,\n            auth,\n            start_to_close_timeout=timedelta(seconds=30),\n            retry_policy=retry_policy\n        )\n\n        # Step 2: Create file share\n        share_result = await workflow.execute_activity(\n            create_file_share,\n            share_config,\n            um_host,\n            auth,\n            start_to_close_timeout=timedelta(minutes=5),\n            retry_policy=retry_policy\n        )\n\n        share_key = share_result.get('key')\n\n        # Step 3: Apply export policy (if not already applied)\n        if share_key:\n            policy_result = await workflow.execute_activity(\n                apply_export_policy,\n                share_key,\n                share_config.export_policy_name,\n                um_host,\n                auth,\n                start_to_close_timeout=timedelta(seconds=30),\n                retry_policy=retry_policy\n            )\n\n        # Step 4: Verify NFS access\n        nfs_verification = await workflow.execute_activity(\n            verify_nfs_access,\n            share_config.svm_key,\n            um_host,\n            auth,\n            start_to_close_timeout=timedelta(seconds=30),\n            retry_policy=retry_policy\n        )\n\n        return {\n            \"share_name\": share_config.name,\n            \"share_key\": share_key,\n            \"export_policy\": export_policy_result,\n            \"nfs_verification\": nfs_verification,\n            \"status\": \"completed\"\n        }\n</code></pre>"},{"location":"temporal-integration/#3-worker-implementation","title":"3. Worker Implementation","text":"<pre><code># worker.py\nimport asyncio\nimport logging\nimport os\nfrom temporalio.client import Client\nfrom temporalio.worker import Worker\n\nfrom netapp_workflows import (\n    SVMCreationWorkflow,\n    NFSShareCreationWorkflow,\n    # Import all activities\n    validate_cluster_health,\n    get_available_aggregates,\n    create_svm,\n    monitor_job_completion,\n    create_network_interface,\n    create_export_policy,\n    create_file_share,\n    apply_export_policy,\n    verify_nfs_access,\n)\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nasync def main():\n    # Connect to Temporal server\n    client = await Client.connect(os.getenv(\"TEMPORAL_HOST\", \"localhost:7233\"))\n\n    # Create worker\n    worker = Worker(\n        client,\n        task_queue=os.getenv(\"TASK_QUEUE\", \"netapp-activeiq\"),\n        workflows=[\n            SVMCreationWorkflow,\n            NFSShareCreationWorkflow,\n        ],\n        activities=[\n            validate_cluster_health,\n            get_available_aggregates,\n            create_svm,\n            monitor_job_completion,\n            create_network_interface,\n            create_export_policy,\n            create_file_share,\n            apply_export_policy,\n            verify_nfs_access,\n        ],\n    )\n\n    logger.info(\"Starting NetApp ActiveIQ Temporal worker...\")\n    await worker.run()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"temporal-integration/#4-client-usage-examples","title":"4. Client Usage Examples","text":"<pre><code># client.py\nimport asyncio\nimport os\nfrom temporalio.client import Client\n\nfrom netapp_workflows import SVMConfig, NFSShareConfig, ExportPolicyRule\n\nasync def run_svm_creation():\n    client = await Client.connect(os.getenv(\"TEMPORAL_HOST\", \"localhost:7233\"))\n\n    # Configure SVM\n    svm_config = SVMConfig(\n        name=\"production_nfs_svm\",\n        cluster_key=\"cluster-12345-abcde\",\n        protocols=[\"nfs\"]\n    )\n\n    # Start workflow\n    result = await client.execute_workflow(\n        \"SVMCreationWorkflow.run\",\n        svm_config,\n        os.getenv(\"NETAPP_UM_HOST\"),\n        (os.getenv(\"NETAPP_USERNAME\"), os.getenv(\"NETAPP_PASSWORD\")),\n        id=f\"svm-creation-{svm_config.name}\",\n        task_queue=\"netapp-activeiq\"\n    )\n\n    print(f\"SVM Creation Result: {result}\")\n    return result\n\nasync def run_nfs_share_creation(svm_key: str):\n    client = await Client.connect(os.getenv(\"TEMPORAL_HOST\", \"localhost:7233\"))\n\n    # Configure NFS share\n    share_config = NFSShareConfig(\n        name=\"shared_documents\",\n        svm_key=svm_key,\n        path=\"/shared_documents\",\n        size=\"500GB\",\n        export_policy_name=\"production_policy\"\n    )\n\n    # Define export rules\n    export_rules = [\n        ExportPolicyRule(\n            clients=[\"10.1.0.0/16\"],\n            protocols=[\"nfs3\", \"nfs4\"],\n            ro_rule=[\"sys\"],\n            rw_rule=[\"sys\"],\n            superuser=[\"sys\"]\n        ),\n        ExportPolicyRule(\n            clients=[\"192.168.1.0/24\"],\n            protocols=[\"nfs3\", \"nfs4\"],\n            ro_rule=[\"sys\"],\n            rw_rule=[\"none\"],\n            superuser=[\"none\"]\n        )\n    ]\n\n    # Start workflow\n    result = await client.execute_workflow(\n        \"NFSShareCreationWorkflow.run\",\n        share_config,\n        export_rules,\n        os.getenv(\"NETAPP_UM_HOST\"),\n        (os.getenv(\"NETAPP_USERNAME\"), os.getenv(\"NETAPP_PASSWORD\")),\n        id=f\"nfs-share-{share_config.name}\",\n        task_queue=\"netapp-activeiq\"\n    )\n\n    print(f\"NFS Share Creation Result: {result}\")\n    return result\n\nif __name__ == \"__main__\":\n    # Example: Create SVM first, then NFS share\n    async def main():\n        svm_result = await run_svm_creation()\n        if svm_result and svm_result.get(\"status\") == \"completed\":\n            await run_nfs_share_creation(svm_result[\"svm_key\"])\n\n    asyncio.run(main())\n</code></pre>"},{"location":"temporal-integration/#running-the-workflows","title":"Running the Workflows","text":""},{"location":"temporal-integration/#1-start-temporal-server","title":"1. Start Temporal Server","text":"<pre><code>docker run -d --name temporalio -p 7233:7233 -p 8080:8080 temporalio/auto-setup\n</code></pre>"},{"location":"temporal-integration/#2-start-the-worker","title":"2. Start the Worker","text":"<pre><code># Set environment variables\nexport NETAPP_UM_HOST=your-unified-manager.company.com\nexport NETAPP_USERNAME=admin\nexport NETAPP_PASSWORD=your-password\n\n# Start worker\npython worker.py\n</code></pre>"},{"location":"temporal-integration/#3-execute-workflows","title":"3. Execute Workflows","text":"<pre><code># In another terminal\npython client.py\n</code></pre>"},{"location":"temporal-integration/#4-monitor-workflows","title":"4. Monitor Workflows","text":"<p>Access the Temporal Web UI at <code>http://localhost:8080</code> to monitor workflow execution, view logs, and debug issues.</p>"},{"location":"temporal-integration/#error-handling-and-best-practices","title":"Error Handling and Best Practices","text":""},{"location":"temporal-integration/#retry-policies","title":"Retry Policies","text":"<pre><code># Custom retry policy for critical operations\ncritical_retry_policy = RetryPolicy(\n    initial_interval=timedelta(seconds=1),\n    maximum_interval=timedelta(minutes=5),\n    maximum_attempts=5,\n    backoff_coefficient=2.0\n)\n\n# Non-critical operations\nstandard_retry_policy = RetryPolicy(\n    initial_interval=timedelta(seconds=2),\n    maximum_interval=timedelta(seconds=30),\n    maximum_attempts=3\n)\n</code></pre>"},{"location":"temporal-integration/#logging-and-monitoring","title":"Logging and Monitoring","text":"<pre><code># Add structured logging\nimport structlog\n\nlogger = structlog.get_logger()\n\n@activity.defn\nasync def monitored_activity(param: str) -&gt; str:\n    logger.info(\"Activity started\", activity=\"monitored_activity\", param=param)\n    try:\n        # Activity logic here\n        result = \"success\"\n        logger.info(\"Activity completed\", result=result)\n        return result\n    except Exception as e:\n        logger.error(\"Activity failed\", error=str(e), param=param)\n        raise\n</code></pre>"},{"location":"temporal-integration/#conclusion","title":"Conclusion","text":"<p>This comprehensive Temporal.io integration provides:</p> <ul> <li>Robust workflow execution for complex NetApp operations</li> <li>Automatic error handling and retry mechanisms</li> <li>Complete observability through the Temporal Web UI</li> <li>Scalable architecture for handling multiple concurrent operations</li> <li>Production-ready implementations based on real sequence diagrams</li> </ul> <p>The workflows can be extended to handle additional NetApp operations like credential updates, performance monitoring, and event processing as defined in the advanced use cases documentation.</p>"},{"location":"api/mcp-tools/","title":"MCP Tools Reference","text":"<p>The NetApp ActiveIQ MCP Server provides a comprehensive set of tools that enable AI assistants to interact with NetApp storage infrastructure through the Model Context Protocol.</p>"},{"location":"api/mcp-tools/#tool-categories","title":"Tool Categories","text":""},{"location":"api/mcp-tools/#information-discovery","title":"\ud83d\udd0d Information &amp; Discovery","text":"<ul> <li>get_clusters - List and query cluster information</li> <li>get_nodes - Get cluster node details</li> <li>get_svms - List Storage Virtual Machines</li> <li>get_volumes - Query volume information</li> <li>get_aggregates - List storage aggregates</li> <li>test_connection - Test NetApp API connectivity</li> </ul>"},{"location":"api/mcp-tools/#monitoring-performance","title":"\ud83d\udcca Monitoring &amp; Performance","text":"<ul> <li>get_cluster_performance - Cluster performance metrics</li> <li>get_volume_performance - Volume performance data</li> <li>get_events - System events and alerts</li> <li>get_capacity_info - Storage capacity information</li> </ul>"},{"location":"api/mcp-tools/#management-operations","title":"\u2699\ufe0f Management Operations","text":"<ul> <li>create_svm - Create Storage Virtual Machine</li> <li>create_volume - Create new volume</li> <li>modify_volume - Modify volume settings</li> <li>create_snapshot - Create volume snapshot</li> </ul>"},{"location":"api/mcp-tools/#event-management","title":"\ud83d\udea8 Event Management","text":"<ul> <li>acknowledge_event - Acknowledge system events</li> <li>resolve_event - Resolve events</li> <li>get_active_alerts - Get active alerts</li> </ul>"},{"location":"api/mcp-tools/#tool-definitions","title":"Tool Definitions","text":""},{"location":"api/mcp-tools/#get_clusters","title":"get_clusters","text":"<p>List and query NetApp cluster information.</p> <pre><code>{\n  \"name\": \"get_clusters\",\n  \"description\": \"Retrieve information about NetApp ONTAP clusters\",\n  \"inputSchema\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"fields\": {\n        \"type\": \"array\",\n        \"items\": {\"type\": \"string\"},\n        \"description\": \"Specific fields to return\",\n        \"default\": [\"name\", \"uuid\", \"version\", \"state\", \"management_ip\"]\n      },\n      \"query\": {\n        \"type\": \"string\",\n        \"description\": \"Filter clusters using query syntax (e.g., 'name:prod*')\"\n      },\n      \"max_records\": {\n        \"type\": \"integer\",\n        \"description\": \"Maximum number of records to return\",\n        \"default\": 20\n      }\n    }\n  }\n}\n</code></pre> <p>Example Usage: <pre><code># Get all clusters with basic info\nresult = await mcp_client.call_tool(\"get_clusters\", {})\n\n# Get production clusters only\nresult = await mcp_client.call_tool(\"get_clusters\", {\n    \"query\": \"name:prod*\",\n    \"fields\": [\"name\", \"version\", \"state\", \"nodes\"]\n})\n</code></pre></p> <p>Response: <pre><code>{\n  \"clusters\": [\n    {\n      \"name\": \"prod-cluster-01\",\n      \"uuid\": \"12345678-1234-1234-1234-123456789012\",\n      \"version\": {\"full\": \"9.12.1\"},\n      \"state\": \"up\",\n      \"management_ip\": \"10.1.1.100\",\n      \"nodes\": 4\n    }\n  ],\n  \"total_records\": 1\n}\n</code></pre></p>"},{"location":"api/mcp-tools/#get_svms","title":"get_svms","text":"<p>List Storage Virtual Machines (SVMs) with filtering options.</p> <pre><code>{\n  \"name\": \"get_svms\",\n  \"description\": \"Retrieve Storage Virtual Machine information\",\n  \"inputSchema\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"cluster_name\": {\n        \"type\": \"string\",\n        \"description\": \"Filter by cluster name\"\n      },\n      \"state\": {\n        \"type\": \"string\",\n        \"enum\": [\"running\", \"stopped\", \"starting\", \"stopping\"],\n        \"description\": \"Filter by SVM state\"\n      },\n      \"protocols\": {\n        \"type\": \"array\",\n        \"items\": {\"type\": \"string\"},\n        \"description\": \"Filter by supported protocols (nfs, cifs, iscsi, fcp)\"\n      },\n      \"fields\": {\n        \"type\": \"array\",\n        \"items\": {\"type\": \"string\"},\n        \"default\": [\"name\", \"uuid\", \"state\", \"cluster\", \"protocols\"]\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"api/mcp-tools/#get_volumes","title":"get_volumes","text":"<p>Query volume information with detailed filtering and sorting options.</p> <pre><code>{\n  \"name\": \"get_volumes\",\n  \"description\": \"Retrieve volume information and capacity details\",\n  \"inputSchema\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"svm_name\": {\n        \"type\": \"string\",\n        \"description\": \"Filter by SVM name\"\n      },\n      \"cluster_name\": {\n        \"type\": \"string\",\n        \"description\": \"Filter by cluster name\"\n      },\n      \"state\": {\n        \"type\": \"string\",\n        \"enum\": [\"online\", \"offline\", \"mixed\"],\n        \"description\": \"Filter by volume state\"\n      },\n      \"size_threshold\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"operator\": {\"type\": \"string\", \"enum\": [\"gt\", \"lt\", \"gte\", \"lte\"]},\n          \"value\": {\"type\": \"integer\"},\n          \"unit\": {\"type\": \"string\", \"enum\": [\"bytes\", \"kb\", \"mb\", \"gb\", \"tb\"]}\n        },\n        \"description\": \"Filter by volume size\"\n      },\n      \"utilization_threshold\": {\n        \"type\": \"number\",\n        \"minimum\": 0,\n        \"maximum\": 100,\n        \"description\": \"Filter volumes above utilization percentage\"\n      },\n      \"order_by\": {\n        \"type\": \"string\",\n        \"enum\": [\"name\", \"size\", \"available\", \"used_percentage\"],\n        \"default\": \"name\"\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"api/mcp-tools/#create_svm","title":"create_svm","text":"<p>Create a new Storage Virtual Machine with specified configuration.</p> <pre><code>{\n  \"name\": \"create_svm\",\n  \"description\": \"Create a new Storage Virtual Machine (SVM)\",\n  \"inputSchema\": {\n    \"type\": \"object\",\n    \"required\": [\"name\", \"cluster_name\"],\n    \"properties\": {\n      \"name\": {\n        \"type\": \"string\",\n        \"description\": \"Name for the new SVM\"\n      },\n      \"cluster_name\": {\n        \"type\": \"string\",\n        \"description\": \"Target cluster name\"\n      },\n      \"protocols\": {\n        \"type\": \"array\",\n        \"items\": {\"type\": \"string\", \"enum\": [\"nfs\", \"cifs\", \"iscsi\", \"fcp\", \"nvme\"]},\n        \"default\": [\"nfs\"],\n        \"description\": \"Protocols to enable\"\n      },\n      \"language\": {\n        \"type\": \"string\",\n        \"default\": \"c.utf_8\",\n        \"description\": \"Language setting\"\n      },\n      \"security_style\": {\n        \"type\": \"string\",\n        \"enum\": [\"unix\", \"ntfs\", \"mixed\"],\n        \"default\": \"unix\",\n        \"description\": \"Security style\"\n      },\n      \"aggregates\": {\n        \"type\": \"array\",\n        \"items\": {\"type\": \"string\"},\n        \"description\": \"List of aggregate names to assign\"\n      },\n      \"dns_config\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"domains\": {\n            \"type\": \"array\",\n            \"items\": {\"type\": \"string\"}\n          },\n          \"servers\": {\n            \"type\": \"array\",\n            \"items\": {\"type\": \"string\"}\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"api/mcp-tools/#get_events","title":"get_events","text":"<p>Retrieve system events with filtering and sorting capabilities.</p> <pre><code>{\n  \"name\": \"get_events\",\n  \"description\": \"Get system events and alerts\",\n  \"inputSchema\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"severity\": {\n        \"type\": \"string\",\n        \"enum\": [\"emergency\", \"alert\", \"critical\", \"error\", \"warning\", \"notice\", \"informational\", \"debug\"],\n        \"description\": \"Filter by event severity\"\n      },\n      \"state\": {\n        \"type\": \"string\",\n        \"enum\": [\"new\", \"acknowledged\", \"resolved\"],\n        \"description\": \"Filter by event state\"\n      },\n      \"source_type\": {\n        \"type\": \"string\",\n        \"enum\": [\"cluster\", \"node\", \"svm\", \"volume\", \"aggregate\"],\n        \"description\": \"Filter by source object type\"\n      },\n      \"time_range\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"start\": {\"type\": \"string\", \"format\": \"date-time\"},\n          \"end\": {\"type\": \"string\", \"format\": \"date-time\"}\n        }\n      },\n      \"max_records\": {\n        \"type\": \"integer\",\n        \"default\": 50\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"api/mcp-tools/#test_connection","title":"test_connection","text":"<p>Test connectivity to NetApp ActiveIQ Unified Manager.</p> <pre><code>{\n  \"name\": \"test_connection\",\n  \"description\": \"Test connection to NetApp ActiveIQ Unified Manager\",\n  \"inputSchema\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"include_details\": {\n        \"type\": \"boolean\",\n        \"default\": false,\n        \"description\": \"Include detailed connection information\"\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"api/mcp-tools/#advanced-tool-usage","title":"Advanced Tool Usage","text":""},{"location":"api/mcp-tools/#workflow-orchestration-tools","title":"Workflow Orchestration Tools","text":""},{"location":"api/mcp-tools/#execute_temporal_workflow","title":"execute_temporal_workflow","text":"<p>Execute complex workflows using Temporal orchestration.</p> <pre><code>{\n  \"name\": \"execute_temporal_workflow\",\n  \"description\": \"Execute a predefined Temporal workflow\",\n  \"inputSchema\": {\n    \"type\": \"object\",\n    \"required\": [\"workflow_name\"],\n    \"properties\": {\n      \"workflow_name\": {\n        \"type\": \"string\",\n        \"enum\": [\"svm_creation\", \"volume_provisioning\", \"data_migration\", \"backup_workflow\"]\n      },\n      \"parameters\": {\n        \"type\": \"object\",\n        \"description\": \"Workflow-specific parameters\"\n      },\n      \"timeout\": {\n        \"type\": \"integer\",\n        \"default\": 3600,\n        \"description\": \"Workflow timeout in seconds\"\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"api/mcp-tools/#batch-operations","title":"Batch Operations","text":""},{"location":"api/mcp-tools/#batch_operation","title":"batch_operation","text":"<p>Execute multiple operations in a single request.</p> <pre><code>{\n  \"name\": \"batch_operation\",\n  \"description\": \"Execute multiple NetApp operations in batch\",\n  \"inputSchema\": {\n    \"type\": \"object\",\n    \"required\": [\"operations\"],\n    \"properties\": {\n      \"operations\": {\n        \"type\": \"array\",\n        \"items\": {\n          \"type\": \"object\",\n          \"required\": [\"tool\", \"arguments\"],\n          \"properties\": {\n            \"tool\": {\"type\": \"string\"},\n            \"arguments\": {\"type\": \"object\"},\n            \"id\": {\"type\": \"string\"}\n          }\n        }\n      },\n      \"fail_on_error\": {\n        \"type\": \"boolean\",\n        \"default\": false,\n        \"description\": \"Stop batch if any operation fails\"\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"api/mcp-tools/#error-handling","title":"Error Handling","text":"<p>All tools follow consistent error response patterns:</p> <pre><code>{\n  \"success\": false,\n  \"error\": {\n    \"code\": \"NETAPP_API_ERROR\",\n    \"message\": \"Failed to retrieve cluster information\",\n    \"details\": {\n      \"http_status\": 401,\n      \"netapp_error\": \"Authentication failed\"\n    }\n  }\n}\n</code></pre>"},{"location":"api/mcp-tools/#common-error-codes","title":"Common Error Codes","text":"<ul> <li><code>AUTHENTICATION_FAILED</code> - Invalid credentials</li> <li><code>AUTHORIZATION_DENIED</code> - Insufficient permissions</li> <li><code>RESOURCE_NOT_FOUND</code> - Requested resource doesn't exist</li> <li><code>VALIDATION_ERROR</code> - Invalid input parameters</li> <li><code>NETAPP_API_ERROR</code> - Error from NetApp API</li> <li><code>TIMEOUT_ERROR</code> - Operation timed out</li> <li><code>CONNECTION_ERROR</code> - Network connectivity issues</li> </ul>"},{"location":"api/mcp-tools/#tool-usage-examples","title":"Tool Usage Examples","text":""},{"location":"api/mcp-tools/#natural-language-queries","title":"Natural Language Queries","text":"<p>With an AI assistant, you can use natural language:</p> <p>\"Show me all volumes that are more than 80% full\" <pre><code># Translates to:\nmcp_client.call_tool(\"get_volumes\", {\n    \"utilization_threshold\": 80,\n    \"fields\": [\"name\", \"svm\", \"size\", \"used_percentage\"]\n})\n</code></pre></p> <p>\"Create a new SVM called 'dev-svm' on the production cluster with NFS protocol\" <pre><code># Translates to:\nmcp_client.call_tool(\"create_svm\", {\n    \"name\": \"dev-svm\",\n    \"cluster_name\": \"prod-cluster-01\",\n    \"protocols\": [\"nfs\"]\n})\n</code></pre></p> <p>\"What are the current critical alerts?\" <pre><code># Translates to:\nmcp_client.call_tool(\"get_events\", {\n    \"severity\": \"critical\",\n    \"state\": \"new\"\n})\n</code></pre></p>"},{"location":"api/mcp-tools/#programmatic-usage","title":"Programmatic Usage","text":"<pre><code>import asyncio\nfrom mcp_client import MCPClient\n\nasync def storage_health_check():\n    client = MCPClient(\"http://localhost:8080\")\n\n    # Get cluster status\n    clusters = await client.call_tool(\"get_clusters\", {})\n\n    # Check for volumes with low space\n    low_space_volumes = await client.call_tool(\"get_volumes\", {\n        \"utilization_threshold\": 90,\n        \"fields\": [\"name\", \"svm\", \"cluster\", \"used_percentage\", \"available\"]\n    })\n\n    # Get recent critical events\n    critical_events = await client.call_tool(\"get_events\", {\n        \"severity\": \"critical\",\n        \"state\": \"new\",\n        \"max_records\": 10\n    })\n\n    return {\n        \"clusters\": clusters,\n        \"low_space_volumes\": low_space_volumes,\n        \"critical_events\": critical_events\n    }\n\n# Run health check\nhealth_status = asyncio.run(storage_health_check())\n</code></pre>"},{"location":"api/mcp-tools/#authentication","title":"Authentication","text":"<p>Tools automatically use the configured NetApp credentials. No additional authentication is required at the tool level.</p>"},{"location":"api/mcp-tools/#rate-limiting","title":"Rate Limiting","text":"<ul> <li>Default rate limit: 100 requests per minute</li> <li>Configurable per tool type</li> <li>Automatic retry with exponential backoff</li> <li>Queue management for burst requests</li> </ul>"},{"location":"api/mcp-tools/#caching","title":"Caching","text":"<ul> <li>Automatic caching for read operations</li> <li>Configurable TTL per tool type</li> <li>Cache invalidation on write operations</li> <li>Redis-based distributed caching</li> </ul>"},{"location":"api/mcp-tools/#next-steps","title":"Next Steps","text":"<ul> <li>Architecture - Understand the underlying architecture</li> <li>Examples - See practical usage examples</li> <li>Error Handling - Learn about error handling patterns</li> <li>Authentication - Configure authentication methods</li> </ul>"},{"location":"api-management/api-management-strategy/","title":"API Management Strategy for NetApp Storage Platform","text":""},{"location":"api-management/api-management-strategy/#overview","title":"Overview","text":"<p>This document outlines the comprehensive API Management strategy for exposing NetApp storage services as products through a secure, scalable, and monitored API gateway using Gravitee.io. The strategy implements a multi-layered security approach with proper governance, monitoring, and lifecycle management.</p>"},{"location":"api-management/api-management-strategy/#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph TB\n    subgraph \"External Consumers\"\n        WebApp[\"Web Applications\"]\n        Mobile[\"Mobile Apps\"]\n        CLI[\"CLI Tools\"]\n        Third[\"Third-party Systems\"]\n    end\n\n    subgraph \"API Gateway Layer - Gravitee.io\"\n        Gateway[\"API Gateway\"]\n        Portal[\"Developer Portal\"]\n        Management[\"API Management Console\"]\n    end\n\n    subgraph \"Security and Policy Layer\"\n        OAuth[\"OAuth 2.0 / OIDC\"]\n        JWT[\"JWT Validation\"]\n        RateLimit[\"Rate Limiting\"]\n        RBAC[\"Role-based Access Control\"]\n    end\n\n    subgraph \"Backend Services\"\n        APIM[\"Internal APIM Layer\"]\n        Temporal[\"Temporal Workflows\"]\n        MCP[\"MCP Server - Optional\"]\n        NetApp[\"NetApp ActiveIQ APIs\"]\n    end\n\n    WebApp --&gt; Gateway\n    Mobile --&gt; Gateway\n    CLI --&gt; Gateway\n    Third --&gt; Gateway\n\n    Gateway --&gt; OAuth\n    Gateway --&gt; JWT\n    Gateway --&gt; RateLimit\n    Gateway --&gt; RBAC\n\n    Gateway --&gt; APIM\n    APIM --&gt; Temporal\n    Temporal --&gt; MCP\n    Temporal --&gt; NetApp\n\n    Portal --&gt; Management\n    Management --&gt; Gateway</code></pre>"},{"location":"api-management/api-management-strategy/#product-oriented-api-catalog","title":"Product-Oriented API Catalog","text":""},{"location":"api-management/api-management-strategy/#1-netapp-storage-management-api-core-product","title":"1. NetApp Storage Management API (Core Product)","text":"<p>Product Description: Comprehensive storage management capabilities including volume operations, SVM management, and performance monitoring.</p> <pre><code>product_definition:\n  name: \"NetApp Storage Management API\"\n  version: \"v1\"\n  description: \"Complete storage lifecycle management\"\n  tier: \"enterprise\"\n  pricing_model: \"usage_based\"\n  sla:\n    availability: \"99.9%\"\n    response_time: \"&lt; 500ms (95th percentile)\"\n    throughput: \"1000 requests/minute\"\n</code></pre> <p>API Endpoints:</p> <ul> <li>Volume Operations: <code>/v1/storage/volumes</code></li> <li>SVM Management: <code>/v1/storage/svms</code></li> <li>Performance Monitoring: <code>/v1/storage/performance</code></li> <li>Capacity Planning: <code>/v1/storage/capacity</code></li> </ul>"},{"location":"api-management/api-management-strategy/#2-netapp-backup-recovery-api","title":"2. NetApp Backup &amp; Recovery API","text":"<p>Product Description: Automated backup management and disaster recovery orchestration.</p> <pre><code>product_definition:\n  name: \"NetApp Backup &amp; Recovery API\"\n  version: \"v1\"\n  description: \"Data protection and recovery services\"\n  tier: \"premium\"\n  pricing_model: \"subscription\"\n  sla:\n    availability: \"99.95%\"\n    response_time: \"&lt; 200ms (95th percentile)\"\n    rpo: \"&lt; 1 hour\"\n    rto: \"&lt; 30 minutes\"\n</code></pre>"},{"location":"api-management/api-management-strategy/#3-netapp-analytics-ai-api","title":"3. NetApp Analytics &amp; AI API","text":"<p>Product Description: AI-powered storage analytics, predictive insights, and optimization recommendations.</p> <pre><code>product_definition:\n  name: \"NetApp Analytics &amp; AI API\"\n  version: \"v1\"\n  description: \"Intelligent storage analytics and optimization\"\n  tier: \"premium_plus\"\n  pricing_model: \"value_based\"\n  sla:\n    availability: \"99.9%\"\n    response_time: \"&lt; 1000ms (95th percentile)\"\n    accuracy: \"&gt; 90% prediction accuracy\"\n</code></pre>"},{"location":"api-management/api-management-strategy/#security-framework","title":"Security Framework","text":""},{"location":"api-management/api-management-strategy/#authentication-authorization","title":"Authentication &amp; Authorization","text":"<pre><code>security_layers:\n  authentication:\n    primary: \"OAuth 2.0 with PKCE\"\n    secondary: \"JWT Bearer Tokens\"\n    mfa_required: true\n    token_expiry: \"1 hour\"\n    refresh_token_expiry: \"24 hours\"\n\n  authorization:\n    model: \"RBAC + ABAC\"\n    scopes:\n      - \"storage:read\"\n      - \"storage:write\"\n      - \"storage:admin\"\n      - \"backup:manage\"\n      - \"analytics:access\"\n\n  api_security:\n    transport: \"TLS 1.3 only\"\n    request_signing: \"HMAC-SHA256\"\n    payload_encryption: \"AES-256-GCM\"\n    certificate_pinning: true\n</code></pre>"},{"location":"api-management/api-management-strategy/#role-based-access-control","title":"Role-Based Access Control","text":"<pre><code>roles:\n  storage_operator:\n    permissions:\n      - \"storage:read\"\n      - \"performance:monitor\"\n    restrictions:\n      - \"no_admin_operations\"\n      - \"read_only_configuration\"\n\n  storage_admin:\n    permissions:\n      - \"storage:read\"\n      - \"storage:write\"\n      - \"storage:admin\"\n      - \"backup:manage\"\n    restrictions:\n      - \"audit_log_required\"\n      - \"approval_required_critical_ops\"\n\n  analytics_user:\n    permissions:\n      - \"analytics:access\"\n      - \"performance:monitor\"\n      - \"capacity:forecast\"\n    restrictions:\n      - \"no_configuration_access\"\n\n  system_admin:\n    permissions:\n      - \"*:*\"\n    restrictions:\n      - \"mfa_required\"\n      - \"audit_log_comprehensive\"\n</code></pre>"},{"location":"api-management/api-management-strategy/#api-gateway-configuration","title":"API Gateway Configuration","text":""},{"location":"api-management/api-management-strategy/#graviteeio-policies-implementation","title":"Gravitee.io Policies Implementation","text":"<pre><code>// Rate Limiting Policy\n{\n  \"rate-limit\": {\n    \"key\": \"{#request.headers['X-Client-ID']}\",\n    \"limit\": 1000,\n    \"periodTime\": 3600,\n    \"periodTimeUnit\": \"SECONDS\"\n  }\n}\n\n// Circuit Breaker Policy\n{\n  \"circuit-breaker\": {\n    \"failureThreshold\": 5,\n    \"timeout\": 10000,\n    \"resetTimeout\": 30000,\n    \"fallback\": {\n      \"status\": 503,\n      \"body\": \"Service temporarily unavailable\"\n    }\n  }\n}\n\n// Request Transformation Policy\n{\n  \"transform-headers\": {\n    \"addHeaders\": [\n      {\n        \"name\": \"X-API-Version\",\n        \"value\": \"v1\"\n      },\n      {\n        \"name\": \"X-Request-ID\",\n        \"value\": \"{#request.id}\"\n      }\n    ],\n    \"removeHeaders\": [\"X-Internal-Token\"]\n  }\n}\n</code></pre>"},{"location":"api-management/api-management-strategy/#api-plans-subscription-management","title":"API Plans &amp; Subscription Management","text":"<pre><code>api_plans:\n  basic_plan:\n    name: \"Basic Storage API Access\"\n    security: \"API Key\"\n    rate_limits:\n      requests_per_minute: 100\n      requests_per_day: 10000\n    features:\n      - \"volume_read_operations\"\n      - \"basic_monitoring\"\n    pricing: \"free\"\n\n  professional_plan:\n    name: \"Professional Storage Management\"\n    security: \"OAuth 2.0\"\n    rate_limits:\n      requests_per_minute: 1000\n      requests_per_day: 100000\n    features:\n      - \"full_volume_management\"\n      - \"svm_operations\"\n      - \"performance_analytics\"\n    pricing: \"$100/month\"\n\n  enterprise_plan:\n    name: \"Enterprise Storage Platform\"\n    security: \"OAuth 2.0 + JWT\"\n    rate_limits:\n      requests_per_minute: 5000\n      requests_per_day: 1000000\n    features:\n      - \"all_storage_operations\"\n      - \"ai_analytics\"\n      - \"priority_support\"\n    pricing: \"custom\"\n</code></pre>"},{"location":"api-management/api-management-strategy/#monitoring-analytics","title":"Monitoring &amp; Analytics","text":""},{"location":"api-management/api-management-strategy/#api-metrics-kpis","title":"API Metrics &amp; KPIs","text":"<pre><code>monitoring_dashboard:\n  operational_metrics:\n    - api_response_time_p95\n    - api_availability_percentage\n    - error_rate_percentage\n    - requests_per_second\n    - concurrent_connections\n\n  business_metrics:\n    - api_adoption_rate\n    - developer_portal_engagement\n    - api_plan_distribution\n    - revenue_per_api\n    - customer_satisfaction_score\n\n  security_metrics:\n    - authentication_failure_rate\n    - authorization_violations\n    - suspicious_activity_alerts\n    - certificate_expiry_warnings\n</code></pre>"},{"location":"api-management/api-management-strategy/#alerting-configuration","title":"Alerting Configuration","text":"<pre><code>alerting_rules:\n  high_error_rate:\n    condition: \"error_rate &gt; 5%\"\n    duration: \"5 minutes\"\n    severity: \"critical\"\n    channels: [\"pagerduty\", \"slack\"]\n\n  response_time_degradation:\n    condition: \"p95_response_time &gt; 1000ms\"\n    duration: \"10 minutes\"\n    severity: \"warning\"\n    channels: [\"slack\", \"email\"]\n\n  security_violation:\n    condition: \"authentication_failures &gt; 10 per minute\"\n    duration: \"1 minute\"\n    severity: \"critical\"\n    channels: [\"security_team\", \"pagerduty\"]\n</code></pre>"},{"location":"api-management/api-management-strategy/#developer-experience","title":"Developer Experience","text":""},{"location":"api-management/api-management-strategy/#api-documentation-strategy","title":"API Documentation Strategy","text":"<pre><code>documentation_approach:\n  format: \"OpenAPI 3.0\"\n  interactive_docs: true\n  code_samples:\n    languages: [\"curl\", \"python\", \"javascript\", \"go\"]\n  tutorials:\n    - \"getting_started\"\n    - \"authentication_guide\"\n    - \"volume_management_walkthrough\"\n    - \"performance_optimization\"\n\n  developer_portal_features:\n    - api_explorer\n    - subscription_management\n    - usage_analytics\n    - support_tickets\n    - community_forum\n</code></pre>"},{"location":"api-management/api-management-strategy/#sdk-strategy","title":"SDK Strategy","text":"<pre><code>sdk_development:\n  languages:\n    - python\n    - javascript\n    - go\n    - java\n\n  features:\n    - automatic_authentication\n    - retry_mechanisms\n    - error_handling\n    - async_support\n    - pagination_helpers\n\n  distribution:\n    - npm_registry\n    - pypi\n    - maven_central\n    - go_modules\n</code></pre>"},{"location":"api-management/api-management-strategy/#deployment-architecture","title":"Deployment Architecture","text":""},{"location":"api-management/api-management-strategy/#graviteeio-components-deployment","title":"Gravitee.io Components Deployment","text":"<pre><code>deployment_architecture:\n  api_gateway:\n    replicas: 3\n    resources:\n      cpu: \"2\"\n      memory: \"4Gi\"\n    config:\n      cluster_mode: true\n      health_checks: enabled\n\n  management_api:\n    replicas: 2\n    resources:\n      cpu: \"1\"\n      memory: \"2Gi\"\n    database: \"mongodb\"\n\n  portal_ui:\n    replicas: 2\n    resources:\n      cpu: \"0.5\"\n      memory: \"1Gi\"\n    cdn_enabled: true\n\n  elasticsearch:\n    replicas: 3\n    resources:\n      cpu: \"2\"\n      memory: \"8Gi\"\n    storage: \"100Gi\"\n</code></pre>"},{"location":"api-management/api-management-strategy/#high-availability-configuration","title":"High Availability Configuration","text":"<pre><code>high_availability:\n  load_balancing:\n    algorithm: \"round_robin\"\n    health_checks:\n      path: \"/health\"\n      interval: \"30s\"\n      timeout: \"5s\"\n\n  clustering:\n    enabled: true\n    discovery: \"kubernetes\"\n    sync_interval: \"5s\"\n\n  backup_strategy:\n    configuration_backup: \"daily\"\n    analytics_backup: \"weekly\"\n    retention: \"30 days\"\n</code></pre>"},{"location":"api-management/api-management-strategy/#security-compliance","title":"Security Compliance","text":""},{"location":"api-management/api-management-strategy/#compliance-frameworks","title":"Compliance Frameworks","text":"<pre><code>compliance_requirements:\n  iso_27001:\n    - access_control_policy\n    - information_security_policy\n    - incident_management\n    - risk_assessment\n\n  soc2_type2:\n    - security_controls\n    - availability_controls\n    - processing_integrity\n    - confidentiality\n\n  gdpr:\n    - data_protection_by_design\n    - consent_management\n    - data_subject_rights\n    - breach_notification\n</code></pre>"},{"location":"api-management/api-management-strategy/#audit-logging","title":"Audit &amp; Logging","text":"<pre><code>audit_configuration:\n  log_levels:\n    security_events: \"INFO\"\n    api_access: \"DEBUG\"\n    system_events: \"WARN\"\n\n  log_retention:\n    security_logs: \"7 years\"\n    access_logs: \"1 year\"\n    system_logs: \"6 months\"\n\n  log_destinations:\n    - \"elasticsearch\"\n    - \"splunk\"\n    - \"s3_archive\"\n</code></pre>"},{"location":"api-management/api-management-strategy/#lifecycle-management","title":"Lifecycle Management","text":""},{"location":"api-management/api-management-strategy/#api-versioning-strategy","title":"API Versioning Strategy","text":"<pre><code>versioning_strategy:\n  approach: \"URL path versioning\"\n  format: \"/v{major}.{minor}\"\n  deprecation_policy:\n    notice_period: \"6 months\"\n    support_period: \"12 months\"\n    migration_assistance: true\n\n  backward_compatibility:\n    breaking_changes: \"major version only\"\n    feature_additions: \"minor version\"\n    bug_fixes: \"patch version\"\n</code></pre>"},{"location":"api-management/api-management-strategy/#change-management","title":"Change Management","text":"<pre><code>change_management:\n  approval_workflow:\n    - \"api_design_review\"\n    - \"security_review\"\n    - \"performance_review\"\n    - \"documentation_review\"\n\n  testing_phases:\n    - \"unit_tests\"\n    - \"integration_tests\"\n    - \"performance_tests\"\n    - \"security_tests\"\n    - \"user_acceptance_tests\"\n\n  deployment_strategy:\n    - \"blue_green_deployment\"\n    - \"canary_releases\"\n    - \"feature_flags\"\n    - \"rollback_procedures\"\n</code></pre>"},{"location":"api-management/api-management-strategy/#cost-optimization","title":"Cost Optimization","text":""},{"location":"api-management/api-management-strategy/#resource-management","title":"Resource Management","text":"<pre><code>cost_optimization:\n  auto_scaling:\n    enabled: true\n    min_replicas: 2\n    max_replicas: 10\n    target_cpu: 70%\n    target_memory: 80%\n\n  caching_strategy:\n    api_responses: \"redis\"\n    ttl: \"5 minutes\"\n    cache_hit_ratio_target: \"&gt; 80%\"\n\n  monitoring_costs:\n    metrics_retention: \"30 days\"\n    logs_retention: \"7 days\"\n    analytics_sampling: \"10%\"\n</code></pre>"},{"location":"api-management/api-management-strategy/#migration-plan","title":"Migration Plan","text":""},{"location":"api-management/api-management-strategy/#implementation-phases","title":"Implementation Phases","text":"<pre><code>migration_phases:\n  phase_1_foundation:\n    duration: \"4 weeks\"\n    deliverables:\n      - \"gravitee_deployment\"\n      - \"basic_security_setup\"\n      - \"core_api_exposure\"\n\n  phase_2_security:\n    duration: \"3 weeks\"\n    deliverables:\n      - \"oauth_integration\"\n      - \"rbac_implementation\"\n      - \"audit_logging\"\n\n  phase_3_advanced:\n    duration: \"4 weeks\"\n    deliverables:\n      - \"developer_portal\"\n      - \"analytics_dashboard\"\n      - \"monetization_setup\"\n\n  phase_4_optimization:\n    duration: \"2 weeks\"\n    deliverables:\n      - \"performance_tuning\"\n      - \"monitoring_enhancement\"\n      - \"documentation_completion\"\n</code></pre> <p>This comprehensive API management strategy provides a secure, scalable, and product-oriented approach to exposing NetApp storage services through Gravitee.io, ensuring proper governance, monitoring, and developer experience while maintaining enterprise-grade security standards.</p>"},{"location":"architecture/faas-function-tom/","title":"Target Operating Model: OpenFaaS MCP Server Deployment","text":""},{"location":"architecture/faas-function-tom/#overview","title":"Overview","text":"<p>This Target Operating Model (TOM) outlines the deployment of a Model Context Protocol (MCP) server using OpenFaaS. OpenFaaS provides a serverless framework to package server functionalities as containerized functions, enabling convenient deployment and autoscaling on Kubernetes, OpenShift, or a standalone server.</p>"},{"location":"architecture/faas-function-tom/#executive-summary","title":"Executive Summary","text":"<p>Deploying MCP server operations with OpenFaaS ensures: - Scalability: Dynamic autoscaling based on demand - Operational Efficiency: Lower overhead by utilizing serverless container functions - Flexibility: Support for any language, provided in a container - Ease of Management: Web-based and CLI management tools</p>"},{"location":"architecture/faas-function-tom/#function-preparation-and-deployment","title":"Function Preparation and Deployment","text":""},{"location":"architecture/faas-function-tom/#step-by-step-deployment","title":"Step-by-Step Deployment","text":""},{"location":"architecture/faas-function-tom/#1-prepare-mcp-server-code","title":"1. Prepare MCP Server Code","text":"<ul> <li>Ready your MCP server executable code.</li> <li>Use a command such as:   <pre><code>npx -y @modelcontextprotocol/server-filesystem /data\n</code></pre></li> </ul>"},{"location":"architecture/faas-function-tom/#2-scaffold-an-openfaas-function","title":"2. Scaffold an OpenFaaS Function","text":"<ul> <li>Initialize your project:   <pre><code>faas-cli new mcp-server --lang dockerfile\n</code></pre></li> <li>This command will structure your function directory with Dockerfile support.</li> </ul>"},{"location":"architecture/faas-function-tom/#3-customize-dockerfile","title":"3. Customize Dockerfile","text":"<ul> <li>Edit the Dockerfile to install dependencies and set the execution command:   <pre><code>FROM node:20-alpine\n\nWORKDIR /app\nRUN npm install -g @modelcontextprotocol/server-filesystem\n\nCOPY handler.sh .\nRUN chmod +x handler.sh\n\nENV fprocess=\"npx -y @modelcontextprotocol/server-filesystem /data\"\nCMD [\"fwatchdog\"]\n</code></pre></li> <li><code>handler.sh</code> facilitates additional custom execution if needed, controlled by the <code>fprocess</code> environment.</li> </ul>"},{"location":"architecture/faas-function-tom/#4-setup-openfaas-function-configuration","title":"4. Setup OpenFaaS Function Configuration","text":"<ul> <li>Configure <code>mcp-server.yml</code>:   <pre><code>provider:\n  name: openfaas\n  gateway: http://127.0.0.1:8080\n\nfunctions:\n  mcp-server:\n    lang: dockerfile\n    handler: ./mcp-server\n    image: your_dockerhub_username/mcp-server:latest\n</code></pre></li> <li>Update with your Docker Hub credentials.</li> </ul>"},{"location":"architecture/faas-function-tom/#5-build-and-deploy","title":"5. Build and Deploy","text":"<ul> <li>Build and push the Docker image:   <pre><code>faas-cli build -f mcp-server.yml\nfaas-cli push -f mcp-server.yml\n</code></pre></li> <li>Deploy your function to the OpenFaaS gateway:   <pre><code>faas-cli deploy -f mcp-server.yml\n</code></pre></li> </ul>"},{"location":"architecture/faas-function-tom/#6-confirm-and-test","title":"6. Confirm and Test","text":"<ul> <li>Confirm deployment:   <pre><code>faas-cli list\n</code></pre></li> <li>Invoke the function:   <pre><code>echo \"test\" | faas-cli invoke mcp-server\n</code></pre></li> </ul>"},{"location":"architecture/faas-function-tom/#architectural-benefits","title":"Architectural Benefits","text":""},{"location":"architecture/faas-function-tom/#function-management","title":"Function Management","text":"<ul> <li>Easy Scaling: Functions scale automatically based on demand</li> <li>Efficient Resource Allocation: Only utilize resources during active requests</li> </ul>"},{"location":"architecture/faas-function-tom/#development-and-operations","title":"Development and Operations","text":"<ul> <li>Language Agnostic: Supported deployment of any language as long as it fits within a container</li> <li>Robust Tooling: OpenFaaS CLI and UI simplify function management</li> </ul>"},{"location":"architecture/faas-function-tom/#key-notes","title":"Key Notes","text":"<ul> <li>The <code>fprocess</code> environment variable defines function execution. It tailors the OpenFaaS handling process uniquely for any MCP node command.</li> <li>OpenFaaS supports a multi-cloud setup, offering flexibility in hosting serverless functions across varied infrastructures.</li> </ul>"},{"location":"architecture/faas-function-tom/#business-impact","title":"Business Impact","text":"<p>Enhanced Flexibility: Allows deploying serverless MCP instances at scale with lower costs and enhanced manageability.</p> <p>Improved Resource Utilization: Ensures optimal cost savings by running functions only when needed, reducing idle time to zero.</p> <p>Deploying an MCP server as an OpenFaaS function provides a modern, cloud-native approach to dynamic serverless deployments, fully leveraging OpenFaaS's robust features for maximum efficiency and ease of operation.</p>"},{"location":"architecture/knative-function-tom/","title":"Target Operating Model: Knative Function-Based MCP Server Deployment","text":""},{"location":"architecture/knative-function-tom/#overview","title":"Overview","text":"<p>This Target Operating Model (TOM) defines a Knative Function-First architecture for deploying NetApp ActiveIQ MCP Server operations using the <code>kn</code> CLI and Knative Functions. This approach transforms traditional storage management into discrete, serverless functions that scale automatically and provide cost-optimal operations.</p>"},{"location":"architecture/knative-function-tom/#executive-summary","title":"Executive Summary","text":"<p>The Knative Function-Based TOM enables: - Function-per-Operation: Each NetApp operation becomes an independent serverless function - Auto-scaling: Functions scale to zero when idle, infinitely when needed - Cost Optimization: Pay only for actual function execution time - DevOps Self-Service: Developers deploy functions using simple <code>kn</code> commands - AI-Native Integration: Each function is optimized for AI assistant consumption</p>"},{"location":"architecture/knative-function-tom/#architecture-paradigm","title":"Architecture Paradigm","text":""},{"location":"architecture/knative-function-tom/#traditional-vs-function-based-deployment","title":"Traditional vs Function-Based Deployment","text":"<pre><code>graph TD\n    subgraph \"Traditional Deployment\"\n        A1[Monolithic MCP Server] --&gt; B1[Always Running]\n        B1 --&gt; C1[Fixed Resources]\n        C1 --&gt; D1[High Costs]\n        D1 --&gt; E1[Manual Scaling]\n    end\n\n    subgraph \"Knative Function-Based\"\n        A2[Function Decomposition] --&gt; B2[Event-Driven Execution]\n        B2 --&gt; C2[Auto-Scaling]\n        C2 --&gt; D2[Cost Optimization]\n        D2 --&gt; E2[DevOps Self-Service]\n\n        subgraph \"Function Catalog\"\n            F1[Storage Monitor Function]\n            F2[Volume Provisioner Function]\n            F3[SVM Manager Function]\n            F4[Performance Analyzer Function]\n            F5[Event Processor Function]\n            F6[Backup Controller Function]\n        end\n\n        E2 --&gt; F1\n        E2 --&gt; F2\n        E2 --&gt; F3\n        E2 --&gt; F4\n        E2 --&gt; F5\n        E2 --&gt; F6\n    end</code></pre>"},{"location":"architecture/knative-function-tom/#function-decomposition-strategy","title":"Function Decomposition Strategy","text":""},{"location":"architecture/knative-function-tom/#netapp-mcp-tools-as-independent-functions","title":"NetApp MCP Tools as Independent Functions","text":"Function Name MCP Tools Included Scaling Pattern Use Case <code>netapp-storage-monitor</code> <code>get_clusters</code>, <code>get_aggregates</code>, <code>get_volumes</code> High frequency Real-time monitoring <code>netapp-volume-ops</code> <code>create_volume</code>, <code>delete_volume</code>, <code>modify_volume</code> On-demand bursts Volume lifecycle <code>netapp-svm-manager</code> <code>create_svm</code>, <code>get_svms</code>, <code>configure_svm</code> Periodic SVM operations <code>netapp-performance</code> <code>get_performance_metrics</code>, <code>analyze_performance</code> Scheduled Performance analysis <code>netapp-events</code> <code>get_events</code>, <code>process_alerts</code> Event-driven Alert handling <code>netapp-backup</code> <code>create_snapshot</code>, <code>backup_operations</code> Scheduled Backup operations"},{"location":"architecture/knative-function-tom/#function-deployment-with-kn-cli","title":"Function Deployment with kn CLI","text":""},{"location":"architecture/knative-function-tom/#1-storage-monitor-function","title":"1. Storage Monitor Function","text":"<pre><code># Create storage monitor function\nkn func create netapp-storage-monitor \\\n  --language python \\\n  --template http\n\n# Update function code\ncat &gt; netapp-storage-monitor/func.py &lt;&lt; 'EOF'\nimport asyncio\nimport json\nfrom parliament import Context\nfrom netapp_mcp_tools import get_clusters, get_aggregates, get_volumes\n\nasync def main(context: Context):\n    \"\"\"Storage monitoring function\"\"\"\n    request_data = context.request.json\n    operation = request_data.get('operation', 'get_clusters')\n\n    if operation == 'get_clusters':\n        result = await get_clusters()\n    elif operation == 'get_aggregates':\n        cluster_uuid = request_data.get('cluster_uuid')\n        result = await get_aggregates(cluster_uuid=cluster_uuid)\n    elif operation == 'get_volumes':\n        filters = request_data.get('filters', {})\n        result = await get_volumes(**filters)\n    else:\n        return {\n            'statusCode': 400,\n            'body': json.dumps({'error': f'Unknown operation: {operation}'})\n        }\n\n    return {\n        'statusCode': 200,\n        'headers': {'Content-Type': 'application/json'},\n        'body': json.dumps(result)\n    }\nEOF\n\n# Deploy function\nkn func deploy \\\n  --namespace netapp-functions \\\n  --env NETAPP_BASE_URL=\"${NETAPP_BASE_URL}\" \\\n  --env NETAPP_USERNAME=\"${NETAPP_USERNAME}\" \\\n  --env NETAPP_PASSWORD=\"${NETAPP_PASSWORD}\" \\\n  --build \\\n  --verbose\n</code></pre>"},{"location":"architecture/knative-function-tom/#2-volume-operations-function","title":"2. Volume Operations Function","text":"<pre><code># Create volume operations function\nkn func create netapp-volume-ops \\\n  --language python \\\n  --template http\n\n# Update function configuration\ncat &gt; netapp-volume-ops/func.yaml &lt;&lt; 'EOF'\nspecVersion: 0.35.0\nname: netapp-volume-ops\nruntime: python\nregistry: your-registry.com/netapp\nimage: your-registry.com/netapp/volume-ops:latest\ncreated: 2024-01-15T10:00:00Z\ninvoke: gunicorn\nbuild:\n  builder: pack\n  buildpacks:\n    - gcr.io/paketo-buildpacks/python\nrun:\n  env:\n    - name: NETAPP_BASE_URL\n      value: '{{env:NETAPP_BASE_URL}}'\n    - name: NETAPP_USERNAME\n      value: '{{env:NETAPP_USERNAME}}'\n    - name: NETAPP_PASSWORD\n      value: '{{env:NETAPP_PASSWORD}}'\n  envs:\n    - name: FUNCTION_TARGET\n      value: main\ndeploy:\n  namespace: netapp-functions\n  options:\n    scale:\n      min: 0\n      max: 20\n      metric: concurrency\n      target: 10\n    resources:\n      requests:\n        cpu: 200m\n        memory: 256Mi\n      limits:\n        cpu: 1000m\n        memory: 512Mi\n    annotations:\n      autoscaling.knative.dev/scaleDownDelay: \"30s\"\n      autoscaling.knative.dev/scaleUpDelay: \"0s\"\nEOF\n\n# Function implementation\ncat &gt; netapp-volume-ops/func.py &lt;&lt; 'EOF'\nimport asyncio\nimport json\nfrom parliament import Context\nfrom netapp_mcp_tools import create_volume, delete_volume, modify_volume\n\nasync def main(context: Context):\n    \"\"\"Volume operations function\"\"\"\n    request_data = context.request.json\n    operation = request_data.get('operation')\n\n    try:\n        if operation == 'create_volume':\n            volume_config = request_data.get('volume_config', {})\n            result = await create_volume(volume_config)\n        elif operation == 'delete_volume':\n            volume_uuid = request_data.get('volume_uuid')\n            result = await delete_volume(volume_uuid)\n        elif operation == 'modify_volume':\n            volume_uuid = request_data.get('volume_uuid')\n            modifications = request_data.get('modifications', {})\n            result = await modify_volume(volume_uuid, modifications)\n        else:\n            return {\n                'statusCode': 400,\n                'body': json.dumps({'error': f'Unknown operation: {operation}'})\n            }\n\n        return {\n            'statusCode': 200,\n            'headers': {'Content-Type': 'application/json'},\n            'body': json.dumps({\n                'success': True,\n                'operation': operation,\n                'result': result\n            })\n        }\n    except Exception as e:\n        return {\n            'statusCode': 500,\n            'body': json.dumps({\n                'success': False,\n                'error': str(e),\n                'operation': operation\n            })\n        }\nEOF\n\n# Deploy with scaling configuration\nkn func deploy \\\n  --namespace netapp-functions \\\n  --build \\\n  --verbose\n</code></pre>"},{"location":"architecture/knative-function-tom/#3-svm-manager-function","title":"3. SVM Manager Function","text":"<pre><code># Create SVM manager function\nkn func create netapp-svm-manager \\\n  --language python \\\n  --template http\n\n# Deploy with custom scaling\nkn func deploy \\\n  --namespace netapp-functions \\\n  --env NETAPP_BASE_URL=\"${NETAPP_BASE_URL}\" \\\n  --env NETAPP_USERNAME=\"${NETAPP_USERNAME}\" \\\n  --env NETAPP_PASSWORD=\"${NETAPP_PASSWORD}\" \\\n  --scale-min 0 \\\n  --scale-max 5 \\\n  --scale-target 2 \\\n  --scale-utilization 70 \\\n  --build\n</code></pre>"},{"location":"architecture/knative-function-tom/#deployment-architecture","title":"Deployment Architecture","text":""},{"location":"architecture/knative-function-tom/#knative-function-ecosystem","title":"Knative Function Ecosystem","text":"<pre><code>graph TB\n    subgraph \"AI Assistant Layer\"\n        A[Claude/GPT Assistant]\n        B[Custom AI Agent]\n    end\n\n    subgraph \"Knative Function Gateway\"\n        C[Knative Serving]\n        D[Function Router]\n        E[Auto-Scaler]\n    end\n\n    subgraph \"NetApp Function Catalog\"\n        F1[Storage Monitor&lt;br/&gt;kn func: netapp-storage-monitor]\n        F2[Volume Ops&lt;br/&gt;kn func: netapp-volume-ops]\n        F3[SVM Manager&lt;br/&gt;kn func: netapp-svm-manager]\n        F4[Performance&lt;br/&gt;kn func: netapp-performance]\n        F5[Events&lt;br/&gt;kn func: netapp-events]\n        F6[Backup&lt;br/&gt;kn func: netapp-backup]\n    end\n\n    subgraph \"NetApp Infrastructure\"\n        G[ActiveIQ Unified Manager]\n        H[ONTAP Clusters]\n    end\n\n    A --&gt; C\n    B --&gt; C\n    C --&gt; D\n    D --&gt; E\n    E --&gt; F1\n    E --&gt; F2\n    E --&gt; F3\n    E --&gt; F4\n    E --&gt; F5\n    E --&gt; F6\n\n    F1 --&gt; G\n    F2 --&gt; G\n    F3 --&gt; G\n    F4 --&gt; G\n    F5 --&gt; G\n    F6 --&gt; G\n    G --&gt; H</code></pre>"},{"location":"architecture/knative-function-tom/#function-orchestration-patterns","title":"Function Orchestration Patterns","text":""},{"location":"architecture/knative-function-tom/#1-simple-function-invocation","title":"1. Simple Function Invocation","text":"<pre><code>sequenceDiagram\n    participant AI as AI Assistant\n    participant KN as Knative Gateway\n    participant SM as Storage Monitor Function\n    participant API as NetApp API\n\n    AI-&gt;&gt;KN: GET /netapp-storage-monitor&lt;br/&gt;{\"operation\": \"get_clusters\"}\n    Note over SM: Function cold start (0\u21921)\n    KN-&gt;&gt;SM: Route request\n    SM-&gt;&gt;API: GET /clusters\n    API--&gt;&gt;SM: Cluster data\n    SM--&gt;&gt;KN: Formatted response\n    KN--&gt;&gt;AI: Storage cluster information\n    Note over SM: Auto-scale to zero after idle timeout</code></pre>"},{"location":"architecture/knative-function-tom/#2-complex-function-composition","title":"2. Complex Function Composition","text":"<pre><code>sequenceDiagram\n    participant AI as AI Assistant\n    participant KN as Knative Gateway\n    participant VO as Volume Ops Function\n    participant SM as Storage Monitor Function\n    participant PA as Performance Function\n    participant API as NetApp API\n\n    AI-&gt;&gt;KN: POST /netapp-volume-ops&lt;br/&gt;{\"operation\": \"create_optimized_volume\"}\n    KN-&gt;&gt;VO: Route request\n    Note over VO: Function starts (0\u21921)\n\n    VO-&gt;&gt;KN: Call storage monitor function\n    KN-&gt;&gt;SM: GET capacity info\n    Note over SM: Function starts (0\u21921)\n    SM-&gt;&gt;API: GET /aggregates\n    API--&gt;&gt;SM: Capacity data\n    SM--&gt;&gt;VO: Available capacity\n\n    VO-&gt;&gt;KN: Call performance function\n    KN-&gt;&gt;PA: GET performance metrics\n    Note over PA: Function starts (0\u21921)\n    PA-&gt;&gt;API: GET /performance/aggregates\n    API--&gt;&gt;PA: Performance data\n    PA--&gt;&gt;VO: Performance recommendations\n\n    VO-&gt;&gt;API: POST /volumes (create optimized)\n    API--&gt;&gt;VO: Volume created\n    VO--&gt;&gt;KN: Complete configuration\n    KN--&gt;&gt;AI: Optimized volume details\n\n    Note over SM,PA,VO: All functions scale to zero</code></pre>"},{"location":"architecture/knative-function-tom/#3-event-driven-function-activation","title":"3. Event-Driven Function Activation","text":"<pre><code>graph LR\n    A[NetApp Event] --&gt; B[Event Bus&lt;br/&gt;Knative Eventing]\n    B --&gt; C[Event Filter]\n    C --&gt; D[Function Trigger]\n\n    subgraph \"Function Activation\"\n        D --&gt; E[netapp-events Function]\n        D --&gt; F[netapp-backup Function]\n        D --&gt; G[netapp-performance Function]\n    end\n\n    E --&gt; H[Process Alert]\n    F --&gt; I[Trigger Backup]\n    G --&gt; J[Performance Analysis]</code></pre>"},{"location":"architecture/knative-function-tom/#advanced-deployment-patterns","title":"Advanced Deployment Patterns","text":""},{"location":"architecture/knative-function-tom/#direct-source-to-url-deployment","title":"Direct Source-to-URL Deployment","text":"<p>For advanced CI/CD scenarios, deploy directly from source using Kaniko or Ko:</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: netapp-mcp-server\nspec:\n  template:\n    metadata:\n      annotations:\n        autoscaling.knative.dev/maxScale: \"10\"\n        autoscaling.knative.dev/minScale: \"1\"\n        autoscaling.knative.dev/scale-down-delay: \"0\"\n        autoscaling.knative.dev/target: \"100\"\n    spec:\n      containerConcurrency: 100\n      containers:\n        - name: netapp-mcp\n          image: ko://github.com/netapp/mcp-server\n          args:\n            - \"--stdio\"\n            - \"python -m mcp_server\"\n            - \"--outputTransport\"\n            - \"sse\"\n          env:\n            - name: NETAPP_BASE_URL\n              valueFrom:\n                secretKeyRef:\n                  name: netapp-credentials\n                  key: NETAPP_BASE_URL\n</code></pre>"},{"location":"architecture/knative-function-tom/#function-template-customization","title":"Function Template Customization","text":"<pre><code># Create custom function template\nkn func create netapp-mcp-server \\\n  --language python \\\n  --template http \\\n  --registry ${REGISTRY}\n\n# Configure scaling and resources\nkn func deploy \\\n  --namespace netapp-mcp \\\n  --env NETAPP_BASE_URL=\"${NETAPP_BASE_URL}\" \\\n  --env NETAPP_USERNAME=\"${NETAPP_USERNAME}\" \\\n  --env NETAPP_PASSWORD=\"${NETAPP_PASSWORD}\" \\\n  --scale-min 0 \\\n  --scale-max 20 \\\n  --scale-target 10 \\\n  --scale-utilization 70 \\\n  --concurrency-target 100 \\\n  --timeout 300s \\\n  --build\n</code></pre>"},{"location":"architecture/knative-function-tom/#comparison-deployment-methods","title":"Comparison: Deployment Methods","text":"Method Command Source Format Registry Use Case kn func CLI <code>kn func deploy --registry &lt;registry&gt;</code> Local project Yes Quick development Manual YAML <code>kubectl apply -f manifest.yaml</code> Container image Yes Custom config Source-to-URL <code>kubectl apply -f manifest.yaml</code> Git repo Yes CI/CD pipelines Direct MCP Container args in YAML MCP server binary Yes Standard MCP"},{"location":"architecture/knative-function-tom/#operational-workflows","title":"Operational Workflows","text":""},{"location":"architecture/knative-function-tom/#devops-self-service-workflow","title":"DevOps Self-Service Workflow","text":"<pre><code>sequenceDiagram\n    participant Dev as DevOps Engineer\n    participant CLI as kn CLI\n    participant Git as Git Repository\n    participant KN as Knative Platform\n    participant API as NetApp API\n\n    Dev-&gt;&gt;CLI: kn func create my-storage-function\n    CLI--&gt;&gt;Dev: Function template created\n\n    Dev-&gt;&gt;Git: Commit function code\n    Git--&gt;&gt;Dev: Code committed\n\n    Dev-&gt;&gt;CLI: kn func deploy --build\n    CLI-&gt;&gt;KN: Deploy function\n    KN--&gt;&gt;CLI: Function deployed\n    CLI--&gt;&gt;Dev: Function URL provided\n\n    Dev-&gt;&gt;CLI: kn func invoke --data '{\"operation\": \"test\"}'\n    CLI-&gt;&gt;KN: Test function\n    KN-&gt;&gt;API: Execute NetApp operation\n    API--&gt;&gt;KN: Return result\n    KN--&gt;&gt;CLI: Function result\n    CLI--&gt;&gt;Dev: Test results displayed</code></pre>"},{"location":"architecture/knative-function-tom/#function-management-commands","title":"Function Management Commands","text":""},{"location":"architecture/knative-function-tom/#core-kn-function-commands","title":"Core kn Function Commands","text":"<pre><code># List all deployed functions\nkn func list --namespace netapp-functions\n\n# Describe a specific function\nkn func describe netapp-storage-monitor\n\n# View function logs\nkn func logs netapp-volume-ops --follow\n\n# Scale function manually\nkn service update netapp-svm-manager \\\n  --scale-min 2 \\\n  --scale-max 10 \\\n  --scale-target 5\n\n# Update function environment variables\nkn service update netapp-performance \\\n  --env PERFORMANCE_THRESHOLD=80 \\\n  --env ANALYSIS_DEPTH=detailed\n\n# Function traffic management\nkn service update netapp-volume-ops \\\n  --traffic netapp-volume-ops-v1=90 \\\n  --traffic netapp-volume-ops-v2=10\n\n# Delete function\nkn func delete netapp-events --namespace netapp-functions\n</code></pre>"},{"location":"architecture/knative-function-tom/#function-development-workflow","title":"Function Development Workflow","text":"<pre><code># Initialize new function project\nkn func create netapp-custom-function \\\n  --language python \\\n  --template http\n\n# Local development and testing\ncd netapp-custom-function\nkn func run --build --verbose\n\n# Test function locally\ncurl -X POST http://localhost:8080 \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"operation\": \"test_operation\", \"params\": {}}'\n\n# Deploy to cluster\nkn func deploy \\\n  --namespace netapp-functions \\\n  --build \\\n  --verbose\n\n# Invoke deployed function\nkn func invoke \\\n  --data '{\"operation\": \"get_clusters\"}' \\\n  --format json\n</code></pre>"},{"location":"architecture/knative-function-tom/#cost-optimization-with-functions","title":"Cost Optimization with Functions","text":""},{"location":"architecture/knative-function-tom/#resource-utilization-model","title":"Resource Utilization Model","text":"<pre><code># Function cost calculation\nclass FunctionCostModel:\n    def __init__(self):\n        self.cost_per_gb_second = 0.0000166667  # Google Cloud pricing\n        self.cost_per_request = 0.0000004       # Per million requests\n\n    def calculate_monthly_cost(self, functions_config):\n        total_cost = 0\n\n        for func_name, config in functions_config.items():\n            # Memory allocation in GB\n            memory_gb = config['memory_mb'] / 1024\n\n            # Execution time in seconds per month\n            executions_per_month = config['monthly_invocations']\n            avg_duration_seconds = config['avg_duration_ms'] / 1000\n\n            # Compute cost\n            compute_cost = (\n                memory_gb *\n                avg_duration_seconds *\n                executions_per_month *\n                self.cost_per_gb_second\n            )\n\n            # Request cost\n            request_cost = executions_per_month * self.cost_per_request\n\n            function_cost = compute_cost + request_cost\n            total_cost += function_cost\n\n            print(f\"{func_name}: ${function_cost:.2f}/month\")\n\n        return total_cost\n\n# Example calculation\nfunctions = {\n    'netapp-storage-monitor': {\n        'memory_mb': 256,\n        'monthly_invocations': 50000,\n        'avg_duration_ms': 800\n    },\n    'netapp-volume-ops': {\n        'memory_mb': 512,\n        'monthly_invocations': 5000,\n        'avg_duration_ms': 2000\n    },\n    'netapp-svm-manager': {\n        'memory_mb': 512,\n        'monthly_invocations': 1000,\n        'avg_duration_ms': 5000\n    }\n}\n\ncost_model = FunctionCostModel()\nmonthly_cost = cost_model.calculate_monthly_cost(functions)\nprint(f\"Total monthly cost: ${monthly_cost:.2f}\")\n\n# Compared to always-on container: $150/month\n# Function-based cost: ~$15/month\n# Savings: 90%\n</code></pre>"},{"location":"architecture/knative-function-tom/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"architecture/knative-function-tom/#function-metrics-dashboard","title":"Function Metrics Dashboard","text":"<pre><code># Grafana dashboard configuration for functions\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: netapp-functions-dashboard\n  namespace: netapp-functions\ndata:\n  dashboard.json: |\n    {\n      \"dashboard\": {\n        \"title\": \"NetApp Functions Dashboard\",\n        \"panels\": [\n          {\n            \"title\": \"Function Invocations\",\n            \"type\": \"graph\",\n            \"targets\": [\n              {\n                \"expr\": \"sum(rate(knative_serving_revision_request_count[5m])) by (service_name)\",\n                \"legendFormat\": \"{{service_name}}\"\n              }\n            ]\n          },\n          {\n            \"title\": \"Function Duration\",\n            \"type\": \"graph\",\n            \"targets\": [\n              {\n                \"expr\": \"histogram_quantile(0.95, rate(knative_serving_revision_request_latencies_bucket[5m]))\",\n                \"legendFormat\": \"95th percentile\"\n              }\n            ]\n          },\n          {\n            \"title\": \"Function Scaling\",\n            \"type\": \"graph\",\n            \"targets\": [\n              {\n                \"expr\": \"knative_serving_revision_actual_replicas\",\n                \"legendFormat\": \"{{service_name}}\"\n              }\n            ]\n          },\n          {\n            \"title\": \"Cost per Function\",\n            \"type\": \"singlestat\",\n            \"targets\": [\n              {\n                \"expr\": \"sum(function_compute_cost_dollars) by (function_name)\"\n              }\n            ]\n          }\n        ]\n      }\n    }\n</code></pre>"},{"location":"architecture/knative-function-tom/#function-health-checks","title":"Function Health Checks","text":"<pre><code># Health check implementation for functions\nimport asyncio\nimport json\nfrom datetime import datetime\n\nasync def health_check():\n    \"\"\"Standard health check for all NetApp functions\"\"\"\n    try:\n        # Test NetApp API connectivity\n        from netapp_client import get_client\n        client = get_client()\n\n        # Simple connectivity test\n        clusters = await client.get_clusters(fields=\"uuid,name\")\n\n        return {\n            'status': 'healthy',\n            'timestamp': datetime.utcnow().isoformat(),\n            'api_connectivity': 'ok',\n            'clusters_accessible': len(clusters.get('records', [])),\n            'function_memory_mb': get_memory_usage(),\n            'uptime_seconds': get_uptime()\n        }\n    except Exception as e:\n        return {\n            'status': 'unhealthy',\n            'timestamp': datetime.utcnow().isoformat(),\n            'error': str(e),\n            'function_memory_mb': get_memory_usage(),\n            'uptime_seconds': get_uptime()\n        }\n\ndef get_memory_usage():\n    \"\"\"Get current memory usage in MB\"\"\"\n    import psutil\n    process = psutil.Process()\n    return process.memory_info().rss / 1024 / 1024\n\ndef get_uptime():\n    \"\"\"Get function uptime in seconds\"\"\"\n    import time\n    global start_time\n    return time.time() - start_time\n</code></pre>"},{"location":"architecture/knative-function-tom/#security-and-compliance","title":"Security and Compliance","text":""},{"location":"architecture/knative-function-tom/#function-level-security","title":"Function-Level Security","text":"<pre><code># Security policy for NetApp functions\napiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: netapp-functions-security\n  namespace: netapp-functions\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: netapp-function\n  rules:\n  - from:\n    - source:\n        principals: [\"cluster.local/ns/ai-assistants/sa/ai-service-account\"]\n    to:\n    - operation:\n        methods: [\"POST\", \"GET\"]\n        paths: [\"/\", \"/health\", \"/metrics\"]\n  - from:\n    - source:\n        principals: [\"cluster.local/ns/monitoring/sa/prometheus\"]\n    to:\n    - operation:\n        methods: [\"GET\"]\n        paths: [\"/metrics\"]\n</code></pre>"},{"location":"architecture/knative-function-tom/#credential-management","title":"Credential Management","text":"<pre><code># Create secrets for function access\nkubectl create secret generic netapp-function-credentials \\\n  --namespace netapp-functions \\\n  --from-literal=NETAPP_BASE_URL=\"${NETAPP_BASE_URL}\" \\\n  --from-literal=NETAPP_USERNAME=\"${NETAPP_USERNAME}\" \\\n  --from-literal=NETAPP_PASSWORD=\"${NETAPP_PASSWORD}\" \\\n  --from-literal=NETAPP_VERIFY_SSL=\"false\"\n\n# Update all functions to use the secret\nfor func in netapp-storage-monitor netapp-volume-ops netapp-svm-manager; do\n  kn service update $func \\\n    --env-from secret:netapp-function-credentials \\\n    --namespace netapp-functions\ndone\n</code></pre>"},{"location":"architecture/knative-function-tom/#business-impact","title":"Business Impact","text":""},{"location":"architecture/knative-function-tom/#operational-metrics","title":"Operational Metrics","text":"Metric Traditional Deployment Function-Based Improvement Cold Start Time N/A (always running) 2-5 seconds N/A Resource Utilization 100% (idle resources) 0% (when idle) 100% efficiency Monthly Cost $150-300 $15-30 90% reduction Scaling Time 5-10 minutes 30 seconds 95% faster Deployment Time 10-15 minutes 2-3 minutes 80% faster Maintenance Overhead High (server management) Minimal (function management) 70% reduction"},{"location":"architecture/knative-function-tom/#developer-experience","title":"Developer Experience","text":"<pre><code># Traditional deployment workflow\ngit commit &amp;&amp; git push\nwait 15 minutes for CI/CD\ncheck server health\nvalidate deployment\nupdate load balancer\nmonitor for issues\n\n# Function-based workflow\nkn func deploy --build\n# Function is live in 2-3 minutes\nkn func invoke --test\n# Ready for AI assistant consumption\n</code></pre>"},{"location":"architecture/knative-function-tom/#implementation-roadmap","title":"Implementation Roadmap","text":""},{"location":"architecture/knative-function-tom/#phase-1-foundation-week-1-2","title":"Phase 1: Foundation (Week 1-2)","text":"<ol> <li> <p>Setup Knative Environment <pre><code># Install Knative Serving\nkubectl apply -f https://github.com/knative/serving/releases/latest/download/serving-crds.yaml\nkubectl apply -f https://github.com/knative/serving/releases/latest/download/serving-core.yaml\n\n# Install Knative CLI\ncurl -L https://github.com/knative/client/releases/latest/download/kn-linux-amd64 -o kn\nchmod +x kn &amp;&amp; sudo mv kn /usr/local/bin/\n\n# Install func CLI\ncurl -L https://github.com/knative/func/releases/latest/download/func_linux_amd64.tar.gz | tar xz\nchmod +x func &amp;&amp; sudo mv func /usr/local/bin/\n</code></pre></p> </li> <li> <p>Create First Function <pre><code># Create and deploy storage monitor function\nkn func create netapp-storage-monitor --language python --template http\n# Implement function logic\nkn func deploy --namespace netapp-functions --build\n</code></pre></p> </li> </ol>"},{"location":"architecture/knative-function-tom/#phase-2-function-decomposition-week-3-4","title":"Phase 2: Function Decomposition (Week 3-4)","text":"<ol> <li>Deploy Core Functions</li> <li>Storage Monitor Function</li> <li>Volume Operations Function</li> <li> <p>SVM Manager Function</p> </li> <li> <p>Implement Function Orchestration</p> </li> <li>Function composition patterns</li> <li>Event-driven triggers</li> <li>Error handling and retries</li> </ol>"},{"location":"architecture/knative-function-tom/#phase-3-optimization-week-5-6","title":"Phase 3: Optimization (Week 5-6)","text":"<ol> <li>Performance Tuning</li> <li>Cold start optimization</li> <li>Memory and CPU right-sizing</li> <li> <p>Connection pooling</p> </li> <li> <p>Cost Optimization</p> </li> <li>Scaling configuration tuning</li> <li>Resource limit optimization</li> <li>Usage pattern analysis</li> </ol>"},{"location":"architecture/knative-function-tom/#phase-4-production-readiness-week-7-8","title":"Phase 4: Production Readiness (Week 7-8)","text":"<ol> <li>Monitoring and Observability</li> <li>Comprehensive metrics</li> <li>Distributed tracing</li> <li> <p>Alerting rules</p> </li> <li> <p>Security and Compliance</p> </li> <li>Function-level security policies</li> <li>Secret management</li> <li>Network policies</li> </ol>"},{"location":"architecture/knative-function-tom/#success-criteria","title":"Success Criteria","text":""},{"location":"architecture/knative-function-tom/#technical-kpis","title":"Technical KPIs","text":"<ul> <li>Function Cold Start: &lt; 3 seconds for 95% of invocations</li> <li>Function Availability: 99.9% uptime</li> <li>Auto-scaling Response: &lt; 30 seconds to scale up</li> <li>Cost Reduction: &gt; 80% compared to traditional deployment</li> </ul>"},{"location":"architecture/knative-function-tom/#business-kpis","title":"Business KPIs","text":"<ul> <li>Developer Productivity: 5x faster deployment cycles</li> <li>Operational Overhead: 70% reduction in maintenance tasks</li> <li>AI Assistant Response Time: &lt; 5 seconds for simple operations</li> <li>Resource Efficiency: 95% reduction in idle resource consumption</li> </ul> <p>This Knative Function-Based Target Operating Model transforms NetApp storage operations into highly efficient, cost-effective, and developer-friendly serverless functions that scale automatically and integrate seamlessly with AI assistants.</p>"},{"location":"architecture/system-design/","title":"System Design","text":""},{"location":"architecture/system-design/#overview","title":"Overview","text":"<p>The NetApp ActiveIQ MCP Server implements a bridge between the Model Context Protocol (MCP) and NetApp's ActiveIQ Unified Manager API, enabling AI assistants and automation tools to interact with NetApp storage infrastructure through natural language interfaces.</p>"},{"location":"architecture/system-design/#high-level-architecture","title":"High-Level Architecture","text":"<pre><code>graph TB\n\n    subgraph \"AI/Client Layer\"\n        A1[Claude Desktop]\n        A2[Custom AI App]\n        A3[MCP Client]\n    end\n\n    subgraph \"MCP Protocol Layer\"\n        B1[MCP Transport]\n        B2[Tool Registry]\n        B3[Schema Validator]\n    end\n\n    subgraph \"NetApp MCP Server Core\"\n        C1[Request Router]\n        C2[Tool Executor]\n        C3[Response Formatter]\n        C4[Authentication Manager]\n        C5[Cache Manager]\n        C6[Event Handler]\n    end\n\n    subgraph \"Integration Layer\"\n        D1[NetApp API Client]\n        D2[Temporal Workflows]\n        D3[Prometheus Metrics]\n        D4[Error Handler]\n    end\n\n    subgraph \"NetApp Infrastructure\"\n        E1[ActiveIQ Unified Manager]\n        E2[ONTAP Clusters]\n        E3[Storage Systems]\n    end\n\n    subgraph \"External Services\"\n        F1[Temporal Server]\n        F2[Redis Cache]\n        F3[Monitoring System]\n    end\n\n    A1 --&gt; B1\n    A2 --&gt; B1\n    A3 --&gt; B1\n\n    B1 --&gt; C1\n    B2 --&gt; C2\n    B3 --&gt; C3\n\n    C1 --&gt; D1\n    C2 --&gt; D2\n    C3 --&gt; D3\n    C4 --&gt; D1\n    C5 --&gt; F2\n    C6 --&gt; D4\n\n    D1 --&gt; E1\n    D2 --&gt; F1\n    D3 --&gt; F3\n\n    E1 --&gt; E2\n    E1 --&gt; E3</code></pre>"},{"location":"architecture/system-design/#component-architecture","title":"Component Architecture","text":""},{"location":"architecture/system-design/#mcp-protocol-layer","title":"MCP Protocol Layer","text":""},{"location":"architecture/system-design/#mcp-transport","title":"MCP Transport","text":"<ul> <li>Purpose: Handles MCP protocol communication</li> <li>Responsibilities:</li> <li>Message serialization/deserialization</li> <li>Protocol version negotiation</li> <li>Connection management</li> <li>Error propagation</li> </ul>"},{"location":"architecture/system-design/#tool-registry","title":"Tool Registry","text":"<ul> <li>Purpose: Maintains registry of available MCP tools</li> <li>Responsibilities:</li> <li>Tool discovery and registration</li> <li>Schema validation</li> <li>Tool metadata management</li> <li>Capability advertisement</li> </ul>"},{"location":"architecture/system-design/#schema-validator","title":"Schema Validator","text":"<ul> <li>Purpose: Validates MCP messages and tool arguments</li> <li>Responsibilities:</li> <li>JSON schema validation</li> <li>Type checking</li> <li>Parameter validation</li> <li>Error reporting</li> </ul>"},{"location":"architecture/system-design/#server-core","title":"Server Core","text":""},{"location":"architecture/system-design/#request-router","title":"Request Router","text":"<ul> <li>Purpose: Routes incoming MCP requests to appropriate handlers</li> <li>Implementation: <pre><code>class RequestRouter:\n    def __init__(self):\n        self.tools = ToolRegistry()\n        self.auth = AuthenticationManager()\n\n    async def route_request(self, request: MCPRequest) -&gt; MCPResponse:\n        # Authenticate request\n        if not await self.auth.validate(request):\n            return MCPResponse.error(\"Authentication failed\")\n\n        # Route to tool\n        tool = self.tools.get_tool(request.method)\n        if not tool:\n            return MCPResponse.error(\"Tool not found\")\n\n        return await tool.execute(request.params)\n</code></pre></li> </ul>"},{"location":"architecture/system-design/#tool-executor","title":"Tool Executor","text":"<ul> <li>Purpose: Executes NetApp operations through registered tools</li> <li>Key Features:</li> <li>Async execution</li> <li>Timeout handling</li> <li>Rate limiting</li> <li>Result caching</li> </ul>"},{"location":"architecture/system-design/#response-formatter","title":"Response Formatter","text":"<ul> <li>Purpose: Formats responses according to MCP specification</li> <li>Responsibilities:</li> <li>Result serialization</li> <li>Error formatting</li> <li>Schema compliance</li> <li>Content negotiation</li> </ul>"},{"location":"architecture/system-design/#authentication-manager","title":"Authentication Manager","text":"<ul> <li>Purpose: Manages NetApp API authentication</li> <li>Implementation: <pre><code>class AuthenticationManager:\n    def __init__(self, config: NetAppConfig):\n        self.session = aiohttp.ClientSession(\n            auth=aiohttp.BasicAuth(config.username, config.password),\n            connector=aiohttp.TCPConnector(ssl=config.verify_ssl)\n        )\n\n    async def validate_connection(self) -&gt; bool:\n        try:\n            async with self.session.get(f\"{self.base_url}/datacenter/cluster/clusters\") as resp:\n                return resp.status == 200\n        except Exception:\n            return False\n</code></pre></li> </ul>"},{"location":"architecture/system-design/#integration-layer","title":"Integration Layer","text":""},{"location":"architecture/system-design/#netapp-api-client","title":"NetApp API Client","text":"<ul> <li>Purpose: Interfaces with ActiveIQ Unified Manager API</li> <li>Key Features:</li> <li>Connection pooling</li> <li>Retry logic</li> <li>Response caching</li> <li>Error handling</li> </ul> <pre><code>class NetAppAPIClient:\n    def __init__(self, config: NetAppConfig):\n        self.base_url = f\"https://{config.host}/api/v2\"\n        self.session = self._create_session(config)\n\n    async def get_clusters(self, **params) -&gt; List[Dict]:\n        url = f\"{self.base_url}/datacenter/cluster/clusters\"\n        async with self.session.get(url, params=params) as resp:\n            data = await resp.json()\n            return data.get('records', [])\n</code></pre>"},{"location":"architecture/system-design/#temporal-workflows","title":"Temporal Workflows","text":"<ul> <li>Purpose: Orchestrates complex, long-running NetApp operations</li> <li>Use Cases:</li> <li>SVM creation workflows</li> <li>Volume provisioning</li> <li>Data migration</li> <li>Backup operations</li> </ul>"},{"location":"architecture/system-design/#prometheus-metrics","title":"Prometheus Metrics","text":"<ul> <li>Purpose: Exposes operational metrics</li> <li>Metrics:</li> <li>Request count and duration</li> <li>NetApp API response times</li> <li>Error rates</li> <li>Cache hit/miss ratios</li> </ul>"},{"location":"architecture/system-design/#data-flow","title":"Data Flow","text":""},{"location":"architecture/system-design/#request-processing-flow","title":"Request Processing Flow","text":"<pre><code>sequenceDiagram\n    participant Client as MCP Client\n    participant Server as MCP Server\n    participant Cache as Cache Layer\n    participant NetApp as NetApp API\n\n    Client-&gt;&gt;Server: MCP Request\n    Server-&gt;&gt;Server: Validate Request\n    Server-&gt;&gt;Server: Authenticate\n    Server-&gt;&gt;Cache: Check Cache\n\n    alt Cache Hit\n        Cache--&gt;&gt;Server: Cached Response\n    else Cache Miss\n        Server-&gt;&gt;NetApp: API Request\n        NetApp--&gt;&gt;Server: API Response\n        Server-&gt;&gt;Cache: Store Response\n    end\n\n    Server-&gt;&gt;Server: Format Response\n    Server--&gt;&gt;Client: MCP Response</code></pre>"},{"location":"architecture/system-design/#error-handling-flow","title":"Error Handling Flow","text":"<pre><code>sequenceDiagram\n    participant Client as MCP Client\n    participant Server as MCP Server\n    participant NetApp as NetApp API\n    participant Monitor as Monitoring\n\n    Client-&gt;&gt;Server: MCP Request\n    Server-&gt;&gt;NetApp: API Request\n    NetApp--&gt;&gt;Server: Error Response\n\n    Server-&gt;&gt;Server: Parse Error\n    Server-&gt;&gt;Monitor: Log Error\n    Server-&gt;&gt;Server: Format Error Response\n    Server--&gt;&gt;Client: MCP Error Response</code></pre>"},{"location":"architecture/system-design/#scalability-design","title":"Scalability Design","text":""},{"location":"architecture/system-design/#horizontal-scaling","title":"Horizontal Scaling","text":"<ul> <li>Stateless Design: Server instances are stateless</li> <li>Load Balancing: Support for multiple server instances</li> <li>Shared Cache: Redis for cross-instance caching</li> <li>Database: External storage for persistent data</li> </ul>"},{"location":"architecture/system-design/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Connection Pooling: Reuse HTTP connections to NetApp APIs</li> <li>Async Processing: Non-blocking I/O operations</li> <li>Caching Strategy: Multi-level caching (memory + Redis)</li> <li>Request Batching: Batch multiple requests when possible</li> </ul>"},{"location":"architecture/system-design/#resource-management","title":"Resource Management","text":"<pre><code>class ResourceManager:\n    def __init__(self, config: Config):\n        self.semaphore = asyncio.Semaphore(config.max_concurrent_requests)\n        self.connection_pool = aiohttp.TCPConnector(\n            limit=config.max_connections,\n            limit_per_host=config.max_connections_per_host\n        )\n\n    async def execute_with_limit(self, coro):\n        async with self.semaphore:\n            return await coro\n</code></pre>"},{"location":"architecture/system-design/#security-architecture","title":"Security Architecture","text":""},{"location":"architecture/system-design/#authentication-flow","title":"Authentication Flow","text":"<pre><code>sequenceDiagram\n    participant Client as MCP Client\n    participant Server as MCP Server\n    participant Auth as Auth Service\n    participant NetApp as NetApp API\n\n    Client-&gt;&gt;Server: Request with Token\n    Server-&gt;&gt;Auth: Validate Token\n    Auth--&gt;&gt;Server: Token Valid\n    Server-&gt;&gt;NetApp: API Request (Service Account)\n    NetApp--&gt;&gt;Server: Response\n    Server--&gt;&gt;Client: MCP Response</code></pre>"},{"location":"architecture/system-design/#security-layers","title":"Security Layers","text":"<ol> <li>Transport Security: TLS encryption</li> <li>Authentication: JWT tokens or API keys</li> <li>Authorization: Role-based access control</li> <li>NetApp Security: Service account with minimal permissions</li> <li>Input Validation: Strict parameter validation</li> <li>Output Sanitization: Sensitive data filtering</li> </ol>"},{"location":"architecture/system-design/#configuration-management","title":"Configuration Management","text":""},{"location":"architecture/system-design/#configuration-schema","title":"Configuration Schema","text":"<pre><code># Server configuration\nserver:\n  host: \"0.0.0.0\"\n  port: 8080\n  workers: 4\n  max_connections: 100\n  timeout: 30\n\n# NetApp configuration\nnetapp:\n  um_host: \"unified-manager.company.com\"\n  username: \"service-account\"\n  password: \"${NETAPP_PASSWORD}\"\n  verify_ssl: false\n  timeout: 30\n  max_retries: 3\n\n# Cache configuration\ncache:\n  type: \"redis\"\n  url: \"redis://localhost:6379\"\n  ttl: 300\n  max_size: 1000\n\n# Monitoring configuration\nmonitoring:\n  metrics_enabled: true\n  metrics_port: 9090\n  health_check_interval: 30\n  log_level: \"INFO\"\n</code></pre>"},{"location":"architecture/system-design/#deployment-architecture","title":"Deployment Architecture","text":""},{"location":"architecture/system-design/#container-architecture","title":"Container Architecture","text":"<pre><code>FROM python:3.11-slim\n\n# Install dependencies\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\n# Copy application\nCOPY . /app\nWORKDIR /app\n\n# Set up user\nRUN adduser --disabled-password --gecos '' netapp\nUSER netapp\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\\n  CMD curl -f http://localhost:8080/health || exit 1\n\nCMD [\"python\", \"-m\", \"netapp_mcp_server\"]\n</code></pre>"},{"location":"architecture/system-design/#kubernetes-deployment","title":"Kubernetes Deployment","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: netapp-mcp-server\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: netapp-mcp-server\n  template:\n    metadata:\n      labels:\n        app: netapp-mcp-server\n    spec:\n      containers:\n      - name: netapp-mcp-server\n        image: netapp/activeiq-mcp-server:latest\n        ports:\n        - containerPort: 8080\n        env:\n        - name: NETAPP_UM_HOST\n          valueFrom:\n            secretKeyRef:\n              name: netapp-credentials\n              key: um-host\n        resources:\n          requests:\n            memory: \"512Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"1Gi\"\n            cpu: \"500m\"\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 10\n</code></pre>"},{"location":"architecture/system-design/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"architecture/system-design/#metrics-collection","title":"Metrics Collection","text":"<ul> <li>Application Metrics: Request rates, response times, error rates</li> <li>System Metrics: CPU, memory, disk usage</li> <li>NetApp Metrics: API response times, connection status</li> <li>Business Metrics: Tool usage, user activity</li> </ul>"},{"location":"architecture/system-design/#logging-strategy","title":"Logging Strategy","text":"<pre><code>import structlog\n\nlogger = structlog.get_logger(\"netapp.mcp.server\")\n\nasync def execute_tool(tool_name: str, params: dict):\n    logger.info(\"tool.execute.start\", tool=tool_name, params=params)\n\n    try:\n        result = await tool.execute(params)\n        logger.info(\"tool.execute.success\", tool=tool_name, result_size=len(result))\n        return result\n    except Exception as e:\n        logger.error(\"tool.execute.error\", tool=tool_name, error=str(e))\n        raise\n</code></pre>"},{"location":"architecture/system-design/#health-checks","title":"Health Checks","text":"<pre><code>class HealthChecker:\n    async def check_health(self) -&gt; Dict[str, Any]:\n        checks = {\n            \"server\": await self.check_server(),\n            \"netapp\": await self.check_netapp_connection(),\n            \"cache\": await self.check_cache(),\n            \"temporal\": await self.check_temporal()\n        }\n\n        status = \"healthy\" if all(checks.values()) else \"unhealthy\"\n        return {\"status\": status, \"checks\": checks}\n</code></pre>"},{"location":"architecture/system-design/#future-enhancements","title":"Future Enhancements","text":""},{"location":"architecture/system-design/#planned-features","title":"Planned Features","text":"<ol> <li>GraphQL Support: Alternative query interface</li> <li>WebSocket Support: Real-time event streaming</li> <li>Plugin System: Extensible tool architecture</li> <li>Multi-tenant Support: Isolated environments</li> <li>Advanced Caching: Intelligent cache invalidation</li> <li>ML Integration: Predictive analytics</li> </ol>"},{"location":"architecture/system-design/#scalability-roadmap","title":"Scalability Roadmap","text":"<ol> <li>Database Layer: Persistent storage for complex queries</li> <li>Event Streaming: Kafka/Pulsar for event processing</li> <li>Microservices: Split into specialized services</li> <li>API Gateway: Centralized routing and security</li> <li>Service Mesh: Advanced networking and observability</li> </ol>"},{"location":"architecture/target-operating-model/","title":"Target Operating Model: NetApp Operations with Temporal Workflows","text":"<p>This document defines the Target Operating Model (TOM) for NetApp storage operations, showcasing three operational paradigms: traditional manual operations, MCP-enabled automation, and the advanced Temporal.io-powered durable execution model.</p>"},{"location":"architecture/target-operating-model/#executive-summary","title":"Executive Summary","text":"<p>The Target Operating Model demonstrates the evolution from manual NetApp operations to AI-assisted automation, culminating in durable, fault-tolerant workflows powered by Temporal.io. This new paradigm ensures that complex, long-running storage operations never fail permanently and can recover from any failure without data loss or manual intervention.</p>"},{"location":"architecture/target-operating-model/#operating-model-comparison","title":"Operating Model Comparison","text":""},{"location":"architecture/target-operating-model/#current-state-traditional-netapp-operations","title":"Current State: Traditional NetApp Operations","text":"<pre><code>graph TB\n    subgraph \"Traditional Operating Model\"\n        A[Storage Request] --&gt; B[Storage Admin]\n        B --&gt; C[Manual Analysis]\n        C --&gt; D[Expert Knowledge]\n        D --&gt; E[CLI/GUI Operations]\n        E --&gt; F[Manual Documentation]\n        F --&gt; G[Result Communication]\n    end\n\n    subgraph \"Challenges\"\n        H[Single Point of Failure]\n        I[Slow Response Times]\n        J[Knowledge Silos]\n        K[Human Error Risk]\n        L[Limited Scalability]\n    end</code></pre>"},{"location":"architecture/target-operating-model/#target-state-devops-driven-with-apim-integration","title":"Target State: DevOps-Driven with APIM Integration","text":"<pre><code>graph TB\n    subgraph \"DevOps-Driven Operating Model\"\n        A[DevOps GUI Request] --&gt; B[API Management APIM]\n        B --&gt; C[Temporal Workflows]\n        C --&gt; D[NetApp API Automation]\n        D --&gt; E[Real-time Analysis]\n        E --&gt; F[Automated Actions]\n        F --&gt; G[Structured Responses]\n\n        subgraph \"Optional MCP Integration\"\n            H[MCP Server on Knative]\n            C --&gt; H\n            H --&gt; D\n        end\n    end\n\n    subgraph \"Benefits\"\n        I[Self-Service DevOps]\n        J[Instant Response]\n        K[Workflow Reliability]\n        L[API Standardization]\n        M[Infinite Scalability]\n    end</code></pre>"},{"location":"architecture/target-operating-model/#new-tom-devops-primary-with-day-2-ai-integration","title":"NEW TOM: DevOps-Primary with Day-2 AI Integration","text":"<pre><code>graph TB\n    subgraph \"DevOps-Primary with Day-2 AI Model\"\n        A[DevOps GUI Request] --&gt; B[API Management APIM]\n        B --&gt; C[Temporal Workflow Engine]\n        C --&gt; D[Durable Execution]\n\n        subgraph \"Orchestrated Activities\"\n            E[Optional MCP Server Functions]\n            F[NetApp API Calls]\n            G[Validation Steps]\n            H[Rollback Logic]\n            I[Notification Services]\n        end\n\n        D --&gt; E\n        D --&gt; F\n        D --&gt; G\n        D --&gt; H\n        D --&gt; I\n\n        subgraph \"Day-2 AI Integration\"\n            J[AI Assistant]\n            K[Predictive Analysis]\n            L[Automated Approvals]\n            M[Anomaly Detection]\n            N[Capacity Planning]\n        end\n\n        B --&gt;|Day-2 Operations| J\n        J --&gt; K\n        J --&gt; L\n        J --&gt; M\n        J --&gt; N\n\n        subgraph \"Durability Features\"\n            O[Fault Tolerance]\n            P[Automatic Retries]\n            Q[State Persistence]\n            R[Partial Failure Recovery]\n            S[Workflow History]\n        end\n    end\n\n    subgraph \"Advanced Benefits\"\n        T[Never Lose Progress]\n        U[Automatic Recovery]\n        V[Complex Workflow Orchestration]\n        W[Audit Trail]\n        X[Human-in-the-Loop]\n        Y[AI-Assisted Operations]\n    end</code></pre>"},{"location":"architecture/target-operating-model/#detailed-operating-models","title":"Detailed Operating Models","text":""},{"location":"architecture/target-operating-model/#1-without-mcp-traditional-storage-operations","title":"1. WITHOUT MCP: Traditional Storage Operations","text":""},{"location":"architecture/target-operating-model/#process-flow","title":"Process Flow","text":"<pre><code>flowchart LR\n    A[Request] --&gt; B[Expert] --&gt; C[Analysis]\n    C --&gt; D[Action]\n    D --&gt; E[Documentation]\n    E --&gt; F[Communication]\n\n    A -.-&gt; A1[Manual&lt;br/&gt;Wait]\n    B -.-&gt; B1[Human&lt;br/&gt;Expert]\n    C -.-&gt; C1[Manual&lt;br/&gt;Tools]\n    D -.-&gt; D1[CLI/GUI&lt;br/&gt;Commands]\n    E -.-&gt; E1[Manual&lt;br/&gt;Notes]\n    F -.-&gt; F1[Email/Chat&lt;br/&gt;Updates]\n\n    style A fill:#ffebee\n    style B fill:#fff3e0\n    style C fill:#e8f5e8\n    style D fill:#e3f2fd\n    style E fill:#f3e5f5\n    style F fill:#fce4ec\n\n    style A1 fill:#ffcdd2\n    style B1 fill:#ffe0b2\n    style C1 fill:#c8e6c9\n    style D1 fill:#bbdefb\n    style E1 fill:#e1bee7\n    style F1 fill:#f8bbd9</code></pre>"},{"location":"architecture/target-operating-model/#operational-characteristics","title":"Operational Characteristics","text":"Aspect Traditional Model Impact Request Handling Email, ticket system, meetings Delays, context loss Analysis Manual data gathering, expert interpretation Time-consuming, error-prone Execution CLI commands, GUI operations Knowledge-dependent, manual Documentation Manual notes, spreadsheets Inconsistent, outdated Knowledge Transfer Training, documentation Slow, incomplete Scalability Limited by expert availability Bottleneck, single points of failure Response Time Hours to days Business impact, user frustration Consistency Varies by individual Quality variance"},{"location":"architecture/target-operating-model/#typical-workflow-example-volume-creation","title":"Typical Workflow Example: Volume Creation","text":"<pre><code># Traditional Process (45-60 minutes)\n1. Receive request via email/ticket (5-30 min wait)\n2. Storage admin reviews request (5 min)\n3. Check available capacity manually (5 min)\n   - ssh to cluster\n   - volume show -vserver X\n   - aggr show-space\n4. Find appropriate aggregate (5 min)\n5. Create volume via CLI (5 min)\n   - volume create -vserver X -volume Y -size Z\n6. Configure export policy (5 min)\n7. Update documentation (10 min)\n8. Notify requestor (5 min)\n</code></pre>"},{"location":"architecture/target-operating-model/#2-with-apim-devops-driven-operations-via-temporal","title":"2. WITH APIM: DevOps-Driven Operations via Temporal","text":""},{"location":"architecture/target-operating-model/#process-flow_1","title":"Process Flow","text":"<pre><code>DevOps GUI \u2192 API Management \u2192 Temporal Workflow \u2192 NetApp APIs \u2192 Structured Response\n     \u2193            \u2193                \u2193                \u2193            \u2193\n  GUI Action   Standardized      Durable         Real-time   Reliable\n   Request     API Gateway      Execution       Operations   Results\n</code></pre>"},{"location":"architecture/target-operating-model/#operational-characteristics_1","title":"Operational Characteristics","text":"Aspect DevOps-Driven Model Impact Request Handling GUI-based, structured workflows Immediate, standardized Analysis Temporal-powered workflow analysis, real-time insights Fast, comprehensive, durable Execution APIM-orchestrated calls, fault-tolerant workflows Reliable, recoverable Documentation Auto-generated workflow history, real-time updates Always current, auditable Knowledge Transfer Workflow-embedded expertise, accessible to DevOps teams Instant, comprehensive Scalability Temporal auto-scaling, concurrent workflow processing Unlimited, elastic Response Time Seconds to minutes with guaranteed completion Business enablement, reliability Consistency Workflow-standardized, best-practice embedded Uniform quality"},{"location":"architecture/target-operating-model/#typical-workflow-example-volume-creation_1","title":"Typical Workflow Example: Volume Creation","text":"<pre><code># DevOps-Driven Process (2-3 minutes)\nDevOps Engineer: Initiates volume creation via GUI form\n\nAPI Management + Temporal Workflow:\n1. Validates request parameters (5 seconds)\n2. Starts VolumeCreation Temporal workflow (instant)\n3. Temporal orchestrates activities:\n   - validate_cluster_health() - ensure cluster availability\n   - find_optimal_aggregate() - identify fastest aggregate\n   - create_volume_activity() - provision with best practices\n   - configure_access_activity() - set up appropriate permissions\n   - enable_monitoring_activity() - configure monitoring\n4. Returns complete configuration details (30 seconds)\n5. Auto-documents workflow history and results (5 seconds)\n6. Optional: AI assistant monitors for Day-2 optimization\n</code></pre>"},{"location":"architecture/target-operating-model/#3-new-tom-devops-primary-with-day-2-ai-operations","title":"3. NEW TOM: DevOps-Primary with Day-2 AI Operations","text":""},{"location":"architecture/target-operating-model/#process-flow_2","title":"Process Flow","text":"<pre><code>DevOps GUI \u2192 API Management \u2192 Temporal Workflow \u2192 Durable Activities \u2192 Guaranteed Completion\n     \u2193            \u2193                \u2193                 \u2193                  \u2193\n  Structured   Standardized       Workflow          Optional MCP       Resilient\n  Requests     API Gateway       Orchestration     + NetApp APIs      Execution\n\n                                      \u2193\n                              Day-2 AI Assistant\n                                      \u2193\n                          Predictive Analysis &amp; Optimization\n</code></pre>"},{"location":"architecture/target-operating-model/#operational-characteristics_2","title":"Operational Characteristics","text":"Aspect DevOps-Primary + Day-2 AI Model Impact Request Handling GUI-driven durable workflow execution, guaranteed completion Never lose requests, structured input validation Analysis Multi-step workflows with checkpoints + AI insights Fault-tolerant analysis with predictive intelligence Execution APIM-orchestrated activities with automatic retries Self-healing operations with AI monitoring Documentation Complete workflow history + AI-generated insights Perfect audit trail with intelligent analysis Knowledge Transfer Workflow-embedded expertise + AI learning Evolutionary improvement with machine learning Scalability Unlimited concurrent workflows + AI optimization Handle enterprise-scale with intelligent resource allocation Response Time Seconds for simple, minutes-hours for complex + AI acceleration Appropriate timing with AI-assisted optimization Consistency Deterministic execution + AI quality assurance Perfect consistency with intelligent validation"},{"location":"architecture/target-operating-model/#advanced-workflow-example-complete-svm-environment-setup","title":"Advanced Workflow Example: Complete SVM Environment Setup","text":"<pre><code># DevOps-Driven Temporal Workflow (30-45 minutes for complex setup)\nDevOps Engineer: Submits SVM environment request via GUI with form validation\n\nTemporal Workflow Execution via APIM:\n1. Parse GUI requirements and validate (Activity 1 - 30 seconds)\n2. Check cluster capacity across multiple sites (Activity 2 - 1 minute)\n3. Select optimal cluster and aggregate (Activity 3 - 30 seconds)\n4. Create SVM with NFS and CIFS protocols (Activity 4 - 2 minutes)\n5. Configure network interfaces with failover (Activity 5 - 3 minutes)\n6. Provision multiple volumes with different tiers (Activity 6 - 5 minutes)\n7. Set up snapshot policies and schedules (Activity 7 - 2 minutes)\n8. Configure export policies and access controls (Activity 8 - 3 minutes)\n9. Integrate with Active Directory (Activity 9 - 5 minutes)\n10. Set up monitoring and alerting (Activity 10 - 2 minutes)\n11. Run validation tests (Activity 11 - 3 minutes)\n12. Generate documentation and access guides (Activity 12 - 1 minute)\n13. Notify team with complete setup details (Activity 13 - 30 seconds)\n\nDay-2 AI Integration:\n- AI monitors workflow execution patterns for optimization\n- Provides predictive capacity planning recommendations\n- Suggests performance tuning based on usage patterns\n- Automates routine maintenance workflows\n\n# If any step fails, Temporal automatically retries with exponential backoff\n# Workflow can be paused for human approval and resumed\n# Complete history is preserved for audit and debugging\n# AI provides continuous optimization insights\n</code></pre>"},{"location":"architecture/target-operating-model/#temporalio-architecture-integration","title":"Temporal.io Architecture Integration","text":""},{"location":"architecture/target-operating-model/#temporal-workflow-engine-with-netapp-operations","title":"Temporal Workflow Engine with NetApp Operations","text":"<pre><code># Temporal Server Configuration\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: temporal-server\nspec:\n  replicas: 3\n  template:\n    spec:\n      containers:\n        - name: temporal-server\n          image: temporalio/auto-setup:latest\n          env:\n            - name: DB\n              value: \"postgresql\"\n            - name: POSTGRES_SEEDS\n              value: \"postgres-service\"\n          ports:\n            - containerPort: 7233\n            - containerPort: 8080\n---\n# NetApp Workflow Worker\napiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: netapp-temporal-worker\nspec:\n  template:\n    spec:\n      containers:\n        - image: netapp/temporal-worker:latest\n          env:\n            - name: TEMPORAL_HOST\n              value: \"temporal-server:7233\"\n            - name: NETAPP_API_ENDPOINT\n              valueFrom:\n                secretKeyRef:\n                  name: netapp-credentials\n                  key: endpoint\n</code></pre>"},{"location":"architecture/target-operating-model/#durable-workflow-benefits","title":"Durable Workflow Benefits","text":""},{"location":"architecture/target-operating-model/#fault-tolerance","title":"\ud83d\udd04 Fault Tolerance","text":"<ul> <li>Automatic Retries: Failed activities retry with exponential backoff</li> <li>Partial Failure Recovery: Resume from last successful checkpoint</li> <li>Worker Failures: Workflows survive worker restarts and deployments</li> <li>Infrastructure Failures: Persist through cluster outages</li> </ul>"},{"location":"architecture/target-operating-model/#complex-orchestration","title":"\ud83d\udcc8 Complex Orchestration","text":"<ul> <li>Multi-Step Processes: Coordinate dozens of NetApp operations</li> <li>Conditional Logic: Smart decision-making based on intermediate results</li> <li>Human-in-the-Loop: Pause for approvals, resume automatically</li> <li>Parallel Execution: Run independent operations concurrently</li> </ul>"},{"location":"architecture/target-operating-model/#observability","title":"\ud83d\udd0d Observability","text":"<ul> <li>Workflow History: Complete execution trace with timing</li> <li>State Inspection: View current workflow state and variables</li> <li>Replay Capability: Debug issues by replaying exact execution</li> <li>Metrics Integration: Built-in metrics for monitoring</li> </ul>"},{"location":"architecture/target-operating-model/#long-running-operations","title":"\u23f1\ufe0f Long-Running Operations","text":"<ul> <li>Days/Weeks Duration: Handle migration and upgrade workflows</li> <li>Scheduled Operations: Cron-like scheduling with durability</li> <li>Event-Driven: React to NetApp events and external triggers</li> <li>Resource Cleanup: Automatic cleanup on workflow completion</li> </ul>"},{"location":"architecture/target-operating-model/#temporal-workflow-patterns-for-netapp","title":"Temporal Workflow Patterns for NetApp","text":""},{"location":"architecture/target-operating-model/#1-simple-request-response-pattern","title":"1. Simple Request-Response Pattern","text":"<pre><code>@workflow.defn\nclass SimpleVolumeCreation:\n    @workflow.run\n    async def run(self, volume_request: VolumeRequest) -&gt; VolumeResult:\n        # Single activity execution\n        return await workflow.execute_activity(\n            create_volume_activity,\n            volume_request,\n            start_to_close_timeout=timedelta(minutes=5)\n        )\n\n@activity.defn\nasync def create_volume_activity(request: VolumeRequest) -&gt; VolumeResult:\n    # Call MCP server\n    mcp_client = get_mcp_client()\n    result = await mcp_client.call_tool(\"create_volume\", request.dict())\n    return VolumeResult.parse_obj(result)\n</code></pre>"},{"location":"architecture/target-operating-model/#2-complex-multi-step-workflow","title":"2. Complex Multi-Step Workflow","text":"<pre><code>@workflow.defn\nclass SVMEnvironmentSetup:\n    @workflow.run\n    async def run(self, setup_request: SVMSetupRequest) -&gt; SVMSetupResult:\n        workflow.logger.info(f\"Starting SVM setup for {setup_request.team_name}\")\n\n        # Step 1: Validate requirements\n        validation = await workflow.execute_activity(\n            validate_requirements,\n            setup_request,\n            start_to_close_timeout=timedelta(minutes=2)\n        )\n\n        if not validation.valid:\n            raise ApplicationError(f\"Validation failed: {validation.errors}\")\n\n        # Step 2: Resource allocation\n        allocation = await workflow.execute_activity(\n            allocate_resources,\n            setup_request,\n            start_to_close_timeout=timedelta(minutes=5)\n        )\n\n        # Step 3: Create SVM\n        svm = await workflow.execute_activity(\n            create_svm,\n            allocation,\n            start_to_close_timeout=timedelta(minutes=10)\n        )\n\n        # Step 4: Configure networking (parallel activities)\n        network_tasks = [\n            workflow.execute_activity(\n                create_management_lif,\n                svm.uuid,\n                start_to_close_timeout=timedelta(minutes=5)\n            ),\n            workflow.execute_activity(\n                create_data_lifs,\n                svm.uuid,\n                start_to_close_timeout=timedelta(minutes=5)\n            )\n        ]\n\n        await asyncio.gather(*network_tasks)\n\n        # Step 5: Human approval for production\n        if setup_request.environment == \"production\":\n            approval = await workflow.execute_activity(\n                request_approval,\n                f\"SVM {svm.name} ready for production deployment\",\n                start_to_close_timeout=timedelta(hours=24)  # Wait up to 24 hours\n            )\n\n            if not approval.approved:\n                # Cleanup resources\n                await workflow.execute_activity(\n                    cleanup_svm,\n                    svm.uuid,\n                    start_to_close_timeout=timedelta(minutes=10)\n                )\n                raise ApplicationError(\"Setup not approved\")\n\n        # Step 6: Final configuration\n        final_config = await workflow.execute_activity(\n            finalize_setup,\n            svm.uuid,\n            start_to_close_timeout=timedelta(minutes=15)\n        )\n\n        # Step 7: Notification\n        await workflow.execute_activity(\n            send_completion_notification,\n            final_config,\n            start_to_close_timeout=timedelta(minutes=2)\n        )\n\n        return SVMSetupResult(\n            svm_uuid=svm.uuid,\n            svm_name=svm.name,\n            access_details=final_config.access_details,\n            setup_duration=workflow.now() - workflow.start_time\n        )\n</code></pre>"},{"location":"architecture/target-operating-model/#3-event-driven-workflow","title":"3. Event-Driven Workflow","text":"<pre><code>@workflow.defn\nclass CapacityManagement:\n    @workflow.run\n    async def run(self, cluster_uuid: str) -&gt; None:\n        # Continuous workflow for capacity management\n        while True:\n            # Check capacity every hour\n            capacity_status = await workflow.execute_activity(\n                check_cluster_capacity,\n                cluster_uuid,\n                start_to_close_timeout=timedelta(minutes=5)\n            )\n\n            if capacity_status.utilization &gt; 80:\n                # Trigger expansion workflow\n                await workflow.execute_child_workflow(\n                    CapacityExpansionWorkflow.run,\n                    cluster_uuid,\n                    id=f\"expansion-{cluster_uuid}-{workflow.now()}\"\n                )\n\n            # Wait for next check or external signal\n            try:\n                await workflow.wait_condition(\n                    lambda: False,  # Never true, so always timeout\n                    timeout=timedelta(hours=1)\n                )\n            except TimeoutError:\n                continue  # Normal hourly check\n\n            # Handle external signals\n            signals = workflow.list_all_signals()\n            for signal in signals:\n                if signal.name == \"emergency_expansion\":\n                    await workflow.execute_child_workflow(\n                        EmergencyExpansionWorkflow.run,\n                        signal.data\n                    )\n</code></pre>"},{"location":"architecture/target-operating-model/#enhanced-operational-metrics-with-temporal","title":"Enhanced Operational Metrics with Temporal","text":"Metric Without MCP With MCP With Temporal Total Improvement Request Response Time 2-48 hours 30 seconds - 5 minutes 30 seconds - 45 minutes* 95%+ reduction Success Rate 70-80% (human error) 95% (automated) 99.9% (durable execution) 99%+ improvement Complex Workflow Execution Days/weeks Not supported Hours with fault tolerance 90%+ reduction Audit Compliance Manual, incomplete Basic logging Complete workflow history Perfect compliance Recovery from Failures Manual intervention Restart from beginning Resume from checkpoint 100% improvement Concurrent Operations 1-2 per admin 100s Unlimited with coordination Unlimited scaling <p>*Complex workflows like complete environment setup can take 30-45 minutes but run reliably with automatic recovery</p>"},{"location":"architecture/target-operating-model/#implementation-roadmap-with-temporal","title":"Implementation Roadmap with Temporal","text":""},{"location":"architecture/target-operating-model/#phase-1-foundation-temporal-months-1-3","title":"Phase 1: Foundation + Temporal (Months 1-3)","text":"<ol> <li> <p>Infrastructure Setup</p> </li> <li> <p>Deploy Temporal.io cluster</p> </li> <li>Set up Knative for MCP functions</li> <li> <p>Configure PostgreSQL for Temporal persistence</p> </li> <li> <p>Basic Workflows</p> </li> <li> <p>Simple volume operations</p> </li> <li>SVM creation with rollback</li> <li> <p>Capacity monitoring workflows</p> </li> <li> <p>Integration Testing</p> </li> <li>Failure simulation and recovery</li> <li>Workflow replay and debugging</li> <li>Performance optimization</li> </ol>"},{"location":"architecture/target-operating-model/#phase-2-advanced-workflows-months-4-6","title":"Phase 2: Advanced Workflows (Months 4-6)","text":"<ol> <li> <p>Complex Orchestration</p> </li> <li> <p>Multi-site disaster recovery setup</p> </li> <li>Large-scale data migration</li> <li> <p>Scheduled maintenance workflows</p> </li> <li> <p>Human Integration</p> </li> <li> <p>Approval workflows</p> </li> <li>Exception handling</li> <li> <p>Manual override capabilities</p> </li> <li> <p>Enterprise Features</p> </li> <li>Multi-tenant workflow isolation</li> <li>Advanced monitoring and alerting</li> <li>Compliance reporting</li> </ol>"},{"location":"architecture/target-operating-model/#phase-3-advanced-ai-integration-months-7-9","title":"Phase 3: Advanced AI Integration (Months 7-9)","text":"<ol> <li> <p>Intelligent Workflows</p> </li> <li> <p>AI-driven capacity planning</p> </li> <li>Predictive maintenance workflows</li> <li> <p>Self-optimizing configurations</p> </li> <li> <p>Machine Learning Integration</p> </li> <li>Performance pattern recognition</li> <li>Anomaly detection workflows</li> <li>Automated tuning recommendations</li> </ol>"},{"location":"architecture/target-operating-model/#business-impact-with-temporal-integration","title":"Business Impact with Temporal Integration","text":""},{"location":"architecture/target-operating-model/#risk-elimination","title":"Risk Elimination","text":"<ul> <li>Zero Data Loss: Workflows guarantee completion or clean rollback</li> <li>No Lost Work: Failed operations resume from last checkpoint</li> <li>Audit Perfection: Complete history of all operations</li> <li>Compliance Automation: Workflows enforce compliance automatically</li> </ul>"},{"location":"architecture/target-operating-model/#operational-excellence","title":"Operational Excellence","text":"<ul> <li>Complex Operations: Handle enterprise-scale migrations and setups</li> <li>Unattended Operations: Workflows run 24/7 without human intervention</li> <li>Smart Recovery: Automatic handling of transient failures</li> <li>Scalable Orchestration: Coordinate hundreds of parallel operations</li> </ul>"},{"location":"architecture/target-operating-model/#business-agility","title":"Business Agility","text":"<ul> <li>Rapid Environment Provisioning: Complete setups in under an hour</li> <li>Reliable Scaling: Predictable capacity expansion</li> <li>Fast Recovery: Automatic disaster recovery workflows</li> <li>Innovation Enablement: Focus on business logic, not infrastructure</li> </ul> <p>This enhanced Target Operating Model with Temporal.io represents the ultimate evolution of NetApp operations: durable, fault-tolerant, and infinitely scalable storage management that never fails permanently and enables operations at enterprise scale with perfect reliability.</p>"},{"location":"architecture/target-operating-model/#knative-function-architecture","title":"Knative Function Architecture","text":""},{"location":"architecture/target-operating-model/#mcp-server-as-knative-service","title":"MCP Server as Knative Service","text":"<pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: netapp-mcp-server\nspec:\n  template:\n    metadata:\n      annotations:\n        autoscaling.knative.dev/minScale: \"0\"\n        autoscaling.knative.dev/maxScale: \"100\"\n        autoscaling.knative.dev/target: \"10\"\n    spec:\n      containers:\n        - image: netapp/mcp-server:latest\n          env:\n            - name: NETAPP_API_ENDPOINT\n              valueFrom:\n                secretKeyRef:\n                  name: netapp-credentials\n                  key: endpoint\n          resources:\n            requests:\n              memory: \"256Mi\"\n              cpu: \"200m\"\n            limits:\n              memory: \"512Mi\"\n              cpu: \"500m\"\n</code></pre>"},{"location":"architecture/target-operating-model/#benefits-of-knative-deployment","title":"Benefits of Knative Deployment","text":""},{"location":"architecture/target-operating-model/#auto-scaling","title":"\ud83d\ude80 Auto-Scaling","text":"<ul> <li>Scale to Zero: No resources consumed when idle</li> <li>Instant Scale-Up: Automatic scaling based on demand</li> <li>Cost Optimization: Pay only for actual usage</li> <li>High Availability: Built-in redundancy and failover</li> </ul>"},{"location":"architecture/target-operating-model/#performance","title":"\u26a1 Performance","text":"<ul> <li>Cold Start Optimization: Sub-second startup times</li> <li>Request Routing: Intelligent traffic distribution</li> <li>Resource Efficiency: Optimal resource utilization</li> <li>Concurrent Processing: Handle multiple requests simultaneously</li> </ul>"},{"location":"architecture/target-operating-model/#operations","title":"\ud83d\udd27 Operations","text":"<ul> <li>Simplified Deployment: GitOps-enabled deployments</li> <li>Version Management: Blue-green deployments</li> <li>Health Monitoring: Built-in health checks</li> <li>Observability: Comprehensive metrics and logging</li> </ul>"},{"location":"architecture/target-operating-model/#organizational-impact","title":"Organizational Impact","text":""},{"location":"architecture/target-operating-model/#role-transformation","title":"Role Transformation","text":""},{"location":"architecture/target-operating-model/#traditional-roles","title":"Traditional Roles","text":"<pre><code>Storage Administrator:\n- Manual operation execution\n- Expert knowledge keeper\n- Bottleneck for requests\n- Documentation maintainer\n\nApplication Teams:\n- Dependent on storage experts\n- Long wait times for storage\n- Limited storage visibility\n- Manual coordination required\n</code></pre>"},{"location":"architecture/target-operating-model/#mcp-enabled-roles","title":"MCP-Enabled Roles","text":"<pre><code>Storage Administrator:\n- Solution architect and optimizer\n- Policy and governance designer\n- Exception handler\n- Strategic planner\n\nApplication Teams:\n- Self-service storage access\n- Real-time storage insights\n- Autonomous problem solving\n- Focus on business logic\n</code></pre>"},{"location":"architecture/target-operating-model/#operational-metrics-comparison","title":"Operational Metrics Comparison","text":"Metric Without MCP With MCP Improvement Request Response Time 2-48 hours 30 seconds - 5 minutes 95%+ reduction Storage Admin Utilization 80% routine tasks 20% routine tasks 4x efficiency gain Error Rate 5-10% human error &lt;1% automated error 90%+ reduction Knowledge Transfer Time Weeks to months Instant access Immediate After-Hours Support On-call expert required 24/7 automated Always available Scaling Capacity Linear with headcount Exponential with automation Unlimited"},{"location":"architecture/target-operating-model/#business-value-realization","title":"Business Value Realization","text":""},{"location":"architecture/target-operating-model/#financial-impact","title":"Financial Impact","text":"<ul> <li>Cost Reduction: 60-80% reduction in operational overhead</li> <li>Revenue Protection: 95% reduction in storage-related outages</li> <li>Productivity Gains: 4x improvement in storage admin efficiency</li> <li>Innovation Acceleration: Teams focus on value-add activities</li> </ul>"},{"location":"architecture/target-operating-model/#operational-excellence_1","title":"Operational Excellence","text":"<ul> <li>Service Quality: Consistent, best-practice operations</li> <li>Compliance: Automated compliance and audit trails</li> <li>Risk Reduction: Elimination of human error</li> <li>Knowledge Preservation: Expertise embedded in automation</li> </ul>"},{"location":"architecture/target-operating-model/#implementation-strategy","title":"Implementation Strategy","text":""},{"location":"architecture/target-operating-model/#phase-1-foundation-months-1-2","title":"Phase 1: Foundation (Months 1-2)","text":"<ol> <li> <p>Knative Platform Setup</p> </li> <li> <p>Deploy Knative Serving on Kubernetes</p> </li> <li>Configure autoscaling and observability</li> <li> <p>Establish CI/CD pipelines</p> </li> <li> <p>MCP Server Development</p> </li> <li> <p>Build NetApp API integration</p> </li> <li>Implement core storage operations</li> <li> <p>Develop error handling and logging</p> </li> <li> <p>Initial Use Cases</p> </li> <li>Storage monitoring and reporting</li> <li>Basic volume operations</li> <li>Capacity planning queries</li> </ol>"},{"location":"architecture/target-operating-model/#phase-2-expansion-months-3-4","title":"Phase 2: Expansion (Months 3-4)","text":"<ol> <li> <p>Advanced Operations</p> </li> <li> <p>SVM management and provisioning</p> </li> <li>Performance optimization workflows</li> <li> <p>Backup and recovery automation</p> </li> <li> <p>Integration Development</p> </li> <li> <p>ITSM system integration</p> </li> <li>Monitoring tool connectivity</li> <li> <p>Notification and alerting</p> </li> <li> <p>Self-Service Portal</p> </li> <li>Web interface for non-technical users</li> <li>Mobile access capabilities</li> <li>Role-based access controls</li> </ol>"},{"location":"architecture/target-operating-model/#phase-3-optimization-months-5-6","title":"Phase 3: Optimization (Months 5-6)","text":"<ol> <li> <p>AI Enhancement</p> </li> <li> <p>Predictive analytics integration</p> </li> <li>Machine learning model deployment</li> <li> <p>Intelligent recommendation engine</p> </li> <li> <p>Process Automation</p> </li> <li> <p>End-to-end workflow automation</p> </li> <li>Exception handling processes</li> <li> <p>Continuous optimization loops</p> </li> <li> <p>Enterprise Integration</p> </li> <li>Enterprise service bus connectivity</li> <li>Master data management integration</li> <li>Business intelligence reporting</li> </ol>"},{"location":"architecture/target-operating-model/#governance-and-control","title":"Governance and Control","text":""},{"location":"architecture/target-operating-model/#without-mcp-manual-governance","title":"Without MCP: Manual Governance","text":"<ul> <li>Access Control: Manual user management</li> <li>Change Management: Paper-based approval processes</li> <li>Audit Trails: Manual documentation, often incomplete</li> <li>Compliance: Periodic manual reviews</li> <li>Policy Enforcement: Relies on human adherence</li> </ul>"},{"location":"architecture/target-operating-model/#with-mcp-automated-governance","title":"With MCP: Automated Governance","text":"<ul> <li>Access Control: Role-based automated access with audit trails</li> <li>Change Management: Automated approval workflows with full traceability</li> <li>Audit Trails: Complete API-level logging and monitoring</li> <li>Compliance: Continuous compliance monitoring and reporting</li> <li>Policy Enforcement: Automated policy implementation and validation</li> </ul>"},{"location":"architecture/target-operating-model/#risk-management","title":"Risk Management","text":""},{"location":"architecture/target-operating-model/#risk-mitigation-strategies","title":"Risk Mitigation Strategies","text":""},{"location":"architecture/target-operating-model/#technical-risks","title":"Technical Risks","text":"<ul> <li>Service Availability: Knative multi-zone deployment with health checks</li> <li>Data Security: End-to-end encryption and secure credential management</li> <li>Performance: Auto-scaling and resource optimization</li> <li>Integration Failures: Circuit breakers and fallback mechanisms</li> </ul>"},{"location":"architecture/target-operating-model/#operational-risks","title":"Operational Risks","text":"<ul> <li>Change Management: GitOps deployment with rollback capabilities</li> <li>Knowledge Loss: Expertise embedded in automation</li> <li>Dependency Management: Multi-vendor API abstraction</li> <li>Business Continuity: Disaster recovery and backup procedures</li> </ul>"},{"location":"architecture/target-operating-model/#success-metrics","title":"Success Metrics","text":""},{"location":"architecture/target-operating-model/#technical-kpis","title":"Technical KPIs","text":"<ul> <li>Response Time: &lt;30 seconds for 95% of requests</li> <li>Availability: 99.9% uptime SLA</li> <li>Scalability: Support 10x current request volume</li> <li>Accuracy: &lt;1% error rate in automated operations</li> </ul>"},{"location":"architecture/target-operating-model/#business-kpis","title":"Business KPIs","text":"<ul> <li>User Satisfaction: &gt;90% satisfaction score</li> <li>Cost Reduction: &gt;60% reduction in operational costs</li> <li>Time to Value: &lt;5 minutes for standard requests</li> <li>Innovation Rate: 4x increase in new storage services</li> </ul> <p>This Target Operating Model demonstrates how Knative-deployed MCP servers transform NetApp operations from expert-dependent manual processes to democratized, automated, and scalable storage services that enable business agility and operational excellence.</p>"},{"location":"architecture/technical-documentation/","title":"NetApp ActiveIQ MCP Server - Technical Documentation","text":""},{"location":"architecture/technical-documentation/#overview","title":"Overview","text":"<p>This document provides comprehensive technical documentation for the NetApp ActiveIQ MCP Server, including detailed component models, sequence diagrams, and architectural patterns for the Temporal.io-powered durable execution system.</p>"},{"location":"architecture/technical-documentation/#component-model-cmp-xxxx","title":"Component Model (CMP-XXXX)","text":""},{"location":"architecture/technical-documentation/#component-registry","title":"Component Registry","text":"Component ID Component Name Type Purpose Dependencies CMP-0001 AI Assistant Interface External Natural language processing and user interaction Claude Desktop, Custom AI Apps CMP-0002 MCP Protocol Gateway Service MCP protocol handling and routing CMP-0003, CMP-0004 CMP-0003 Request Router Service Route MCP requests to appropriate handlers CMP-0005, CMP-0006 CMP-0004 Schema Validator Service Validate MCP messages and tool arguments JSON Schema Library CMP-0005 Tool Registry Service Manage and discover available MCP tools CMP-0008, CMP-0009 CMP-0006 Response Formatter Service Format responses according to MCP specification CMP-0003 CMP-0007 Authentication Manager Service Handle NetApp API authentication and security CMP-0010, CMP-0015 CMP-0008 NetApp API Client Service Interface with ActiveIQ Unified Manager API CMP-0007, CMP-0016 CMP-0009 Temporal Workflow Engine Platform Orchestrate durable, fault-tolerant workflows CMP-0010, CMP-0011, CMP-0012 CMP-0010 Workflow Activities Service Execute individual NetApp operations CMP-0008, CMP-0013 CMP-0011 Temporal Workers Service Process workflow activities and maintain state CMP-0009, CMP-0010 CMP-0012 Workflow State Store Data Persist workflow state and execution history PostgreSQL Database CMP-0013 Cache Manager Service Manage response caching and performance optimization Redis, CMP-0008 CMP-0014 Event Processor Service Handle NetApp events and trigger workflows CMP-0009, CMP-0016 CMP-0015 Security Context Service Manage credentials, certificates, and access control Kubernetes Secrets, Vault CMP-0016 NetApp Infrastructure External ActiveIQ Unified Manager and ONTAP clusters External NetApp Systems CMP-0017 Knative Serving Platform Serverless container orchestration and auto-scaling Kubernetes, Istio/Kourier CMP-0018 Monitoring Stack Platform Observability, metrics, and alerting Prometheus, Grafana, Jaeger CMP-0019 Configuration Manager Service Manage environment-specific configurations ConfigMaps, Secrets CMP-0020 Health Monitor Service System health checks and readiness probes CMP-0008, CMP-0009, CMP-0013"},{"location":"architecture/technical-documentation/#component-architecture-diagram","title":"Component Architecture Diagram","text":"<pre><code>graph TB\n    subgraph \"External Layer\"\n        CMP0001[CMP-0001: AI Assistant Interface]\n        CMP0016[CMP-0016: NetApp Infrastructure]\n    end\n\n    subgraph \"API Gateway Layer\"\n        CMP0002[CMP-0002: MCP Protocol Gateway]\n        CMP0003[CMP-0003: Request Router]\n        CMP0004[CMP-0004: Schema Validator]\n        CMP0006[CMP-0006: Response Formatter]\n    end\n\n    subgraph \"Service Layer\"\n        CMP0005[CMP-0005: Tool Registry]\n        CMP0007[CMP-0007: Authentication Manager]\n        CMP0008[CMP-0008: NetApp API Client]\n        CMP0013[CMP-0013: Cache Manager]\n        CMP0014[CMP-0014: Event Processor]\n        CMP0020[CMP-0020: Health Monitor]\n    end\n\n    subgraph \"Workflow Orchestration Layer\"\n        CMP0009[CMP-0009: Temporal Workflow Engine]\n        CMP0010[CMP-0010: Workflow Activities]\n        CMP0011[CMP-0011: Temporal Workers]\n    end\n\n    subgraph \"Data Layer\"\n        CMP0012[CMP-0012: Workflow State Store]\n    end\n\n    subgraph \"Platform Layer\"\n        CMP0017[CMP-0017: Knative Serving]\n        CMP0015[CMP-0015: Security Context]\n        CMP0018[CMP-0018: Monitoring Stack]\n        CMP0019[CMP-0019: Configuration Manager]\n    end\n\n    CMP0001 --&gt; CMP0002\n    CMP0002 --&gt; CMP0003\n    CMP0003 --&gt; CMP0004\n    CMP0003 --&gt; CMP0005\n    CMP0003 --&gt; CMP0006\n    CMP0005 --&gt; CMP0009\n    CMP0007 --&gt; CMP0008\n    CMP0008 --&gt; CMP0016\n    CMP0009 --&gt; CMP0010\n    CMP0009 --&gt; CMP0011\n    CMP0009 --&gt; CMP0012\n    CMP0010 --&gt; CMP0008\n    CMP0010 --&gt; CMP0013\n    CMP0011 --&gt; CMP0012\n    CMP0014 --&gt; CMP0009\n    CMP0016 --&gt; CMP0014</code></pre>"},{"location":"architecture/technical-documentation/#detailed-sequence-diagrams","title":"Detailed Sequence Diagrams","text":""},{"location":"architecture/technical-documentation/#1-simple-volume-creation-workflow-devops-gui-primary","title":"1. Simple Volume Creation Workflow (DevOps GUI Primary)","text":"<pre><code>sequenceDiagram\n    participant DevOps as DevOps GUI\n    participant APIM as API Management\n    participant Temporal as CMP-0009: Temporal Engine\n    participant Worker as CMP-0011: Temporal Worker\n    participant Activities as CMP-0010: Workflow Activities\n    participant MCP as CMP-0002: MCP Gateway (Optional)\n    participant Client as CMP-0008: NetApp Client\n    participant NetApp as CMP-0016: NetApp Infrastructure\n\n    Note over DevOps,NetApp: DevOps-Driven Volume Creation (2-3 minutes)\n\n    DevOps-&gt;&gt;APIM: POST /api/v1/workflows/create-volume\n    Note right of DevOps: {\"name\": \"db-vol-001\", \"size\": \"500GB\", \"svm\": \"prod-svm\"}\n\n    APIM-&gt;&gt;Temporal: Start VolumeCreation Workflow\n    Temporal-&gt;&gt;Worker: Assign Volume Creation Task\n\n    Worker-&gt;&gt;Activities: Execute validate_volume_request\n    Activities-&gt;&gt;Client: Validate Volume Parameters\n    Client-&gt;&gt;NetApp: GET /datacenter/storage/aggregates\n    NetApp--&gt;&gt;Client: Available Aggregates\n    Activities--&gt;&gt;Worker: Validation Complete\n\n    Worker-&gt;&gt;Activities: Execute create_volume_activity\n\n    alt MCP Integration Available\n        Activities-&gt;&gt;MCP: Call MCP create_volume tool\n        MCP-&gt;&gt;Client: Execute NetApp API Call\n    else Direct API Integration\n        Activities-&gt;&gt;Client: Direct NetApp API Call\n    end\n\n    Client-&gt;&gt;NetApp: POST /datacenter/storage/volumes\n    Note right of Client: Create volume with optimal aggregate\n    NetApp--&gt;&gt;Client: Volume Created (UUID: vol-123)\n\n    Client-&gt;&gt;NetApp: GET /datacenter/storage/volumes/vol-123\n    NetApp--&gt;&gt;Client: Volume Details\n\n    Activities--&gt;&gt;Worker: Volume Creation Complete\n    Worker--&gt;&gt;Temporal: Workflow Success\n    Temporal--&gt;&gt;APIM: Volume Created Response\n    APIM--&gt;&gt;DevOps: Volume: db-vol-001 (500GB) created successfully\n\n    Note over DevOps: DevOps validates volume creation in GUI dashboard</code></pre>"},{"location":"architecture/technical-documentation/#2-complex-svm-environment-setup-with-temporal-devops-primary","title":"2. Complex SVM Environment Setup with Temporal (DevOps Primary)","text":"<pre><code>sequenceDiagram\n    participant DevOps as DevOps GUI\n    participant APIM as API Management\n    participant Temporal as CMP-0009: Temporal Engine\n    participant Worker as CMP-0011: Temporal Worker\n    participant Activities as CMP-0010: Workflow Activities\n    participant Client as CMP-0008: NetApp Client\n    participant State as CMP-0012: State Store\n    participant NetApp as CMP-0016: NetApp Infrastructure\n    participant EventProc as CMP-0014: Event Processor\n\n    Note over DevOps,EventProc: DevOps-Driven SVM Environment Setup (30-45 minutes)\n\n    DevOps-&gt;&gt;APIM: POST /api/v1/workflows/svm-environment-setup\n    Note right of DevOps: {\"team\": \"mobile-dev\", \"environment\": \"development\", \"protocols\": [\"nfs\", \"cifs\"]}\n\n    APIM-&gt;&gt;Temporal: Start SVMEnvironmentSetup Workflow\n\n    Temporal-&gt;&gt;State: Persist Workflow State\n    State--&gt;&gt;Temporal: State Saved\n\n    Temporal-&gt;&gt;Worker: Assign Workflow\n    Worker-&gt;&gt;Activities: Execute validate_requirements\n\n    Activities-&gt;&gt;Client: Validate Team Requirements\n    Client-&gt;&gt;NetApp: GET /datacenter/cluster/clusters\n    NetApp--&gt;&gt;Client: Cluster Information\n    Activities--&gt;&gt;Worker: Validation Complete\n\n    Worker-&gt;&gt;State: Update Workflow State\n    Worker-&gt;&gt;Activities: Execute allocate_resources\n\n    Activities-&gt;&gt;Client: Find Optimal Resources\n    Client-&gt;&gt;NetApp: GET /datacenter/storage/aggregates\n    NetApp--&gt;&gt;Client: Available Aggregates\n    Activities--&gt;&gt;Worker: Resources Allocated\n\n    Worker-&gt;&gt;State: Update Workflow State\n    Worker-&gt;&gt;Activities: Execute create_svm\n\n    Activities-&gt;&gt;Client: Create SVM\n    Client-&gt;&gt;NetApp: POST /datacenter/svm/svms\n    NetApp--&gt;&gt;Client: SVM Created (UUID: svm-456)\n    Activities--&gt;&gt;Worker: SVM Creation Complete\n\n    Worker-&gt;&gt;State: Update Workflow State\n\n    par Parallel Network Configuration\n        Worker-&gt;&gt;Activities: Execute create_management_lif\n        Activities-&gt;&gt;Client: Create Management LIF\n        Client-&gt;&gt;NetApp: POST /network/ip/interfaces\n        NetApp--&gt;&gt;Client: Management LIF Created\n        Activities--&gt;&gt;Worker: Management LIF Ready\n    and\n        Worker-&gt;&gt;Activities: Execute create_data_lifs\n        Activities-&gt;&gt;Client: Create Data LIFs\n        Client-&gt;&gt;NetApp: POST /network/ip/interfaces (x2)\n        NetApp--&gt;&gt;Client: Data LIFs Created\n        Activities--&gt;&gt;Worker: Data LIFs Ready\n    end\n\n    Worker-&gt;&gt;State: Update Workflow State\n    Worker-&gt;&gt;Activities: Execute provision_volumes\n\n    Activities-&gt;&gt;Client: Create Development Volumes\n    Client-&gt;&gt;NetApp: POST /datacenter/storage/volumes (x3)\n    NetApp--&gt;&gt;Client: Volumes Created\n    Activities--&gt;&gt;Worker: Volumes Provisioned\n\n    Worker-&gt;&gt;State: Update Workflow State\n    Worker-&gt;&gt;Activities: Execute configure_snapshots\n\n    Activities-&gt;&gt;Client: Configure Snapshot Policies\n    Client-&gt;&gt;NetApp: POST /storage/snapshot-policies\n    NetApp--&gt;&gt;Client: Policies Configured\n    Activities--&gt;&gt;Worker: Snapshots Configured\n\n    Worker-&gt;&gt;State: Update Workflow State\n    Worker-&gt;&gt;Activities: Execute finalize_setup\n\n    Activities-&gt;&gt;Client: Final Configuration\n    Client-&gt;&gt;NetApp: Multiple API calls for final config\n    NetApp--&gt;&gt;Client: Configuration Complete\n    Activities--&gt;&gt;Worker: Setup Finalized\n\n    Worker-&gt;&gt;State: Update Final Workflow State\n    Worker-&gt;&gt;Activities: Execute send_notification\n\n    Activities-&gt;&gt;EventProc: Send Team Notification\n    EventProc--&gt;&gt;Activities: Notification Sent\n\n    Worker--&gt;&gt;Temporal: Workflow Complete\n    Temporal--&gt;&gt;Router: SVM Environment Ready\n    Router--&gt;&gt;Gateway: Environment Setup Complete\n    Gateway--&gt;&gt;AI: Development environment ready for mobile-dev team\n\n    Note over AI: Complete environment with SVM, networks, volumes, and policies ready</code></pre>"},{"location":"architecture/technical-documentation/#3-event-driven-capacity-management-day-2-ai-integration","title":"3. Event-Driven Capacity Management (Day-2 AI Integration)","text":"<pre><code>sequenceDiagram\n    participant NetApp as CMP-0016: NetApp Infrastructure\n    participant EventProc as CMP-0014: Event Processor\n    participant APIM as API Management\n    participant Temporal as CMP-0009: Temporal Engine\n    participant Worker as CMP-0011: Temporal Worker\n    participant Activities as CMP-0010: Workflow Activities\n    participant Client as CMP-0008: NetApp Client\n    participant Monitor as CMP-0020: Health Monitor\n    participant AI as CMP-0001: AI Assistant (Day-2)\n\n    Note over NetApp,AI: Continuous Capacity Management with Day-2 AI\n\n    loop Every Hour\n        Temporal-&gt;&gt;Worker: Execute capacity_check_activity\n        Worker-&gt;&gt;Activities: Check Cluster Capacity\n        Activities-&gt;&gt;Client: Get Capacity Metrics\n        Client-&gt;&gt;NetApp: GET /datacenter/storage/aggregates\n        NetApp--&gt;&gt;Client: Capacity Data\n        Activities--&gt;&gt;Worker: Capacity Status: 75% Used\n        Worker--&gt;&gt;Temporal: Normal Capacity\n        Temporal-&gt;&gt;Temporal: Sleep 1 Hour\n    end\n\n    Note over NetApp: Capacity reaches 85%\n\n    NetApp-&gt;&gt;EventProc: Capacity Alert Event\n    EventProc-&gt;&gt;APIM: Forward Alert to API Management\n    APIM-&gt;&gt;Temporal: Signal: high_capacity_alert\n\n    Temporal-&gt;&gt;Worker: Trigger Capacity Analysis\n    Worker-&gt;&gt;Activities: Execute detailed_capacity_analysis\n\n    Activities-&gt;&gt;Client: Analyze Capacity Trends\n    Client-&gt;&gt;NetApp: GET /gateways/clusters/{uuid}/metrics/aggregates/perf\n    NetApp--&gt;&gt;Client: Historical Performance Data\n\n    Activities-&gt;&gt;Client: Get Volume Growth Patterns\n    Client-&gt;&gt;NetApp: GET /datacenter/storage/volumes?fields=size,growth\n    NetApp--&gt;&gt;Client: Volume Growth Data\n\n    Activities--&gt;&gt;Worker: Analysis: Need 2TB Additional Capacity\n\n    Worker-&gt;&gt;Activities: Execute generate_expansion_plan\n    Activities-&gt;&gt;Client: Create Expansion Recommendations\n    Client-&gt;&gt;NetApp: GET /datacenter/storage/disks?available=true\n    NetApp--&gt;&gt;Client: Available Disk Information\n\n    Activities--&gt;&gt;Worker: Expansion Plan Ready\n\n    Worker-&gt;&gt;Activities: Execute request_approval\n    Activities-&gt;&gt;APIM: Send Approval Request via API\n    APIM-&gt;&gt;AI: Forward to AI Assistant (Day-2 Operation)\n    Note right of APIM: \"Cluster prod-01 needs 2TB expansion. AI approval?\"\n\n    AI--&gt;&gt;APIM: AI Analysis &amp; Approval: APPROVED\n    APIM--&gt;&gt;Activities: Approval: APPROVED\n\n    Worker-&gt;&gt;Activities: Execute capacity_expansion\n    Activities-&gt;&gt;Client: Add Disks to Aggregate\n    Client-&gt;&gt;NetApp: POST /datacenter/storage/aggregates/{id}/disks\n    NetApp--&gt;&gt;Client: Expansion In Progress\n\n    Activities-&gt;&gt;Monitor: Monitor Expansion Progress\n    Monitor-&gt;&gt;Client: Check Expansion Status\n    Client-&gt;&gt;NetApp: GET /management-server/jobs/{job-id}\n    NetApp--&gt;&gt;Client: Job Status: COMPLETED\n    Monitor--&gt;&gt;Activities: Expansion Complete\n\n    Activities--&gt;&gt;Worker: Capacity Expansion Successful\n    Worker-&gt;&gt;Activities: Execute send_completion_notification\n\n    Activities-&gt;&gt;APIM: Send Completion Notification\n    APIM-&gt;&gt;AI: Forward to AI Assistant (Day-2 Update)\n    Note right of APIM: \"Capacity expansion completed. 2TB added to prod-01\"\n\n    Worker--&gt;&gt;Temporal: Capacity Management Cycle Complete\n\n    Note over Temporal: Resume Normal Monitoring</code></pre>"},{"location":"architecture/technical-documentation/#4-failure-recovery-and-retry-mechanism-devops-primary","title":"4. Failure Recovery and Retry Mechanism (DevOps Primary)","text":"<pre><code>sequenceDiagram\n    participant DevOps as DevOps GUI\n    participant APIM as API Management\n    participant Temporal as CMP-0009: Temporal Engine\n    participant Worker as CMP-0011: Temporal Worker\n    participant Activities as CMP-0010: Workflow Activities\n    participant Client as CMP-0008: NetApp Client\n    participant State as CMP-0012: State Store\n    participant NetApp as CMP-0016: NetApp Infrastructure\n\n    Note over DevOps,NetApp: DevOps SVM Creation with Failure Recovery\n\n    DevOps-&gt;&gt;APIM: Start SVM Creation via GUI\n    APIM-&gt;&gt;Temporal: Start SVM Creation Workflow\n    Temporal-&gt;&gt;State: Persist Initial State\n\n    Temporal-&gt;&gt;Worker: Execute Workflow\n    Worker-&gt;&gt;Activities: validate_cluster_health\n    Activities-&gt;&gt;Client: Check Cluster Health\n    Client-&gt;&gt;NetApp: GET /datacenter/cluster/clusters/{id}\n    NetApp--&gt;&gt;Client: Cluster Healthy\n    Activities--&gt;&gt;Worker: Validation Success\n\n    Worker-&gt;&gt;State: Update: validation_complete\n    Worker-&gt;&gt;Activities: create_svm\n    Activities-&gt;&gt;Client: Create SVM\n    Client-&gt;&gt;NetApp: POST /datacenter/svm/svms\n\n    Note over NetApp: Network Timeout\n    NetApp--&gt;&gt;Client: ERROR: Connection Timeout\n    Client--&gt;&gt;Activities: Network Error\n    Activities--&gt;&gt;Worker: Activity Failed\n\n    Worker-&gt;&gt;State: Update: create_svm_failed (attempt 1)\n    Worker-&gt;&gt;Temporal: Report Activity Failure\n\n    Note over Temporal: Automatic Retry with Exponential Backoff\n    Temporal-&gt;&gt;Temporal: Wait 2 seconds\n\n    Temporal-&gt;&gt;Worker: Retry create_svm (attempt 2)\n    Worker-&gt;&gt;Activities: create_svm (retry)\n    Activities-&gt;&gt;Client: Create SVM (retry)\n    Client-&gt;&gt;NetApp: POST /datacenter/svm/svms\n    NetApp--&gt;&gt;Client: SVM Created Successfully\n    Activities--&gt;&gt;Worker: SVM Creation Success\n\n    Worker-&gt;&gt;State: Update: svm_created\n    Worker-&gt;&gt;Activities: configure_network\n    Activities-&gt;&gt;Client: Configure Network Interfaces\n    Client-&gt;&gt;NetApp: POST /network/ip/interfaces\n\n    Note over Client: Worker Pod Crashes\n    Client--&gt;&gt;Activities: [Connection Lost]\n\n    Note over Temporal: Worker Failure Detected\n    Temporal-&gt;&gt;State: Read Last Known State\n    State--&gt;&gt;Temporal: State: svm_created, network_pending\n\n    Note over Temporal: New Worker Assigned\n    Temporal-&gt;&gt;Worker: Resume from configure_network\n    Worker-&gt;&gt;State: Read Current State\n    State--&gt;&gt;Worker: SVM UUID: svm-789, Status: created\n\n    Worker-&gt;&gt;Activities: configure_network (resume)\n    Activities-&gt;&gt;Client: Configure Network Interfaces\n    Client-&gt;&gt;NetApp: POST /network/ip/interfaces\n    NetApp--&gt;&gt;Client: Network Configured\n    Activities--&gt;&gt;Worker: Network Configuration Success\n\n    Worker-&gt;&gt;State: Update: network_configured\n    Worker-&gt;&gt;Activities: finalize_svm\n    Activities-&gt;&gt;Client: Final SVM Configuration\n    Client-&gt;&gt;NetApp: PATCH /datacenter/svm/svms/{id}\n    NetApp--&gt;&gt;Client: SVM Finalized\n    Activities--&gt;&gt;Worker: Finalization Complete\n\n    Worker-&gt;&gt;State: Update: workflow_complete\n    Worker--&gt;&gt;Temporal: Workflow Success\n    Temporal--&gt;&gt;APIM: SVM Created Successfully\n    APIM--&gt;&gt;DevOps: SVM Creation Complete\n\n    Note over DevOps: SVM creation completed despite network timeout and worker failure</code></pre>"},{"location":"architecture/technical-documentation/#5-human-in-the-loop-approval-workflow-devops-ai-day-2","title":"5. Human-in-the-Loop Approval Workflow (DevOps + AI Day-2)","text":"<pre><code>sequenceDiagram\n    participant DevOps as DevOps GUI\n    participant APIM as API Management\n    participant Temporal as CMP-0009: Temporal Engine\n    participant Worker as CMP-0011: Temporal Worker\n    participant Activities as CMP-0010: Workflow Activities\n    participant Client as CMP-0008: NetApp Client\n    participant State as CMP-0012: State Store\n    participant NetApp as CMP-0016: NetApp Infrastructure\n    participant Approver as External: Human Approver\n    participant AI as CMP-0001: AI Assistant (Day-2)\n    participant Notification as External: Notification System\n\n    Note over DevOps,Notification: DevOps Production SVM Creation with AI-Assisted Approval\n\n    DevOps-&gt;&gt;APIM: Create Production SVM Request\n    Note right of DevOps: Environment: \"production\"\n    APIM-&gt;&gt;Temporal: Create Production SVM Workflow\n\n    Temporal-&gt;&gt;State: Persist Workflow State\n    Temporal-&gt;&gt;Worker: Execute Workflow\n\n    Worker-&gt;&gt;Activities: validate_production_requirements\n    Activities-&gt;&gt;Client: Validate Production Setup\n    Client-&gt;&gt;NetApp: Multiple validation API calls\n    NetApp--&gt;&gt;Client: Validation Results\n    Activities--&gt;&gt;Worker: Requirements Valid\n\n    Worker-&gt;&gt;State: Update: validation_complete\n    Worker-&gt;&gt;Activities: prepare_svm_config\n    Activities-&gt;&gt;Client: Generate Production Configuration\n    Client-&gt;&gt;NetApp: GET cluster and aggregate information\n    NetApp--&gt;&gt;Client: Infrastructure Data\n    Activities--&gt;&gt;Worker: Configuration Prepared\n\n    Worker-&gt;&gt;State: Update: config_prepared\n    Worker-&gt;&gt;Activities: request_production_approval\n\n    Activities-&gt;&gt;APIM: Send Approval Request\n    APIM-&gt;&gt;Notification: Forward Approval Request\n    Note right of APIM: \"Production SVM 'finance-prod' ready for creation. Requires approval.\"\n\n    Notification-&gt;&gt;Approver: Email/Slack Notification\n    Note right of Notification: \"Approve production SVM creation?\"\n\n    par Human Approval Process\n        Activities-&gt;&gt;State: Update: awaiting_approval\n        Worker-&gt;&gt;Temporal: Activity Running (wait for approval)\n\n        Note over Temporal: Workflow waits up to 24 hours for approval\n\n        loop Wait for Approval\n            Temporal-&gt;&gt;State: Check Approval Status\n            State--&gt;&gt;Temporal: Status: awaiting_approval\n            Temporal-&gt;&gt;Temporal: Sleep 30 seconds\n        end\n    and AI-Assisted Analysis (Day-2)\n        APIM-&gt;&gt;AI: Request AI Analysis of SVM Config\n        AI-&gt;&gt;Client: Analyze NetApp Infrastructure\n        Client-&gt;&gt;NetApp: GET cluster capacity and performance\n        NetApp--&gt;&gt;Client: Infrastructure data\n        AI--&gt;&gt;APIM: AI Recommendation: \"Approved - optimal configuration\"\n    end\n\n    Note over Approver: Approver reviews request with AI recommendation (after 2 hours)\n\n    Approver-&gt;&gt;Notification: APPROVE (based on AI analysis)\n    Notification-&gt;&gt;APIM: Approval Signal: APPROVED\n    APIM-&gt;&gt;Activities: Approval Signal: APPROVED\n    Activities--&gt;&gt;Worker: Approval Received\n\n    Worker-&gt;&gt;State: Update: approved\n    Worker-&gt;&gt;Activities: create_production_svm\n\n    Activities-&gt;&gt;Client: Create Production SVM\n    Client-&gt;&gt;NetApp: POST /datacenter/svm/svms\n    Note right of Client: Production-grade configuration\n    NetApp--&gt;&gt;Client: SVM Created (UUID: svm-prod-001)\n\n    Activities--&gt;&gt;Worker: Production SVM Created\n\n    Worker-&gt;&gt;Activities: configure_production_security\n    Activities-&gt;&gt;Client: Apply Security Policies\n    Client-&gt;&gt;NetApp: Configure LDAP, export policies, etc.\n    NetApp--&gt;&gt;Client: Security Configured\n\n    Worker-&gt;&gt;Activities: setup_monitoring\n    Activities-&gt;&gt;Client: Enable Production Monitoring\n    Client-&gt;&gt;NetApp: Configure alerts and monitoring\n    NetApp--&gt;&gt;Client: Monitoring Active\n\n    Worker-&gt;&gt;State: Update: workflow_complete\n    Worker-&gt;&gt;Activities: send_completion_notification\n\n    Activities-&gt;&gt;APIM: Send Success Notification\n    APIM-&gt;&gt;Notification: Forward Success Notification\n    Notification-&gt;&gt;AI: Production SVM Ready (Day-2 Update)\n    Notification-&gt;&gt;Approver: Creation Completed\n\n    Worker--&gt;&gt;Temporal: Workflow Complete\n    Temporal--&gt;&gt;APIM: Production SVM 'finance-prod' created and ready\n    APIM--&gt;&gt;DevOps: SVM Creation Complete\n\n    Note over DevOps: Production environment ready with full approval trail and AI assistance</code></pre>"},{"location":"architecture/technical-documentation/#6-multi-site-data-replication-setup-devops-primary","title":"6. Multi-Site Data Replication Setup (DevOps Primary)","text":"<pre><code>sequenceDiagram\n    participant DevOps as DevOps GUI\n    participant APIM as API Management\n    participant Temporal as CMP-0009: Temporal Engine\n    participant Worker as CMP-0011: Temporal Worker\n    participant Activities as CMP-0010: Workflow Activities\n    participant Client as CMP-0008: NetApp Client\n    participant State as CMP-0012: State Store\n    participant PrimarySite as Primary: Primary NetApp Cluster\n    participant SecondarySite as Secondary: Secondary NetApp Cluster\n    participant Monitor as CMP-0020: Health Monitor\n\n    Note over DevOps,Monitor: DevOps Multi-Site SnapMirror Replication Setup\n\n    DevOps-&gt;&gt;APIM: Setup Disaster Recovery Replication\n    Note right of DevOps: Source: \"prod-vol-001\", Target Site: \"dr-site\"\n    APIM-&gt;&gt;Temporal: Setup Disaster Recovery Replication\n\n    Temporal-&gt;&gt;State: Persist Workflow State\n    Temporal-&gt;&gt;Worker: Execute Multi-Site Replication Workflow\n\n    Worker-&gt;&gt;Activities: validate_source_volume\n    Activities-&gt;&gt;Client: Validate Source Volume\n    Client-&gt;&gt;PrimarySite: GET /datacenter/storage/volumes/prod-vol-001\n    PrimarySite--&gt;&gt;Client: Volume Details\n    Activities--&gt;&gt;Worker: Source Volume Valid\n\n    Worker-&gt;&gt;State: Update: source_validated\n    Worker-&gt;&gt;Activities: validate_destination_cluster\n    Activities-&gt;&gt;Client: Check Destination Cluster\n    Client-&gt;&gt;SecondarySite: GET /datacenter/cluster/clusters\n    SecondarySite--&gt;&gt;Client: Cluster Status: Healthy\n    Activities--&gt;&gt;Worker: Destination Valid\n\n    Worker-&gt;&gt;State: Update: destination_validated\n    Worker-&gt;&gt;Activities: check_network_connectivity\n\n    par Check Inter-Cluster Connectivity\n        Activities-&gt;&gt;Client: Test Primary to Secondary\n        Client-&gt;&gt;PrimarySite: GET /network/ip/interfaces\n        PrimarySite--&gt;&gt;Client: Network Interfaces\n        Client-&gt;&gt;SecondarySite: POST /network/ping-test\n        SecondarySite--&gt;&gt;Client: Connectivity OK\n    and\n        Activities-&gt;&gt;Client: Check Bandwidth\n        Client-&gt;&gt;PrimarySite: GET /gateways/clusters/{id}/metrics/network/perf\n        PrimarySite--&gt;&gt;Client: Network Performance Metrics\n    end\n\n    Activities--&gt;&gt;Worker: Network Connectivity Verified\n\n    Worker-&gt;&gt;State: Update: network_verified\n    Worker-&gt;&gt;Activities: create_destination_volume\n\n    Activities-&gt;&gt;Client: Create DP Volume on Secondary\n    Client-&gt;&gt;SecondarySite: POST /datacenter/storage/volumes\n    Note right of Client: Type: DP (Data Protection)\n    SecondarySite--&gt;&gt;Client: DP Volume Created (UUID: dp-vol-001)\n\n    Activities--&gt;&gt;Worker: Destination Volume Ready\n\n    Worker-&gt;&gt;State: Update: destination_created\n    Worker-&gt;&gt;Activities: establish_snapmirror_relationship\n\n    Activities-&gt;&gt;Client: Create SnapMirror Relationship\n    Client-&gt;&gt;PrimarySite: POST /snapmirror/relationships\n    Note right of Client: Source: prod-vol-001, Destination: dp-vol-001\n    PrimarySite--&gt;&gt;Client: Relationship Created (UUID: sm-rel-001)\n\n    Activities--&gt;&gt;Worker: SnapMirror Relationship Established\n\n    Worker-&gt;&gt;State: Update: relationship_created\n    Worker-&gt;&gt;Activities: initialize_snapmirror\n\n    Activities-&gt;&gt;Client: Start Initial Transfer\n    Client-&gt;&gt;PrimarySite: POST /snapmirror/relationships/sm-rel-001/transfers\n    PrimarySite--&gt;&gt;Client: Transfer Started (Job: job-123)\n\n    Activities-&gt;&gt;Monitor: Monitor Initial Transfer\n\n    loop Monitor Transfer Progress\n        Monitor-&gt;&gt;Client: Check Transfer Status\n        Client-&gt;&gt;PrimarySite: GET /management-server/jobs/job-123\n        PrimarySite--&gt;&gt;Client: Progress: 45% Complete\n        Monitor-&gt;&gt;State: Update Progress\n        Monitor-&gt;&gt;Monitor: Wait 60 seconds\n    end\n\n    PrimarySite--&gt;&gt;Client: Transfer Complete\n    Monitor--&gt;&gt;Activities: Initial Transfer Successful\n\n    Worker-&gt;&gt;State: Update: initialized\n    Worker-&gt;&gt;Activities: configure_replication_schedule\n\n    Activities-&gt;&gt;Client: Set Replication Schedule\n    Client-&gt;&gt;PrimarySite: PATCH /snapmirror/relationships/sm-rel-001\n    Note right of Client: Schedule: Every 4 hours\n    PrimarySite--&gt;&gt;Client: Schedule Configured\n\n    Activities--&gt;&gt;Worker: Schedule Active\n\n    Worker-&gt;&gt;State: Update: schedule_configured\n    Worker-&gt;&gt;Activities: setup_monitoring_alerts\n\n    Activities-&gt;&gt;Client: Configure SnapMirror Alerts\n    Client-&gt;&gt;PrimarySite: POST /management-server/events/rules\n    PrimarySite--&gt;&gt;Client: Alert Rules Created\n\n    Activities-&gt;&gt;Client: Configure Secondary Site Monitoring\n    Client-&gt;&gt;SecondarySite: POST /management-server/events/rules\n    SecondarySite--&gt;&gt;Client: Alert Rules Created\n\n    Worker-&gt;&gt;State: Update: monitoring_configured\n    Worker-&gt;&gt;Activities: test_failover_capability\n\n    Activities-&gt;&gt;Client: Test Break and Resync\n    Client-&gt;&gt;SecondarySite: POST /snapmirror/relationships/sm-rel-001/break\n    SecondarySite--&gt;&gt;Client: Break Successful\n\n    Activities-&gt;&gt;Client: Test Resync\n    Client-&gt;&gt;PrimarySite: POST /snapmirror/relationships/sm-rel-001/resync\n    PrimarySite--&gt;&gt;Client: Resync Initiated\n\n    Activities-&gt;&gt;Monitor: Wait for Resync\n    Monitor-&gt;&gt;Client: Check Resync Status\n    Client-&gt;&gt;PrimarySite: GET /snapmirror/relationships/sm-rel-001\n    PrimarySite--&gt;&gt;Client: Status: Snapmirrored\n    Monitor--&gt;&gt;Activities: Failover Test Complete\n\n    Worker-&gt;&gt;State: Update: tested\n    Worker-&gt;&gt;Activities: generate_dr_documentation\n\n    Activities-&gt;&gt;Client: Generate DR Procedures\n    Client--&gt;&gt;Activities: DR Documentation Created\n\n    Worker-&gt;&gt;State: Update: workflow_complete\n    Worker--&gt;&gt;Temporal: Multi-Site Replication Complete\n\n    Temporal--&gt;&gt;APIM: DR Replication configured successfully\n    APIM--&gt;&gt;DevOps: DR Setup Complete\n    Note right of DevOps: prod-vol-001 \u2192 dr-site, Schedule: 4hrs, Failover tested</code></pre>"},{"location":"architecture/technical-documentation/#component-interaction-patterns","title":"Component Interaction Patterns","text":""},{"location":"architecture/technical-documentation/#1-request-processing-pattern","title":"1. Request Processing Pattern","text":"<pre><code>graph LR\n    A[CMP-0001&lt;br/&gt;AI Assistant&lt;br/&gt;Interface] --&gt;|MCP Request| B[CMP-0002&lt;br/&gt;MCP Protocol&lt;br/&gt;Gateway]\n    B --&gt;|Route| C[CMP-0003&lt;br/&gt;Request&lt;br/&gt;Router]\n    C --&gt;|Validate| D[CMP-0004&lt;br/&gt;Schema&lt;br/&gt;Validator]\n    C --&gt;|Lookup| E[CMP-0005&lt;br/&gt;Tool&lt;br/&gt;Registry]\n    C --&gt;|Authenticate| F[CMP-0007&lt;br/&gt;Authentication&lt;br/&gt;Manager]\n    C --&gt;|Execute| G[CMP-0008&lt;br/&gt;NetApp API&lt;br/&gt;Client]\n    G --&gt;|Cache| H[CMP-0013&lt;br/&gt;Cache&lt;br/&gt;Manager]\n    C --&gt;|Format| I[CMP-0006&lt;br/&gt;Response&lt;br/&gt;Formatter]\n    I --&gt;|Response| B\n    B --&gt;|Result| A\n\n    style A fill:#e1f5fe\n    style B fill:#f3e5f5\n    style C fill:#e8f5e8\n    style G fill:#fff3e0\n    style H fill:#fce4ec</code></pre>"},{"location":"architecture/technical-documentation/#2-workflow-orchestration-pattern","title":"2. Workflow Orchestration Pattern","text":"<pre><code>graph TD\n    A[CMP-0001&lt;br/&gt;AI Assistant&lt;br/&gt;Interface] --&gt;|Workflow Request| B[CMP-0009&lt;br/&gt;Temporal Workflow&lt;br/&gt;Engine]\n    B --&gt;|Assign| C[CMP-0011&lt;br/&gt;Temporal&lt;br/&gt;Workers]\n    C --&gt;|Execute| D[CMP-0010&lt;br/&gt;Workflow&lt;br/&gt;Activities]\n    D --&gt;|NetApp Call| E[CMP-0008&lt;br/&gt;NetApp API&lt;br/&gt;Client]\n    E --&gt;|API Request| F[CMP-0016&lt;br/&gt;NetApp&lt;br/&gt;Infrastructure]\n    C --&gt;|Persist State| G[CMP-0012&lt;br/&gt;Workflow State&lt;br/&gt;Store]\n    B --&gt;|Monitor| H[CMP-0018&lt;br/&gt;Monitoring&lt;br/&gt;Stack]\n    F --&gt;|Events| I[CMP-0014&lt;br/&gt;Event&lt;br/&gt;Processor]\n    I --&gt;|Trigger| B\n\n    style A fill:#e1f5fe\n    style B fill:#e8f5e8\n    style C fill:#e8f5e8\n    style D fill:#e8f5e8\n    style E fill:#fff3e0\n    style F fill:#ffebee\n    style G fill:#f1f8e9\n    style H fill:#fce4ec\n    style I fill:#fff8e1</code></pre>"},{"location":"architecture/technical-documentation/#3-event-driven-pattern","title":"3. Event-Driven Pattern","text":"<pre><code>graph LR\n    A[CMP-0016&lt;br/&gt;NetApp&lt;br/&gt;Infrastructure] --&gt;|Events| B[CMP-0014&lt;br/&gt;Event&lt;br/&gt;Processor]\n    B --&gt;|Process &amp; Filter| C[CMP-0009&lt;br/&gt;Temporal Workflow&lt;br/&gt;Engine]\n    C --&gt;|Orchestrate| D[CMP-0011&lt;br/&gt;Temporal&lt;br/&gt;Workers]\n    D --&gt;|Execute Activities| E[CMP-0010&lt;br/&gt;Workflow&lt;br/&gt;Activities]\n    E --&gt;|NetApp Actions| F[CMP-0008&lt;br/&gt;NetApp API&lt;br/&gt;Client]\n    F --&gt;|API Calls| A\n\n    style A fill:#ffebee\n    style B fill:#fff8e1\n    style C fill:#e8f5e8\n    style D fill:#e8f5e8\n    style E fill:#e8f5e8\n    style F fill:#fff3e0</code></pre>"},{"location":"architecture/technical-documentation/#4-cache-enabled-request-pattern","title":"4. Cache-Enabled Request Pattern","text":"<pre><code>graph TD\n    A[CMP-0001&lt;br/&gt;AI Assistant&lt;br/&gt;Interface] --&gt;|Query Request| B[CMP-0002&lt;br/&gt;MCP Protocol&lt;br/&gt;Gateway]\n    B --&gt; C[CMP-0003&lt;br/&gt;Request&lt;br/&gt;Router]\n    C --&gt; D[CMP-0013&lt;br/&gt;Cache&lt;br/&gt;Manager]\n\n    D --&gt;|Cache Hit| E[Cached Response]\n    D --&gt;|Cache Miss| F[CMP-0008&lt;br/&gt;NetApp API&lt;br/&gt;Client]\n\n    F --&gt; G[CMP-0016&lt;br/&gt;NetApp&lt;br/&gt;Infrastructure]\n    G --&gt; H[Fresh Data]\n    H --&gt; I[Update Cache]\n    I --&gt; J[Return Response]\n\n    E --&gt; K[Format Response]\n    J --&gt; K\n    K --&gt; B\n    B --&gt; A\n\n    style A fill:#e1f5fe\n    style D fill:#fce4ec\n    style F fill:#fff3e0\n    style G fill:#ffebee</code></pre>"},{"location":"architecture/technical-documentation/#5-security-context-pattern","title":"5. Security Context Pattern","text":"<pre><code>graph TD\n    A[Incoming Request] --&gt; B[CMP-0007&lt;br/&gt;Authentication&lt;br/&gt;Manager]\n    B --&gt; C[CMP-0015&lt;br/&gt;Security&lt;br/&gt;Context]\n    C --&gt; D{Credentials&lt;br/&gt;Valid?}\n\n    D --&gt;|Yes| E[CMP-0008&lt;br/&gt;NetApp API&lt;br/&gt;Client]\n    D --&gt;|No| F[Access Denied]\n\n    E --&gt; G[Encrypted Channel]\n    G --&gt; H[CMP-0016&lt;br/&gt;NetApp&lt;br/&gt;Infrastructure]\n\n    C --&gt; I[CMP-0019&lt;br/&gt;Configuration&lt;br/&gt;Manager]\n    I --&gt; J[Environment&lt;br/&gt;Configs]\n\n    style B fill:#ffcdd2\n    style C fill:#ffcdd2\n    style E fill:#fff3e0\n    style G fill:#e8f5e8\n    style H fill:#ffebee\n    style I fill:#f3e5f5</code></pre>"},{"location":"architecture/technical-documentation/#6-health-monitoring-pattern","title":"6. Health Monitoring Pattern","text":"<pre><code>graph LR\n    A[CMP-0020&lt;br/&gt;Health&lt;br/&gt;Monitor] --&gt;|Check| B[CMP-0008&lt;br/&gt;NetApp API&lt;br/&gt;Client]\n    A --&gt;|Check| C[CMP-0009&lt;br/&gt;Temporal Workflow&lt;br/&gt;Engine]\n    A --&gt;|Check| D[CMP-0013&lt;br/&gt;Cache&lt;br/&gt;Manager]\n    A --&gt;|Check| E[CMP-0012&lt;br/&gt;Workflow State&lt;br/&gt;Store]\n\n    B --&gt;|Status| F[Health Report]\n    C --&gt;|Status| F\n    D --&gt;|Status| F\n    E --&gt;|Status| F\n\n    F --&gt; G[CMP-0018&lt;br/&gt;Monitoring&lt;br/&gt;Stack]\n    G --&gt; H[Alerts &amp;&lt;br/&gt;Dashboards]\n\n    style A fill:#e3f2fd\n    style F fill:#e3f2fd\n    style G fill:#fce4ec\n    style H fill:#fce4ec</code></pre>"},{"location":"architecture/technical-documentation/#technical-specifications","title":"Technical Specifications","text":""},{"location":"architecture/technical-documentation/#component-specifications","title":"Component Specifications","text":""},{"location":"architecture/technical-documentation/#cmp-0009-temporal-workflow-engine","title":"CMP-0009: Temporal Workflow Engine","text":"<p>Configuration: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: temporal-server\n  labels:\n    component: CMP-0009\nspec:\n  replicas: 3\n  template:\n    spec:\n      containers:\n      - name: temporal-server\n        image: temporalio/auto-setup:1.20.0\n        ports:\n        - containerPort: 7233\n          name: grpc\n        - containerPort: 8080\n          name: web\n        env:\n        - name: DB\n          value: postgresql\n        - name: POSTGRES_SEEDS\n          value: postgres-cluster:5432\n        - name: DYNAMIC_CONFIG_FILE_PATH\n          value: /etc/temporal/config/dynamicconfig.yaml\n        resources:\n          requests:\n            memory: 1Gi\n            cpu: 500m\n          limits:\n            memory: 2Gi\n            cpu: 1000m\n</code></pre></p> <p>Interfaces: - gRPC API: Port 7233 for workflow and activity communication - Web UI: Port 8080 for workflow monitoring and debugging - Database: PostgreSQL for state persistence</p>"},{"location":"architecture/technical-documentation/#cmp-0008-netapp-api-client","title":"CMP-0008: NetApp API Client","text":"<p>Configuration: <pre><code>class NetAppAPIClient:\n    def __init__(self, config: NetAppConfig):\n        self.base_url = f\"https://{config.host}/api/v2\"\n        self.session = aiohttp.ClientSession(\n            auth=aiohttp.BasicAuth(config.username, config.password),\n            connector=aiohttp.TCPConnector(\n                ssl=False if not config.verify_ssl else None,\n                limit=20,\n                limit_per_host=10,\n                keepalive_timeout=30\n            ),\n            timeout=aiohttp.ClientTimeout(total=config.timeout)\n        )\n\n    async def make_request(self, method: str, endpoint: str, **kwargs):\n        url = f\"{self.base_url}/{endpoint.lstrip('/')}\"\n        async with self.session.request(method, url, **kwargs) as response:\n            response.raise_for_status()\n            return await response.json()\n</code></pre></p> <p>API Endpoints Supported: - Administration: <code>/admin/*</code> - Datacenter: <code>/datacenter/*</code> - Storage Provider: <code>/storage-provider/*</code> - Management Server: <code>/management-server/*</code> - Gateways: <code>/gateways/*</code></p>"},{"location":"architecture/technical-documentation/#performance-specifications","title":"Performance Specifications","text":"Component Throughput Latency Scalability Resource Requirements CMP-0002 1000 req/sec &lt;10ms Horizontal (Knative) 256Mi RAM, 200m CPU CMP-0008 500 req/sec &lt;100ms Connection pooling 512Mi RAM, 500m CPU CMP-0009 10000 workflows/sec &lt;50ms Cluster-wide 2Gi RAM, 1 CPU CMP-0011 1000 activities/sec Variable Worker scaling 1Gi RAM, 500m CPU CMP-0013 5000 ops/sec &lt;5ms Redis cluster 1Gi RAM, 200m CPU"},{"location":"architecture/technical-documentation/#security-specifications","title":"Security Specifications","text":""},{"location":"architecture/technical-documentation/#authentication-and-authorization","title":"Authentication and Authorization","text":"<pre><code># CMP-0015: Security Context Configuration\napiVersion: v1\nkind: Secret\nmetadata:\n  name: netapp-credentials\n  namespace: netapp-mcp\ntype: Opaque\ndata:\n  endpoint: &lt;base64-encoded-url&gt;\n  username: &lt;base64-encoded-username&gt;\n  password: &lt;base64-encoded-password&gt;\n  ca-cert: &lt;base64-encoded-ca-certificate&gt;\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: netapp-mcp-service\n  namespace: netapp-mcp\n  annotations:\n    component: CMP-0007\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: netapp-mcp\n  name: netapp-mcp-role\nrules:\n- apiGroups: [\"\"]\n  resources: [\"secrets\", \"configmaps\"]\n  verbs: [\"get\", \"list\"]\n- apiGroups: [\"serving.knative.dev\"]\n  resources: [\"services\"]\n  verbs: [\"get\", \"list\", \"patch\"]\n</code></pre>"},{"location":"architecture/technical-documentation/#network-security","title":"Network Security","text":"<pre><code># Network Policy for Component Isolation\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: netapp-mcp-network-policy\n  namespace: netapp-mcp\nspec:\n  podSelector:\n    matchLabels:\n      app.kubernetes.io/part-of: netapp-mcp\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          name: knative-serving\n    ports:\n    - protocol: TCP\n      port: 8080\n  egress:\n  - to:\n    - namespaceSelector: {}\n    ports:\n    - protocol: TCP\n      port: 443  # NetApp HTTPS\n    - protocol: TCP\n      port: 7233 # Temporal gRPC\n    - protocol: TCP\n      port: 5432 # PostgreSQL\n    - protocol: TCP\n      port: 6379 # Redis\n</code></pre>"},{"location":"architecture/technical-documentation/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"architecture/technical-documentation/#metrics-collection","title":"Metrics Collection","text":"<pre><code># CMP-0018: Monitoring Stack Configuration\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: netapp-mcp-components\n  namespace: netapp-mcp\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/part-of: netapp-mcp\n  endpoints:\n  - port: metrics\n    path: /metrics\n    interval: 30s\n    scrapeTimeout: 10s\n</code></pre>"},{"location":"architecture/technical-documentation/#key-metrics","title":"Key Metrics","text":"Component Metric Name Type Purpose CMP-0002 <code>mcp_requests_total</code> Counter Track request volume CMP-0008 <code>netapp_api_calls_total</code> Counter Monitor API usage CMP-0009 <code>temporal_workflow_executions_total</code> Counter Workflow tracking CMP-0011 <code>temporal_activity_executions_total</code> Counter Activity monitoring CMP-0013 <code>cache_operations_total</code> Counter Cache performance CMP-0020 <code>health_check_status</code> Gauge System health"},{"location":"architecture/technical-documentation/#distributed-tracing","title":"Distributed Tracing","text":"<pre><code># OpenTelemetry Integration\nfrom opentelemetry import trace\nfrom opentelemetry.exporter.jaeger.thrift import JaegerExporter\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\n\n# Configure tracing for all components\ntracer_provider = TracerProvider()\njaeger_exporter = JaegerExporter(\n    agent_host_name=\"jaeger-agent\",\n    agent_port=6831\n)\nspan_processor = BatchSpanProcessor(jaeger_exporter)\ntracer_provider.add_span_processor(span_processor)\ntrace.set_tracer_provider(tracer_provider)\n\n# Component-specific tracers\ncmp_0002_tracer = trace.get_tracer(\"CMP-0002-MCP-Gateway\")\ncmp_0008_tracer = trace.get_tracer(\"CMP-0008-NetApp-Client\")\ncmp_0009_tracer = trace.get_tracer(\"CMP-0009-Temporal-Engine\")\n</code></pre> <p>This comprehensive technical documentation provides detailed component models, sequence diagrams, and specifications for implementing and operating the NetApp ActiveIQ MCP Server with Temporal.io integration.</p>"},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/","title":"NetApp ActiveIQ MCP Server - ArgoCD &amp; Helm Deployment Guide","text":"<p>This comprehensive guide covers deploying the NetApp ActiveIQ Unified Manager MCP Server using ArgoCD for GitOps and Helm for package management on Knative.</p>"},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Architecture Overview</li> <li>Prerequisites</li> <li>Helm Chart Structure</li> <li>ArgoCD Configuration</li> <li>Environment Setup</li> <li>Deployment Process</li> <li>GitOps Workflow</li> <li>Monitoring and Observability</li> <li>Security Configuration</li> <li>Troubleshooting</li> <li>Best Practices</li> </ol>"},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#architecture-overview","title":"Architecture Overview","text":""},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#gitops-architecture-with-argocd","title":"GitOps Architecture with ArgoCD","text":"<pre><code>flowchart TD\n    subgraph \"Git Repository\"\n        subgraph \"Helm Chart\"\n            A1[Chart.yaml]\n            A2[values.yaml]\n            A3[values-production.yaml]\n            A4[templates/]\n            A5[knative-service.yaml]\n            A6[rbac.yaml]\n            A7[configmap.yaml]\n            A8[extras.yaml]\n        end\n    end\n\n    subgraph \"ArgoCD Server\"\n        subgraph \"Application Controller\"\n            B1[Monitors Git Repository]\n            B2[Renders Helm Templates]\n            B3[Applies to Kubernetes]\n            B4[Manages Sync Policies]\n        end\n    end\n\n    subgraph \"Kubernetes Cluster\"\n        subgraph \"Knative Service\"\n            subgraph \"NetApp MCP Server Pod\"\n                C1[FastMCP Framework]\n                C2[NetApp API Client]\n                C3[Auto-scaling Enabled]\n            end\n        end\n    end\n\n    A4 --\"Git Pull\"--&gt; B1\n    B1 --&gt; B2\n    B2 --&gt; B3\n    B3 --\"Kubernetes API\"--&gt; C1\n    B4 --&gt; B3</code></pre>"},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#key-benefits","title":"Key Benefits","text":"<ul> <li>GitOps Workflow: Declarative, versioned infrastructure</li> <li>Automated Deployment: Self-healing and drift detection</li> <li>Multi-Environment: Consistent deployments across environments</li> <li>Helm Packaging: Reusable, configurable templates</li> <li>Knative Serverless: Auto-scaling and resource efficiency</li> </ul>"},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#prerequisites","title":"Prerequisites","text":""},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#infrastructure-requirements","title":"Infrastructure Requirements","text":"<pre><code># Required components\nkubectl &gt;= 1.24\nhelm &gt;= 3.8\nargocd &gt;= 2.5\n\n# Kubernetes cluster with:\n# - Knative Serving &gt;= 1.8\n# - ArgoCD &gt;= 2.5\n# - Prometheus Operator (optional)\n# - External Secrets Operator (optional)\n</code></pre>"},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#repository-structure","title":"Repository Structure","text":"<pre><code>\u251c\u2500\u2500 helm/\n\u2502   \u2514\u2500\u2500 netapp-mcp/\n\u2502       \u251c\u2500\u2500 Chart.yaml\n\u2502       \u251c\u2500\u2500 values.yaml\n\u2502       \u251c\u2500\u2500 values-production.yaml\n\u2502       \u251c\u2500\u2500 values-staging.yaml\n\u2502       \u2514\u2500\u2500 templates/\n\u2502           \u251c\u2500\u2500 _helpers.tpl\n\u2502           \u251c\u2500\u2500 knative-service.yaml\n\u2502           \u251c\u2500\u2500 rbac.yaml\n\u2502           \u251c\u2500\u2500 configmap.yaml\n\u2502           \u2514\u2500\u2500 extras.yaml\n\u251c\u2500\u2500 argocd/\n\u2502   \u251c\u2500\u2500 application.yaml\n\u2502   \u251c\u2500\u2500 application-staging.yaml\n\u2502   \u2514\u2500\u2500 application-dev.yaml\n\u2514\u2500\u2500 docs/\n    \u2514\u2500\u2500 ARGOCD_HELM_DEPLOYMENT.md\n</code></pre>"},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#helm-chart-structure","title":"Helm Chart Structure","text":""},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#chart-configuration","title":"Chart Configuration","text":"<p>The Helm chart provides comprehensive configuration options:</p>"},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#core-components","title":"Core Components","text":"<ul> <li>Knative Service: Serverless deployment with auto-scaling</li> <li>RBAC: ServiceAccount, Role, and RoleBinding</li> <li>ConfigMap: Application configuration and feature flags</li> <li>Secret: NetApp credentials (optional, supports external secrets)</li> <li>NetworkPolicy: Security and traffic restrictions</li> <li>ServiceMonitor: Prometheus monitoring integration</li> </ul>"},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#key-features","title":"Key Features","text":"<ul> <li>Multi-environment support: Dev, staging, production values</li> <li>External secrets integration: Support for external-secrets operator</li> <li>Comprehensive validation: Input validation and error handling</li> <li>Production-ready defaults: Security, performance, and reliability</li> <li>Extensible configuration: Support for custom volumes, environment variables</li> </ul>"},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#values-hierarchy","title":"Values Hierarchy","text":"<pre><code># Base values.yaml (development defaults)\nimage:\n  registry: docker.io\n  tag: \"latest\"\n\nknative:\n  template:\n    annotations:\n      autoscaling.knative.dev/minScale: \"1\"\n      autoscaling.knative.dev/maxScale: \"10\"\n\n# Production overrides (values-production.yaml)\nimage:\n  registry: registry.company.com\n  tag: \"1.0.0\"\n\nknative:\n  template:\n    annotations:\n      autoscaling.knative.dev/minScale: \"2\"\n      autoscaling.knative.dev/maxScale: \"20\"\n</code></pre>"},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#argocd-configuration","title":"ArgoCD Configuration","text":""},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#application-manifest","title":"Application Manifest","text":"<p>The ArgoCD Application provides:</p> <ul> <li>GitOps Integration: Automatic sync from Git repository</li> <li>Helm Rendering: Template processing with environment-specific values</li> <li>Sync Policies: Automated deployment with self-healing</li> <li>Multi-environment: Support for dev, staging, and production</li> <li>RBAC Integration: Role-based access control</li> </ul>"},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#project-configuration","title":"Project Configuration","text":"<p>The ArgoCD AppProject includes:</p> <ul> <li>Source Repositories: Allowed Git repositories</li> <li>Destination Clusters: Target Kubernetes clusters and namespaces</li> <li>Resource Whitelist: Allowed Kubernetes resources</li> <li>RBAC Policies: Role-based access for teams</li> <li>Sync Windows: Deployment time restrictions</li> </ul>"},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#environment-setup","title":"Environment Setup","text":""},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#1-create-git-repository","title":"1. Create Git Repository","text":"<pre><code># Initialize repository structure\nmkdir netapp-mcp-server\ncd netapp-mcp-server\n\n# Create directory structure\nmkdir -p helm/netapp-mcp/templates\nmkdir -p argocd\nmkdir -p docs\n\n# Copy Helm chart files\ncp -r helm/ netapp-mcp-server/helm/\ncp -r argocd/ netapp-mcp-server/argocd/\n\n# Initialize Git repository\ngit init\ngit add .\ngit commit -m \"Initial commit: NetApp MCP Server Helm chart and ArgoCD configuration\"\ngit remote add origin https://github.com/your-org/netapp-mcp-server\ngit push -u origin main\n</code></pre>"},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#2-install-argocd","title":"2. Install ArgoCD","text":"<pre><code># Create ArgoCD namespace\nkubectl create namespace argocd\n\n# Install ArgoCD\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml\n\n# Wait for ArgoCD to be ready\nkubectl wait --for=condition=available --timeout=300s deployment/argocd-server -n argocd\n\n# Get ArgoCD admin password\nkubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d\n\n# Port forward to access ArgoCD UI\nkubectl port-forward svc/argocd-server -n argocd 8080:443\n</code></pre>"},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#3-configure-argocd-cli","title":"3. Configure ArgoCD CLI","text":"<pre><code># Install ArgoCD CLI\nbrew install argocd  # macOS\n# or\ncurl -sSL -o /usr/local/bin/argocd https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64\n\n# Login to ArgoCD\nargocd login localhost:8080 --username admin --password &lt;password&gt;\n\n# Create project\nargocd proj create netapp-integration \\\n  --description \"NetApp Integration Platform Project\" \\\n  --src \"https://github.com/your-org/netapp-mcp-server\" \\\n  --dest \"https://kubernetes.default.svc,netapp-mcp\"\n</code></pre>"},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#deployment-process","title":"Deployment Process","text":""},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#1-validate-helm-chart","title":"1. Validate Helm Chart","text":"<pre><code># Navigate to chart directory\ncd helm/netapp-mcp\n\n# Validate chart syntax\nhelm lint .\n\n# Test template rendering\nhelm template netapp-mcp-server . \\\n  --values values.yaml \\\n  --values values-production.yaml\n\n# Check for Kubernetes resource validation\nhelm template netapp-mcp-server . \\\n  --values values.yaml \\\n  --values values-production.yaml | kubectl apply --dry-run=client -f -\n</code></pre>"},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#2-create-environment-specific-values","title":"2. Create Environment-Specific Values","text":"<pre><code># Create staging values\ncat &gt; values-staging.yaml &lt;&lt; EOF\nimage:\n  tag: \"staging\"\n\nknative:\n  template:\n    annotations:\n      autoscaling.knative.dev/minScale: \"1\"\n      autoscaling.knative.dev/maxScale: \"5\"\n\napp:\n  logLevel: \"DEBUG\"\n  extraEnv:\n    ENVIRONMENT: \"staging\"\n\nmonitoring:\n  serviceMonitor:\n    enabled: false\nEOF\n\n# Create development values\ncat &gt; values-dev.yaml &lt;&lt; EOF\nimage:\n  tag: \"dev\"\n\nknative:\n  template:\n    annotations:\n      autoscaling.knative.dev/minScale: \"0\"\n      autoscaling.knative.dev/maxScale: \"3\"\n\napp:\n  logLevel: \"DEBUG\"\n  extraEnv:\n    ENVIRONMENT: \"development\"\n\nnetworkPolicy:\n  enabled: false\n\ntests:\n  enabled: false\nEOF\n</code></pre>"},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#3-deploy-with-argocd","title":"3. Deploy with ArgoCD","text":"<pre><code># Create ArgoCD application\nkubectl apply -f argocd/application.yaml\n\n# Check application status\nargocd app get netapp-mcp-server\n\n# Sync application\nargocd app sync netapp-mcp-server\n\n# Monitor deployment\nargocd app wait netapp-mcp-server --health\n</code></pre>"},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#4-verify-deployment","title":"4. Verify Deployment","text":"<pre><code># Check Knative service\nkubectl get ksvc netapp-mcp-server -n netapp-mcp\n\n# Check pods\nkubectl get pods -n netapp-mcp\n\n# Check logs\nkubectl logs -l app.kubernetes.io/name=netapp-mcp-server -n netapp-mcp\n\n# Run Helm tests\nhelm test netapp-mcp-server -n netapp-mcp\n</code></pre>"},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#gitops-workflow","title":"GitOps Workflow","text":""},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#1-development-workflow","title":"1. Development Workflow","text":"<pre><code># 1. Create feature branch\ngit checkout -b feature/update-scaling\n\n# 2. Update Helm values\nvim helm/netapp-mcp/values-production.yaml\n\n# 3. Test changes locally\nhelm template netapp-mcp-server helm/netapp-mcp \\\n  --values helm/netapp-mcp/values.yaml \\\n  --values helm/netapp-mcp/values-production.yaml\n\n# 4. Commit changes\ngit add .\ngit commit -m \"feat: increase production scaling limits\"\n\n# 5. Create pull request\ngit push origin feature/update-scaling\n</code></pre>"},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#2-promotion-workflow","title":"2. Promotion Workflow","text":"<pre><code># Development \u2192 Staging\nargocd app create netapp-mcp-server-staging \\\n  --repo https://github.com/your-org/netapp-mcp-server \\\n  --path helm/netapp-mcp \\\n  --dest-server https://kubernetes.default.svc \\\n  --dest-namespace netapp-mcp-staging \\\n  --values-literal-file values.yaml \\\n  --values-literal-file values-staging.yaml\n\n# Staging \u2192 Production (after approval)\nargocd app sync netapp-mcp-server --prune\n</code></pre>"},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#3-rollback-procedure","title":"3. Rollback Procedure","text":"<pre><code># View application history\nargocd app history netapp-mcp-server\n\n# Rollback to previous version\nargocd app rollback netapp-mcp-server &lt;revision-id&gt;\n\n# Or rollback via Git\ngit revert &lt;commit-hash&gt;\ngit push origin main\n</code></pre>"},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#1-argocd-monitoring","title":"1. ArgoCD Monitoring","text":"<pre><code># Check application health\nargocd app get netapp-mcp-server --output wide\n\n# View sync status\nargocd app sync-status netapp-mcp-server\n\n# Check resource status\nkubectl get application netapp-mcp-server -n argocd -o yaml\n</code></pre>"},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#2-application-monitoring","title":"2. Application Monitoring","text":"<pre><code># Prometheus metrics (if ServiceMonitor enabled)\nkubectl get servicemonitor netapp-mcp-server-servicemonitor -n netapp-mcp\n\n# Check Knative service metrics\nkubectl get ksvc netapp-mcp-server -n netapp-mcp -o yaml\n\n# Application logs\nkubectl logs -l app.kubernetes.io/name=netapp-mcp-server -n netapp-mcp --tail=100\n</code></pre>"},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#3-alerting-configuration","title":"3. Alerting Configuration","text":"<pre><code># PrometheusRule for alerting\napiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  name: netapp-mcp-server-alerts\nspec:\n  groups:\n    - name: netapp-mcp-server\n      rules:\n        - alert: NetAppMCPServerDown\n          expr: up{job=\"netapp-mcp-server\"} == 0\n          for: 5m\n          annotations:\n            summary: \"NetApp MCP Server is down\"\n\n        - alert: ArgocdSyncFailed\n          expr: argocd_app_health_status{health_status!=\"Healthy\"} == 1\n          for: 10m\n          annotations:\n            summary: \"ArgoCD sync failed for {% raw %}{{ $labels.name }}{% endraw %}\"\n</code></pre>"},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#security-configuration","title":"Security Configuration","text":""},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#1-external-secrets-integration","title":"1. External Secrets Integration","text":"<pre><code># ExternalSecret for production credentials\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: netapp-activeiq-prod-credentials\n  namespace: netapp-mcp\nspec:\n  refreshInterval: 1h\n  secretStoreRef:\n    name: vault-backend\n    kind: SecretStore\n  target:\n    name: netapp-activeiq-prod-credentials\n    creationPolicy: Owner\n  data:\n    - secretKey: NETAPP_BASE_URL\n      remoteRef:\n        key: netapp/production/activeiq\n        property: base_url\n    - secretKey: NETAPP_USERNAME\n      remoteRef:\n        key: netapp/production/activeiq\n        property: username\n    - secretKey: NETAPP_PASSWORD\n      remoteRef:\n        key: netapp/production/activeiq\n        property: password\n</code></pre>"},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#2-rbac-configuration","title":"2. RBAC Configuration","text":"<pre><code># ArgoCD RBAC policy\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: argocd-rbac-cm\n  namespace: argocd\ndata:\n  policy.csv: |\n    # NetApp team permissions\n    p, role:netapp-admin, applications, *, netapp-integration/*, allow\n    p, role:netapp-admin, repositories, *, *, allow\n    p, role:netapp-developer, applications, get, netapp-integration/*, allow\n    p, role:netapp-developer, applications, sync, netapp-integration/*, allow\n\n    # Group assignments\n    g, netapp-admins, role:netapp-admin\n    g, netapp-developers, role:netapp-developer\n</code></pre>"},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#3-network-policies","title":"3. Network Policies","text":"<pre><code># Network policy for ArgoCD access\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: argocd-netapp-mcp-access\n  namespace: netapp-mcp\nspec:\n  podSelector:\n    matchLabels:\n      app.kubernetes.io/name: netapp-mcp-server\n  policyTypes:\n    - Ingress\n  ingress:\n    - from:\n        - namespaceSelector:\n            matchLabels:\n              name: argocd\n      ports:\n        - protocol: TCP\n          port: 8080\n</code></pre>"},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#troubleshooting","title":"Troubleshooting","text":""},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#1-argocd-issues","title":"1. ArgoCD Issues","text":"<pre><code># Check ArgoCD application status\nargocd app get netapp-mcp-server --hard-refresh\n\n# View application events\nkubectl describe application netapp-mcp-server -n argocd\n\n# Check ArgoCD server logs\nkubectl logs deployment/argocd-server -n argocd\n\n# Refresh application\nargocd app get netapp-mcp-server --refresh\n</code></pre>"},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#2-helm-issues","title":"2. Helm Issues","text":"<pre><code># Check Helm release status\nhelm status netapp-mcp-server -n netapp-mcp\n\n# View Helm release history\nhelm history netapp-mcp-server -n netapp-mcp\n\n# Debug template rendering\nhelm template netapp-mcp-server helm/netapp-mcp \\\n  --values values.yaml \\\n  --values values-production.yaml \\\n  --debug\n\n# Validate against Kubernetes API\nhelm template netapp-mcp-server helm/netapp-mcp \\\n  --values values.yaml \\\n  --values values-production.yaml | \\\n  kubectl apply --dry-run=server -f -\n</code></pre>"},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#3-knative-issues","title":"3. Knative Issues","text":"<pre><code># Check Knative service status\nkubectl describe ksvc netapp-mcp-server -n netapp-mcp\n\n# Check Knative revisions\nkubectl get revisions -n netapp-mcp\n\n# Check Knative configuration\nkubectl get configuration netapp-mcp-server -n netapp-mcp -o yaml\n\n# Check Knative routes\nkubectl get routes -n netapp-mcp\n</code></pre>"},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#4-common-problems-and-solutions","title":"4. Common Problems and Solutions","text":"Problem Cause Solution Sync Failed Invalid Helm values Validate Helm chart with <code>helm lint</code> Pod CrashLoop Missing secrets Check secret configuration and external-secrets Service Not Ready Resource limits Increase memory/CPU limits Network Issues NetworkPolicy blocking Review and adjust NetworkPolicy rules Image Pull Errors Registry access Verify image registry and pull secrets"},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#best-practices","title":"Best Practices","text":""},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#1-repository-organization","title":"1. Repository Organization","text":"<pre><code># Recommended structure\n\u251c\u2500\u2500 apps/\n\u2502   \u251c\u2500\u2500 production/\n\u2502   \u2502   \u2514\u2500\u2500 netapp-mcp-server.yaml\n\u2502   \u251c\u2500\u2500 staging/\n\u2502   \u2502   \u2514\u2500\u2500 netapp-mcp-server.yaml\n\u2502   \u2514\u2500\u2500 development/\n\u2502       \u2514\u2500\u2500 netapp-mcp-server.yaml\n\u251c\u2500\u2500 charts/\n\u2502   \u2514\u2500\u2500 netapp-mcp/\n\u251c\u2500\u2500 environments/\n\u2502   \u251c\u2500\u2500 production/\n\u2502   \u251c\u2500\u2500 staging/\n\u2502   \u2514\u2500\u2500 development/\n\u2514\u2500\u2500 shared/\n    \u251c\u2500\u2500 secrets/\n    \u2514\u2500\u2500 policies/\n</code></pre>"},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#2-version-management","title":"2. Version Management","text":"<pre><code># Use semantic versioning for Helm charts\n# Chart.yaml\nversion: 1.2.3\nappVersion: \"1.2.3\"\n\n# Tag releases in Git\ngit tag -a v1.2.3 -m \"Release version 1.2.3\"\ngit push origin v1.2.3\n\n# Use specific image tags in production\nimage:\n  tag: \"v1.2.3\"  # Not \"latest\"\n</code></pre>"},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#3-environment-promotion","title":"3. Environment Promotion","text":"<pre><code># 1. Always test in development first\n# 2. Promote to staging for integration testing\n# 3. Deploy to production with approval\n\n# Use different branches for different environments\ngit checkout -b staging/v1.2.3\ngit checkout -b production/v1.2.3\n</code></pre>"},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#4-security-practices","title":"4. Security Practices","text":"<ul> <li>Never commit secrets to Git repository</li> <li>Use external secret management (Vault, AWS Secrets Manager)</li> <li>Implement RBAC for ArgoCD access</li> <li>Enable NetworkPolicies in production</li> <li>Use signed commits for critical changes</li> <li>Regular security scanning of images and charts</li> </ul>"},{"location":"deployment/ARGOCD_HELM_DEPLOYMENT/#5-monitoring-and-alerting","title":"5. Monitoring and Alerting","text":"<ul> <li>Monitor ArgoCD sync status with Prometheus</li> <li>Set up alerts for failed deployments</li> <li>Track deployment metrics and trends</li> <li>Implement health checks for applications</li> <li>Use distributed tracing for complex workflows</li> </ul> <p>This comprehensive guide provides everything needed to successfully deploy and operate the NetApp ActiveIQ MCP Server using ArgoCD and Helm in a production GitOps environment.</p>"},{"location":"deployment/KNATIVE_DEPLOYMENT/","title":"NetApp ActiveIQ MCP Server - Knative Function Deployment","text":"<p>Deploy NetApp ActiveIQ MCP Server as serverless Knative functions to enable scalable, on-demand storage operations with automatic scaling and cost optimization.</p>"},{"location":"deployment/KNATIVE_DEPLOYMENT/#target-operating-model","title":"Target Operating Model","text":"<p>This deployment enables a function-based architecture where NetApp storage operations are delivered as serverless functions that scale automatically based on demand, transforming traditional expert-dependent storage management into democratized, AI-accessible operations.</p>"},{"location":"deployment/KNATIVE_DEPLOYMENT/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Prerequisites</li> <li>Architecture Overview</li> <li>Container Preparation</li> <li>Kubernetes Resources</li> <li>Deployment Process</li> <li>Configuration Management</li> <li>Monitoring and Observability</li> <li>Security Considerations</li> <li>Scaling and Performance</li> <li>Troubleshooting</li> <li>Maintenance and Updates</li> </ol>"},{"location":"deployment/KNATIVE_DEPLOYMENT/#prerequisites","title":"Prerequisites","text":""},{"location":"deployment/KNATIVE_DEPLOYMENT/#infrastructure-requirements","title":"Infrastructure Requirements","text":"<ul> <li>Kubernetes Cluster: v1.24+ with Knative Serving installed</li> <li>Knative Serving: v1.8+ with networking layer (Istio/Kourier)</li> <li>Container Registry: Docker Hub, GCR, ECR, or private registry</li> <li>Resource Requirements:</li> <li>Minimum: 2 vCPUs, 4GB RAM per cluster</li> <li>Recommended: 4 vCPUs, 8GB RAM per cluster</li> </ul>"},{"location":"deployment/KNATIVE_DEPLOYMENT/#tools-required","title":"Tools Required","text":"<pre><code># Install required CLI tools\nkubectl &gt;= 1.24\nkustomize &gt;= 4.5\ndocker &gt;= 20.10\nkn (Knative CLI) &gt;= 1.8\n\n# Optional but recommended\nhelm &gt;= 3.8\nistioctl &gt;= 1.15 (if using Istio)\n</code></pre>"},{"location":"deployment/KNATIVE_DEPLOYMENT/#access-requirements","title":"Access Requirements","text":"<ul> <li>Kubernetes Cluster: Admin or sufficient RBAC permissions</li> <li>NetApp ActiveIQ: API access credentials</li> <li>Container Registry: Push/pull permissions</li> </ul>"},{"location":"deployment/KNATIVE_DEPLOYMENT/#architecture-overview","title":"Architecture Overview","text":""},{"location":"deployment/KNATIVE_DEPLOYMENT/#knative-serverless-architecture","title":"Knative Serverless Architecture","text":"<pre><code>flowchart TD\n    subgraph \"Client Layer\"\n        AI[AI Assistant]\n    end\n\n    subgraph \"Knative Infrastructure\"\n        Gateway[Knative Gateway&lt;br/&gt;Istio/Kourier]\n\n        subgraph \"Knative Service\"\n            subgraph \"NetApp MCP Server Pod\"\n                subgraph \"MCP Server Container\"\n                    FastMCP[FastMCP Framework]\n                    NetAppClient[NetApp API Client]\n                    MCPTools[17 MCP Tools]\n                end\n            end\n        end\n    end\n\n    subgraph \"External Systems\"\n        NetAppUM[NetApp ActiveIQ&lt;br/&gt;Unified Manager]\n    end\n\n    AI --\"MCP Protocol\"--&gt; Gateway\n    Gateway --&gt; NetAppClient\n    FastMCP --&gt; MCPTools\n    NetAppClient --\"HTTPS API Calls\"--&gt; NetAppUM</code></pre>"},{"location":"deployment/KNATIVE_DEPLOYMENT/#key-benefits","title":"Key Benefits","text":"<ul> <li>Auto-scaling: Scale to zero when idle, scale up on demand</li> <li>Resource Efficiency: Pay only for actual usage</li> <li>High Availability: Built-in redundancy and failover</li> <li>Blue-Green Deployments: Zero-downtime updates</li> <li>Traffic Splitting: Gradual rollout capabilities</li> </ul>"},{"location":"deployment/KNATIVE_DEPLOYMENT/#container-preparation","title":"Container Preparation","text":""},{"location":"deployment/KNATIVE_DEPLOYMENT/#1-build-container-image","title":"1. Build Container Image","text":"<pre><code># Navigate to project directory\ncd /Users/brun_s/Documents/veille-technologique/Professionel/donnees-d-entree/PE-AsProduct/netapp\n\n# Build the container image\ndocker build -t netapp-mcp-server:latest \\\n  --build-arg BUILD_DATE=\"$(date -u +'%Y-%m-%dT%H:%M:%SZ')\" \\\n  --build-arg VCS_REF=\"$(git rev-parse HEAD)\" \\\n  --build-arg VERSION=\"1.0.0\" \\\n  -f Dockerfile .\n\n# Tag for registry (replace with your registry)\ndocker tag netapp-mcp-server:latest your-registry.com/netapp/mcp-server:1.0.0\ndocker tag netapp-mcp-server:latest your-registry.com/netapp/mcp-server:latest\n</code></pre>"},{"location":"deployment/KNATIVE_DEPLOYMENT/#2-push-to-container-registry","title":"2. Push to Container Registry","text":"<pre><code># Login to your container registry\ndocker login your-registry.com\n\n# Push images\ndocker push your-registry.com/netapp/mcp-server:1.0.0\ndocker push your-registry.com/netapp/mcp-server:latest\n</code></pre>"},{"location":"deployment/KNATIVE_DEPLOYMENT/#3-verify-container","title":"3. Verify Container","text":"<pre><code># Test container locally\ndocker run --rm -it \\\n  -e NETAPP_BASE_URL=\"https://your-netapp-aiqum.example.com/api\" \\\n  -e NETAPP_USERNAME=\"test-user\" \\\n  -e NETAPP_PASSWORD=\"test-password\" \\\n  your-registry.com/netapp/mcp-server:latest \\\n  python test_mcp_server.py connection\n</code></pre>"},{"location":"deployment/KNATIVE_DEPLOYMENT/#kubernetes-resources","title":"Kubernetes Resources","text":""},{"location":"deployment/KNATIVE_DEPLOYMENT/#directory-structure","title":"Directory Structure","text":"<pre><code>k8s/\n\u251c\u2500\u2500 rbac.yaml              # Namespace, ServiceAccount, RBAC\n\u251c\u2500\u2500 secret.yaml            # Credentials and certificates\n\u251c\u2500\u2500 configmap.yaml         # Application configuration\n\u251c\u2500\u2500 knative-service.yaml   # Knative Service definition\n\u2514\u2500\u2500 kustomization.yaml     # Kustomize configuration\n</code></pre>"},{"location":"deployment/KNATIVE_DEPLOYMENT/#resource-overview","title":"Resource Overview","text":"Resource Purpose Key Features Namespace Isolation <code>netapp-mcp</code> namespace ServiceAccount Identity Minimal permissions Secret Credentials NetApp API credentials ConfigMap Configuration Feature flags, endpoints Knative Service Deployment Auto-scaling, serverless NetworkPolicy Security Traffic restrictions"},{"location":"deployment/KNATIVE_DEPLOYMENT/#deployment-process","title":"Deployment Process","text":""},{"location":"deployment/KNATIVE_DEPLOYMENT/#1-prepare-environment","title":"1. Prepare Environment","text":"<pre><code># Set environment variables\nexport KUBE_NAMESPACE=\"netapp-mcp\"\nexport NETAPP_BASE_URL=\"https://your-netapp-aiqum.example.com/api\"\nexport NETAPP_USERNAME=\"your-username\"\nexport NETAPP_PASSWORD=\"your-password\"\nexport CONTAINER_REGISTRY=\"your-registry.com\"\nexport IMAGE_TAG=\"1.0.0\"\n</code></pre>"},{"location":"deployment/KNATIVE_DEPLOYMENT/#2-update-configuration","title":"2. Update Configuration","text":"<pre><code># Update secret with actual credentials\nsed -i \"s|https://your-netapp-aiqum.example.com/api|${NETAPP_BASE_URL}|g\" k8s/secret.yaml\nsed -i \"s|your-username|${NETAPP_USERNAME}|g\" k8s/secret.yaml\nsed -i \"s|your-password|${NETAPP_PASSWORD}|g\" k8s/secret.yaml\n\n# Update image reference\nsed -i \"s|netapp-mcp-server:latest|${CONTAINER_REGISTRY}/netapp/mcp-server:${IMAGE_TAG}|g\" k8s/knative-service.yaml\n</code></pre>"},{"location":"deployment/KNATIVE_DEPLOYMENT/#3-deploy-with-kustomize","title":"3. Deploy with Kustomize","text":"<pre><code># Preview deployment\nkubectl kustomize k8s/\n\n# Deploy all resources\nkubectl apply -k k8s/\n\n# Verify deployment\nkubectl get all -n netapp-mcp\n</code></pre>"},{"location":"deployment/KNATIVE_DEPLOYMENT/#4-alternative-individual-resource-deployment","title":"4. Alternative: Individual Resource Deployment","text":"<pre><code># Deploy in order\nkubectl apply -f k8s/rbac.yaml\nkubectl apply -f k8s/secret.yaml\nkubectl apply -f k8s/configmap.yaml\nkubectl apply -f k8s/knative-service.yaml\n</code></pre>"},{"location":"deployment/KNATIVE_DEPLOYMENT/#5-verify-deployment","title":"5. Verify Deployment","text":"<pre><code># Check Knative service status\nkn service describe netapp-mcp-server -n netapp-mcp\n\n# Check pod status\nkubectl get pods -n netapp-mcp\n\n# Check service logs\nkubectl logs -f -l app=netapp-mcp-server -n netapp-mcp\n\n# Test service endpoint\nkubectl get ksvc netapp-mcp-server -n netapp-mcp -o jsonpath='{.status.url}'\n</code></pre>"},{"location":"deployment/KNATIVE_DEPLOYMENT/#configuration-management","title":"Configuration Management","text":""},{"location":"deployment/KNATIVE_DEPLOYMENT/#environment-specific-configurations","title":"Environment-Specific Configurations","text":""},{"location":"deployment/KNATIVE_DEPLOYMENT/#development-environment","title":"Development Environment","text":"<pre><code># k8s/overlays/dev/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nnamespace: netapp-mcp-dev\n\nresources:\n  - ../../base\n\npatches:\n  - target:\n      kind: Service\n      name: netapp-mcp-server\n      group: serving.knative.dev\n    patch: |-\n      - op: replace\n        path: /spec/template/metadata/annotations/autoscaling.knative.dev~1minScale\n        value: \"0\"\n      - op: replace\n        path: /spec/template/spec/containers/0/resources/limits/memory\n        value: \"256Mi\"\n</code></pre>"},{"location":"deployment/KNATIVE_DEPLOYMENT/#production-environment","title":"Production Environment","text":"<pre><code># k8s/overlays/prod/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nnamespace: netapp-mcp-prod\n\nresources:\n  - ../../base\n\npatches:\n  - target:\n      kind: Service\n      name: netapp-mcp-server\n      group: serving.knative.dev\n    patch: |-\n      - op: replace\n        path: /spec/template/metadata/annotations/autoscaling.knative.dev~1minScale\n        value: \"2\"\n      - op: replace\n        path: /spec/template/metadata/annotations/autoscaling.knative.dev~1maxScale\n        value: \"50\"\n      - op: replace\n        path: /spec/template/spec/containers/0/resources/limits/memory\n        value: \"1Gi\"\n</code></pre>"},{"location":"deployment/KNATIVE_DEPLOYMENT/#credential-management","title":"Credential Management","text":""},{"location":"deployment/KNATIVE_DEPLOYMENT/#using-external-secret-operators","title":"Using External Secret Operators","text":"<pre><code># For external-secrets operator\napiVersion: external-secrets.io/v1beta1\nkind: SecretStore\nmetadata:\n  name: vault-backend\n  namespace: netapp-mcp\nspec:\n  provider:\n    vault:\n      server: \"https://vault.company.com\"\n      path: \"secret\"\n      auth:\n        kubernetes:\n          mountPath: \"kubernetes\"\n          role: \"netapp-mcp-role\"\n---\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: netapp-credentials\n  namespace: netapp-mcp\nspec:\n  refreshInterval: 1h\n  secretStoreRef:\n    name: vault-backend\n    kind: SecretStore\n  target:\n    name: netapp-activeiq-credentials\n    creationPolicy: Owner\n  data:\n    - secretKey: NETAPP_BASE_URL\n      remoteRef:\n        key: netapp/activeiq\n        property: base_url\n    - secretKey: NETAPP_USERNAME\n      remoteRef:\n        key: netapp/activeiq\n        property: username\n    - secretKey: NETAPP_PASSWORD\n      remoteRef:\n        key: netapp/activeiq\n        property: password\n</code></pre>"},{"location":"deployment/KNATIVE_DEPLOYMENT/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"deployment/KNATIVE_DEPLOYMENT/#prometheus-metrics","title":"Prometheus Metrics","text":"<pre><code># ServiceMonitor for Prometheus\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: netapp-mcp-server\n  namespace: netapp-mcp\nspec:\n  selector:\n    matchLabels:\n      app: netapp-mcp-server\n  endpoints:\n  - port: http\n    path: /metrics\n    interval: 30s\n    scrapeTimeout: 10s\n</code></pre>"},{"location":"deployment/KNATIVE_DEPLOYMENT/#logging-configuration","title":"Logging Configuration","text":"<pre><code># Fluent Bit configuration for log aggregation\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: fluent-bit-config\n  namespace: netapp-mcp\ndata:\n  fluent-bit.conf: |\n    [INPUT]\n        Name tail\n        Path /var/log/containers/netapp-mcp-server-*.log\n        Tag kube.netapp.mcp.*\n        Parser docker\n        DB /var/log/flb_kube.db\n        Mem_Buf_Limit 50MB\n\n    [OUTPUT]\n        Name forward\n        Match kube.netapp.mcp.*\n        Host fluent-aggregator.logging.svc.cluster.local\n        Port 24224\n</code></pre>"},{"location":"deployment/KNATIVE_DEPLOYMENT/#health-check-endpoints","title":"Health Check Endpoints","text":"<pre><code># Add to mcp_server.py for health checks\n@mcp.tool()\nasync def health_check() -&gt; str:\n    \"\"\"Health check endpoint for Kubernetes probes\"\"\"\n    try:\n        client = get_client()\n        # Simple test to verify connectivity\n        await client._make_request(\"GET\", \"/admin/system\")\n        return json.dumps({\"status\": \"healthy\", \"timestamp\": datetime.utcnow().isoformat()})\n    except Exception as e:\n        return json.dumps({\"status\": \"unhealthy\", \"error\": str(e), \"timestamp\": datetime.utcnow().isoformat()})\n</code></pre>"},{"location":"deployment/KNATIVE_DEPLOYMENT/#security-considerations","title":"Security Considerations","text":""},{"location":"deployment/KNATIVE_DEPLOYMENT/#network-security","title":"Network Security","text":"<pre><code># Istio VirtualService for additional security\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: netapp-mcp-server\n  namespace: netapp-mcp\nspec:\n  hosts:\n  - netapp-mcp-server.netapp-mcp.svc.cluster.local\n  http:\n  - match:\n    - headers:\n        authorization:\n          prefix: \"Bearer \"\n    route:\n    - destination:\n        host: netapp-mcp-server.netapp-mcp.svc.cluster.local\n  - route:\n    - destination:\n        host: netapp-mcp-server.netapp-mcp.svc.cluster.local\n      weight: 0\n    fault:\n      abort:\n        percentage:\n          value: 100\n        httpStatus: 401\n</code></pre>"},{"location":"deployment/KNATIVE_DEPLOYMENT/#pod-security-standards","title":"Pod Security Standards","text":"<pre><code># PodSecurityPolicy (deprecated) or Pod Security Standards\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: netapp-mcp\n  labels:\n    pod-security.kubernetes.io/enforce: restricted\n    pod-security.kubernetes.io/audit: restricted\n    pod-security.kubernetes.io/warn: restricted\n</code></pre>"},{"location":"deployment/KNATIVE_DEPLOYMENT/#scaling-and-performance","title":"Scaling and Performance","text":""},{"location":"deployment/KNATIVE_DEPLOYMENT/#autoscaling-configuration","title":"Autoscaling Configuration","text":"<pre><code># Custom autoscaling based on CPU and memory\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: netapp-mcp-hpa\n  namespace: netapp-mcp\nspec:\n  scaleTargetRef:\n    apiVersion: serving.knative.dev/v1\n    kind: Service\n    name: netapp-mcp-server\n  minReplicas: 1\n  maxReplicas: 20\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization\n        averageUtilization: 80\n  behavior:\n    scaleUp:\n      stabilizationWindowSeconds: 60\n      policies:\n      - type: Percent\n        value: 100\n        periodSeconds: 60\n    scaleDown:\n      stabilizationWindowSeconds: 300\n      policies:\n      - type: Percent\n        value: 50\n        periodSeconds: 60\n</code></pre>"},{"location":"deployment/KNATIVE_DEPLOYMENT/#performance-tuning","title":"Performance Tuning","text":"<pre><code># Knative Service with performance optimizations\nspec:\n  template:\n    metadata:\n      annotations:\n        # Cold start optimization\n        autoscaling.knative.dev/activation-scale: \"3\"\n        # Request routing optimization\n        serving.knative.dev/rollout-duration: \"120s\"\n        # Resource optimization\n        run.googleapis.com/cpu-throttling: \"false\"\n    spec:\n      containers:\n      - name: netapp-mcp-server\n        resources:\n          requests:\n            memory: \"512Mi\"\n            cpu: \"500m\"\n          limits:\n            memory: \"1Gi\"\n            cpu: \"1000m\"\n        env:\n        - name: PYTHONUNBUFFERED\n          value: \"1\"\n        - name: HTTPX_POOL_CONNECTIONS\n          value: \"20\"\n        - name: HTTPX_POOL_MAX_KEEPALIVE\n          value: \"10\"\n</code></pre>"},{"location":"deployment/KNATIVE_DEPLOYMENT/#troubleshooting","title":"Troubleshooting","text":""},{"location":"deployment/KNATIVE_DEPLOYMENT/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"deployment/KNATIVE_DEPLOYMENT/#1-pod-startup-issues","title":"1. Pod Startup Issues","text":"<pre><code># Check pod events\nkubectl describe pod -l app=netapp-mcp-server -n netapp-mcp\n\n# Check container logs\nkubectl logs -l app=netapp-mcp-server -n netapp-mcp --previous\n\n# Check resource constraints\nkubectl top pods -n netapp-mcp\n</code></pre>"},{"location":"deployment/KNATIVE_DEPLOYMENT/#2-network-connectivity-issues","title":"2. Network Connectivity Issues","text":"<pre><code># Test network connectivity from pod\nkubectl exec -it &lt;pod-name&gt; -n netapp-mcp -- \\\n  curl -k https://your-netapp-aiqum.example.com/api/admin/system\n\n# Check DNS resolution\nkubectl exec -it &lt;pod-name&gt; -n netapp-mcp -- \\\n  nslookup your-netapp-aiqum.example.com\n\n# Verify network policies\nkubectl get networkpolicy -n netapp-mcp\n</code></pre>"},{"location":"deployment/KNATIVE_DEPLOYMENT/#3-knative-service-issues","title":"3. Knative Service Issues","text":"<pre><code># Check Knative service status\nkn service describe netapp-mcp-server -n netapp-mcp\n\n# Check revisions\nkn revision list -s netapp-mcp-server -n netapp-mcp\n\n# Check routes\nkn route describe netapp-mcp-server -n netapp-mcp\n</code></pre>"},{"location":"deployment/KNATIVE_DEPLOYMENT/#4-authentication-issues","title":"4. Authentication Issues","text":"<pre><code># Verify secret content\nkubectl get secret netapp-activeiq-credentials -n netapp-mcp -o yaml\n\n# Test credentials manually\nkubectl run test-pod --rm -i --tty --image=curlimages/curl -- \\\n  curl -u \"username:password\" \\\n  https://your-netapp-aiqum.example.com/api/admin/system\n</code></pre>"},{"location":"deployment/KNATIVE_DEPLOYMENT/#debug-mode-deployment","title":"Debug Mode Deployment","text":"<pre><code># Temporary debug configuration\nspec:\n  template:\n    spec:\n      containers:\n      - name: netapp-mcp-server\n        command: [\"/bin/bash\"]\n        args: [\"-c\", \"sleep 3600\"]\n        env:\n        - name: LOG_LEVEL\n          value: \"DEBUG\"\n        - name: PYTHONUNBUFFERED\n          value: \"1\"\n</code></pre>"},{"location":"deployment/KNATIVE_DEPLOYMENT/#maintenance-and-updates","title":"Maintenance and Updates","text":""},{"location":"deployment/KNATIVE_DEPLOYMENT/#rolling-updates","title":"Rolling Updates","text":"<pre><code># Update image tag in kustomization.yaml\nsed -i 's/newTag: .*/newTag: 1.1.0/' k8s/kustomization.yaml\n\n# Apply updates\nkubectl apply -k k8s/\n\n# Monitor rollout\nkubectl rollout status deployment/netapp-mcp-server -n netapp-mcp\n</code></pre>"},{"location":"deployment/KNATIVE_DEPLOYMENT/#blue-green-deployment","title":"Blue-Green Deployment","text":"<pre><code># Knative traffic splitting\nspec:\n  traffic:\n  - percent: 90\n    revisionName: netapp-mcp-server-v1\n  - percent: 10\n    revisionName: netapp-mcp-server-v2\n    tag: canary\n</code></pre>"},{"location":"deployment/KNATIVE_DEPLOYMENT/#backup-and-recovery","title":"Backup and Recovery","text":"<pre><code># Backup current configuration\nkubectl get all,secrets,configmaps -n netapp-mcp -o yaml &gt; netapp-mcp-backup.yaml\n\n# Restore from backup\nkubectl apply -f netapp-mcp-backup.yaml\n</code></pre>"},{"location":"deployment/KNATIVE_DEPLOYMENT/#health-monitoring","title":"Health Monitoring","text":"<pre><code>#!/bin/bash\n# Health check script\nNAMESPACE=\"netapp-mcp\"\nSERVICE=\"netapp-mcp-server\"\n\n# Check service health\nif kn service describe $SERVICE -n $NAMESPACE | grep -q \"Ready.*True\"; then\n    echo \"\u2713 Service is healthy\"\nelse\n    echo \"\u2717 Service is unhealthy\"\n    kubectl logs -l app=$SERVICE -n $NAMESPACE --tail=50\nfi\n\n# Check scaling\nREPLICAS=$(kubectl get pods -l app=$SERVICE -n $NAMESPACE --no-headers | wc -l)\necho \"Current replicas: $REPLICAS\"\n\n# Check resource usage\nkubectl top pods -l app=$SERVICE -n $NAMESPACE\n</code></pre>"},{"location":"deployment/KNATIVE_DEPLOYMENT/#integration-examples","title":"Integration Examples","text":""},{"location":"deployment/KNATIVE_DEPLOYMENT/#cicd-pipeline-github-actions","title":"CI/CD Pipeline (GitHub Actions)","text":"<pre><code># .github/workflows/deploy-knative.yml\nname: Deploy to Knative\non:\n  push:\n    branches: [main]\n    paths: ['mcp_server.py', 'k8s/**']\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v3\n\n    - name: Build and push image\n      run: |\n        docker build -t {% raw %}${{ secrets.REGISTRY }}{% endraw %}/netapp/mcp-server:{% raw %}${{ github.sha }}{% endraw %} .\n        docker push {% raw %}${{ secrets.REGISTRY }}{% endraw %}/netapp/mcp-server:{% raw %}${{ github.sha }}{% endraw %}\n\n    - name: Deploy to Kubernetes\n      run: |\n        echo \"{% raw %}${{ secrets.KUBECONFIG }}{% endraw %}\" | base64 -d &gt; kubeconfig\n        export KUBECONFIG=kubeconfig\n\n        cd k8s\n        kustomize edit set image netapp-mcp-server={% raw %}${{ secrets.REGISTRY }}{% endraw %}/netapp/mcp-server:{% raw %}${{ github.sha }}{% endraw %}\n        kubectl apply -k .\n\n        # Wait for rollout\n        kubectl rollout status ksvc/netapp-mcp-server -n netapp-mcp\n</code></pre>"},{"location":"deployment/KNATIVE_DEPLOYMENT/#temporal-workflow-integration","title":"Temporal Workflow Integration","text":"<pre><code># temporal_netapp_workflow.py\n@workflow.defn\nclass NetAppMCPWorkflow:\n    @workflow.run\n    async def run(self, request: NetAppRequest) -&gt; NetAppResponse:\n        # Use MCP server for data collection\n        mcp_data = await workflow.execute_activity(\n            query_mcp_server,\n            request,\n            start_to_close_timeout=timedelta(minutes=5)\n        )\n\n        # Process data and execute NetApp operations\n        result = await workflow.execute_activity(\n            process_netapp_data,\n            mcp_data,\n            start_to_close_timeout=timedelta(minutes=10)\n        )\n\n        return result\n\n@activity.defn\nasync def query_mcp_server(request: NetAppRequest) -&gt; dict:\n    # Connect to Knative service\n    mcp_url = \"http://netapp-mcp-server.netapp-mcp.svc.cluster.local\"\n\n    async with httpx.AsyncClient() as client:\n        response = await client.post(\n            f\"{mcp_url}/query\",\n            json=request.dict()\n        )\n        return response.json()\n</code></pre> <p>This comprehensive deployment guide provides everything needed to successfully deploy and operate the NetApp ActiveIQ MCP Server on Knative, with production-ready configurations, monitoring, security, and maintenance procedures.</p>"},{"location":"deployment/function-based-architecture/","title":"Function-Based Architecture: NetApp MCP Server on Knative","text":""},{"location":"deployment/function-based-architecture/#overview","title":"Overview","text":"<p>This document details the function-based deployment architecture that transforms NetApp storage operations into serverless, scalable functions using Knative, enabling AI-assisted storage management with automatic scaling and cost optimization.</p>"},{"location":"deployment/function-based-architecture/#architectural-paradigm-shift","title":"Architectural Paradigm Shift","text":""},{"location":"deployment/function-based-architecture/#traditional-architecture-monolithic-storage-management","title":"Traditional Architecture: Monolithic Storage Management","text":"<pre><code>graph TB\n    subgraph \"Traditional Deployment\"\n        A[Load Balancer] --&gt; B[VM/Container]\n        B --&gt; C[NetApp CLI/GUI Tools]\n        B --&gt; D[Manual Scripts]\n        B --&gt; E[Documentation]\n\n        subgraph \"Challenges\"\n            F[Always Running]\n            G[Fixed Resources]\n            H[Manual Scaling]\n            I[Single Point of Failure]\n        end\n    end</code></pre>"},{"location":"deployment/function-based-architecture/#function-based-architecture-serverless-storage-operations","title":"Function-Based Architecture: Serverless Storage Operations","text":"<pre><code>graph TB\n    subgraph \"Knative Function Architecture\"\n        A[AI Assistant] --&gt; B[Knative Gateway]\n        B --&gt; C[Function Router]\n\n        subgraph \"Auto-Scaling Functions\"\n            D[Storage Monitor Function]\n            E[Volume Provisioner Function]\n            F[SVM Manager Function]\n            G[Performance Analyzer Function]\n            H[Backup Controller Function]\n        end\n\n        C --&gt; D\n        C --&gt; E\n        C --&gt; F\n        C --&gt; G\n        C --&gt; H\n\n        subgraph \"Benefits\"\n            I[Scale to Zero]\n            J[Auto-Scaling]\n            K[Cost Optimization]\n            L[High Availability]\n        end\n    end</code></pre>"},{"location":"deployment/function-based-architecture/#function-decomposition-strategy","title":"Function Decomposition Strategy","text":""},{"location":"deployment/function-based-architecture/#netapp-operations-as-functions","title":"NetApp Operations as Functions","text":"Function Purpose Scaling Pattern Resource Profile Storage Monitor Real-time capacity and health monitoring High frequency, predictable Low CPU, Medium Memory Volume Provisioner Create and manage volumes On-demand bursts Medium CPU, High Memory SVM Manager Storage Virtual Machine operations Infrequent, scheduled High CPU, High Memory Performance Analyzer Performance metrics and analysis Periodic, data-intensive High CPU, Medium Memory Event Processor Alert and event management Event-driven, variable Low CPU, Low Memory Backup Controller Backup and snapshot operations Scheduled, batch Medium CPU, High Memory"},{"location":"deployment/function-based-architecture/#function-deployment-model","title":"Function Deployment Model","text":"<pre><code># Example: Storage Monitor Function\napiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: netapp-storage-monitor\n  namespace: netapp-functions\nspec:\n  template:\n    metadata:\n      annotations:\n        autoscaling.knative.dev/minScale: \"1\"\n        autoscaling.knative.dev/maxScale: \"10\"\n        autoscaling.knative.dev/target: \"5\"\n        autoscaling.knative.dev/targetUtilizationPercentage: \"70\"\n    spec:\n      containers:\n      - image: netapp/storage-monitor:latest\n        env:\n        - name: FUNCTION_TYPE\n          value: \"STORAGE_MONITOR\"\n        - name: NETAPP_API_ENDPOINT\n          valueFrom:\n            secretKeyRef:\n              name: netapp-credentials\n              key: endpoint\n        resources:\n          requests:\n            memory: \"256Mi\"\n            cpu: \"200m\"\n          limits:\n            memory: \"512Mi\"\n            cpu: \"500m\"\n        ports:\n        - containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 5\n</code></pre>"},{"location":"deployment/function-based-architecture/#function-orchestration-patterns","title":"Function Orchestration Patterns","text":""},{"location":"deployment/function-based-architecture/#1-direct-function-invocation","title":"1. Direct Function Invocation","text":"<pre><code>sequenceDiagram\n    participant AI as AI Assistant\n    participant GW as Knative Gateway\n    participant SM as Storage Monitor Function\n    participant API as NetApp API\n\n    AI-&gt;&gt;GW: \"Show me volume utilization\"\n    GW-&gt;&gt;SM: Route to Storage Monitor\n    Note over SM: Function scales from 0 to 1\n    SM-&gt;&gt;API: GET /volumes?fields=utilization\n    API--&gt;&gt;SM: Volume data\n    SM--&gt;&gt;GW: Formatted response\n    GW--&gt;&gt;AI: Volume utilization report\n    Note over SM: Function scales back to 0 after idle</code></pre>"},{"location":"deployment/function-based-architecture/#2-function-composition-for-complex-operations","title":"2. Function Composition for Complex Operations","text":"<pre><code>sequenceDiagram\n    participant AI as AI Assistant\n    participant GW as Knative Gateway\n    participant VM as Volume Manager\n    participant SM as Storage Monitor\n    participant PM as Performance Monitor\n    participant API as NetApp API\n\n    AI-&gt;&gt;GW: \"Create optimized volume for database\"\n    GW-&gt;&gt;VM: Route to Volume Manager\n    VM-&gt;&gt;SM: Check capacity availability\n    SM-&gt;&gt;API: GET /aggregates\n    API--&gt;&gt;SM: Aggregate data\n    SM--&gt;&gt;VM: Capacity report\n    VM-&gt;&gt;PM: Analyze performance requirements\n    PM-&gt;&gt;API: GET /performance/aggregates\n    API--&gt;&gt;PM: Performance data\n    PM--&gt;&gt;VM: Performance recommendations\n    VM-&gt;&gt;API: POST /volumes (create optimized volume)\n    API--&gt;&gt;VM: Volume creation result\n    VM--&gt;&gt;GW: Complete volume configuration\n    GW--&gt;&gt;AI: Volume created with optimization details</code></pre>"},{"location":"deployment/function-based-architecture/#3-event-driven-function-activation","title":"3. Event-Driven Function Activation","text":"<pre><code>graph LR\n    A[NetApp Event] --&gt; B[Event Bus]\n    B --&gt; C[Event Filter]\n    C --&gt; D[Function Trigger]\n\n    subgraph \"Conditional Function Activation\"\n        D --&gt; E[Critical Alert Function]\n        D --&gt; F[Capacity Alert Function]\n        D --&gt; G[Performance Alert Function]\n    end\n\n    E --&gt; H[Incident Response]\n    F --&gt; I[Auto-Scaling Action]\n    G --&gt; J[Performance Tuning]</code></pre>"},{"location":"deployment/function-based-architecture/#cost-optimization-through-functions","title":"Cost Optimization Through Functions","text":""},{"location":"deployment/function-based-architecture/#resource-utilization-comparison","title":"Resource Utilization Comparison","text":"Deployment Model Idle Resource Usage Peak Resource Usage Cost Efficiency Traditional VM 100% (always running) 100% Low Container (always-on) 80% (baseline resources) 100% Medium Knative Functions 0% (scale to zero) 100% (auto-scale) High"},{"location":"deployment/function-based-architecture/#cost-model-analysis","title":"Cost Model Analysis","text":"<pre><code># Cost comparison calculation\ndef calculate_monthly_costs():\n    # Traditional deployment\n    traditional_cost = {\n        'vm_instance': 150,  # Always running VM\n        'storage': 50,       # Persistent storage\n        'networking': 30,    # Network costs\n        'total': 230\n    }\n\n    # Function-based deployment\n    function_cost = {\n        'compute_time': 45,      # Pay per execution\n        'storage': 10,           # Minimal persistent storage\n        'networking': 15,        # Reduced network costs\n        'knative_overhead': 5,   # Platform costs\n        'total': 75\n    }\n\n    savings = traditional_cost['total'] - function_cost['total']\n    savings_percentage = (savings / traditional_cost['total']) * 100\n\n    return {\n        'traditional': traditional_cost,\n        'functions': function_cost,\n        'monthly_savings': savings,\n        'savings_percentage': savings_percentage\n    }\n\n# Result: ~67% cost reduction with function-based architecture\n</code></pre>"},{"location":"deployment/function-based-architecture/#auto-scaling-patterns","title":"Auto-Scaling Patterns","text":""},{"location":"deployment/function-based-architecture/#1-demand-based-scaling","title":"1. Demand-Based Scaling","text":"<pre><code># Horizontal Pod Autoscaler for functions\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: netapp-volume-provisioner-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: serving.knative.dev/v1\n    kind: Service\n    name: netapp-volume-provisioner\n  minReplicas: 0\n  maxReplicas: 50\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n  - type: Pods\n    pods:\n      metric:\n        name: concurrent_requests\n      target:\n        type: AverageValue\n        averageValue: \"10\"\n  behavior:\n    scaleUp:\n      stabilizationWindowSeconds: 30\n      policies:\n      - type: Percent\n        value: 100\n        periodSeconds: 30\n    scaleDown:\n      stabilizationWindowSeconds: 300\n      policies:\n      - type: Percent\n        value: 50\n        periodSeconds: 60\n</code></pre>"},{"location":"deployment/function-based-architecture/#2-predictive-scaling","title":"2. Predictive Scaling","text":"<pre><code># Predictive scaling based on historical patterns\nclass PredictiveScaler:\n    def __init__(self):\n        self.patterns = {\n            'business_hours': (9, 17),  # 9 AM to 5 PM\n            'peak_days': ['monday', 'tuesday', 'wednesday'],\n            'maintenance_windows': ['sunday_2am']\n        }\n\n    def predict_scaling_needs(self, current_time):\n        hour = current_time.hour\n        day = current_time.strftime('%A').lower()\n\n        # Pre-scale for business hours\n        if self.patterns['business_hours'][0] &lt;= hour &lt;= self.patterns['business_hours'][1]:\n            if day in self.patterns['peak_days']:\n                return {'min_scale': 2, 'max_scale': 20}\n            else:\n                return {'min_scale': 1, 'max_scale': 10}\n\n        # Scale to zero during off-hours\n        return {'min_scale': 0, 'max_scale': 5}\n\n    def apply_scaling_config(self, service_name, scaling_config):\n        # Update Knative service annotations\n        annotations = {\n            'autoscaling.knative.dev/minScale': str(scaling_config['min_scale']),\n            'autoscaling.knative.dev/maxScale': str(scaling_config['max_scale'])\n        }\n        # Apply via Kubernetes API\n        return self.update_knative_service(service_name, annotations)\n</code></pre>"},{"location":"deployment/function-based-architecture/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"deployment/function-based-architecture/#function-level-metrics","title":"Function-Level Metrics","text":"<pre><code># ServiceMonitor for function metrics\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: netapp-functions-monitor\n  namespace: netapp-functions\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: netapp-function\n  endpoints:\n  - port: metrics\n    path: /metrics\n    interval: 30s\n    scrapeTimeout: 10s\n  namespaceSelector:\n    matchNames:\n    - netapp-functions\n</code></pre>"},{"location":"deployment/function-based-architecture/#key-function-metrics","title":"Key Function Metrics","text":"Metric Purpose Alert Threshold <code>function_invocation_count</code> Track function usage N/A (informational) <code>function_duration_seconds</code> Monitor performance &gt;30s for 95<sup>th</sup> percentile <code>function_error_rate</code> Track reliability &gt;5% error rate <code>function_cold_start_duration</code> Optimize startup &gt;5s startup time <code>function_concurrent_requests</code> Scale monitoring &gt;80% of max capacity <code>function_memory_usage_bytes</code> Resource optimization &gt;90% of limit"},{"location":"deployment/function-based-architecture/#distributed-tracing","title":"Distributed Tracing","text":"<pre><code># OpenTelemetry tracing for function calls\nfrom opentelemetry import trace\nfrom opentelemetry.instrumentation.requests import RequestsInstrumentor\n\ntracer = trace.get_tracer(__name__)\n\n@mcp.tool()\nasync def create_volume_with_tracing(volume_config: dict) -&gt; str:\n    with tracer.start_as_current_span(\"create_volume\") as span:\n        span.set_attribute(\"volume.size\", volume_config.get(\"size\"))\n        span.set_attribute(\"volume.svm\", volume_config.get(\"svm\"))\n\n        try:\n            # Function execution\n            result = await netapp_client.create_volume(volume_config)\n            span.set_attribute(\"operation.status\", \"success\")\n            span.set_attribute(\"volume.uuid\", result.get(\"uuid\"))\n            return result\n        except Exception as e:\n            span.set_attribute(\"operation.status\", \"error\")\n            span.set_attribute(\"error.message\", str(e))\n            raise\n</code></pre>"},{"location":"deployment/function-based-architecture/#security-in-function-architecture","title":"Security in Function Architecture","text":""},{"location":"deployment/function-based-architecture/#function-level-security","title":"Function-Level Security","text":"<pre><code># Pod Security Context for functions\napiVersion: v1\nkind: Pod\nspec:\n  securityContext:\n    runAsNonRoot: true\n    runAsUser: 1000\n    runAsGroup: 1000\n    fsGroup: 1000\n    seccompProfile:\n      type: RuntimeDefault\n  containers:\n  - name: netapp-function\n    securityContext:\n      allowPrivilegeEscalation: false\n      readOnlyRootFilesystem: true\n      capabilities:\n        drop:\n        - ALL\n    volumeMounts:\n    - name: tmp\n      mountPath: /tmp\n    - name: var-tmp\n      mountPath: /var/tmp\n  volumes:\n  - name: tmp\n    emptyDir: {}\n  - name: var-tmp\n    emptyDir: {}\n</code></pre>"},{"location":"deployment/function-based-architecture/#network-security-for-functions","title":"Network Security for Functions","text":"<pre><code># NetworkPolicy for function isolation\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: netapp-functions-netpol\n  namespace: netapp-functions\nspec:\n  podSelector:\n    matchLabels:\n      app.kubernetes.io/component: netapp-function\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          name: knative-serving\n    ports:\n    - protocol: TCP\n      port: 8080\n  egress:\n  - to:\n    - namespaceSelector: {}\n      podSelector:\n        matchLabels:\n          app: netapp-api\n    ports:\n    - protocol: TCP\n      port: 443\n  - to: []\n    ports:\n    - protocol: TCP\n      port: 53\n    - protocol: UDP\n      port: 53\n</code></pre>"},{"location":"deployment/function-based-architecture/#deployment-strategies","title":"Deployment Strategies","text":""},{"location":"deployment/function-based-architecture/#1-blue-green-deployments","title":"1. Blue-Green Deployments","text":"<pre><code># Traffic splitting for gradual rollout\napiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: netapp-storage-monitor\nspec:\n  traffic:\n  - percent: 90\n    revisionName: netapp-storage-monitor-v1\n  - percent: 10\n    revisionName: netapp-storage-monitor-v2\n    tag: canary\n</code></pre>"},{"location":"deployment/function-based-architecture/#2-canary-deployments","title":"2. Canary Deployments","text":"<pre><code># Automated canary deployment controller\nclass CanaryController:\n    def __init__(self):\n        self.success_threshold = 0.95  # 95% success rate\n        self.error_threshold = 0.05    # 5% error rate\n\n    async def promote_canary(self, service_name, canary_revision):\n        # Monitor canary metrics for 10 minutes\n        metrics = await self.monitor_canary(canary_revision, duration=600)\n\n        if metrics['success_rate'] &gt;= self.success_threshold:\n            # Promote canary to 100% traffic\n            await self.update_traffic_split(service_name, {\n                canary_revision: 100\n            })\n            return True\n        else:\n            # Rollback canary\n            await self.rollback_canary(service_name)\n            return False\n\n    async def monitor_canary(self, revision, duration):\n        # Collect metrics from Prometheus\n        query = f'sum(rate(function_requests_total{{revision=\"{revision}\"}}[5m]))'\n        # Implementation details...\n        return {'success_rate': 0.98, 'error_rate': 0.02}\n</code></pre>"},{"location":"deployment/function-based-architecture/#best-practices","title":"Best Practices","text":""},{"location":"deployment/function-based-architecture/#function-design-principles","title":"Function Design Principles","text":"<ol> <li>Single Responsibility: Each function handles one specific NetApp operation</li> <li>Stateless Design: Functions maintain no state between invocations</li> <li>Idempotent Operations: Functions can be safely retried</li> <li>Fast Startup: Optimize cold start times for better user experience</li> <li>Resource Efficiency: Right-size function resources for optimal cost</li> </ol>"},{"location":"deployment/function-based-architecture/#performance-optimization","title":"Performance Optimization","text":"<pre><code># Connection pooling for NetApp API calls\nclass NetAppClientPool:\n    def __init__(self):\n        self.pool = asyncio.Queue(maxsize=10)\n        self.initialize_pool()\n\n    async def initialize_pool(self):\n        for _ in range(5):  # Pre-create 5 connections\n            client = NetAppClient()\n            await client.connect()\n            await self.pool.put(client)\n\n    async def get_client(self):\n        if self.pool.empty():\n            # Create new client if pool is empty\n            client = NetAppClient()\n            await client.connect()\n            return client\n        return await self.pool.get()\n\n    async def return_client(self, client):\n        if not client.is_connected():\n            await client.reconnect()\n        await self.pool.put(client)\n\n# Usage in function\n@mcp.tool()\nasync def optimized_volume_query(query_params: dict) -&gt; str:\n    client = await client_pool.get_client()\n    try:\n        result = await client.get_volumes(query_params)\n        return json.dumps(result)\n    finally:\n        await client_pool.return_client(client)\n</code></pre> <p>This function-based architecture transforms NetApp storage operations from traditional monolithic deployments to highly scalable, cost-effective serverless functions that automatically adapt to demand while maintaining high availability and performance.</p>"},{"location":"deployment/knative-functions-deployment/","title":"NetApp MCP Server - Knative Functions Deployment Guide","text":"<p>Complete guide for deploying the NetApp ActiveIQ MCP Server as Knative Functions using the <code>kn func</code> CLI for serverless, auto-scaling operation.</p>"},{"location":"deployment/knative-functions-deployment/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview</li> <li>Prerequisites</li> <li>Installation Setup</li> <li>Complete MCP Server Deployment</li> <li>Individual Function Components</li> <li>Testing and Validation</li> <li>Management Operations</li> <li>Traffic Management</li> <li>Monitoring and Troubleshooting</li> <li>Production Considerations</li> </ol>"},{"location":"deployment/knative-functions-deployment/#overview","title":"Overview","text":"<p>This deployment method transforms the NetApp MCP Server into serverless Knative Functions that:</p> <ul> <li>Scale to Zero: No resource consumption when idle</li> <li>Auto-scale: Instant scaling based on demand</li> <li>Cost Optimized: Pay only for actual usage</li> <li>High Availability: Built-in redundancy and failover</li> <li>Blue-Green Deployments: Zero-downtime updates</li> </ul>"},{"location":"deployment/knative-functions-deployment/#architecture-benefits","title":"Architecture Benefits","text":"<pre><code>flowchart TD\n    A[AI Assistant] --&gt;|MCP Protocol| B[Knative Gateway&lt;br/&gt;Istio/Kourier]\n    B --&gt; C[NetApp MCP Server Function]\n\n    subgraph C[\"NetApp MCP Server Function\"]\n        subgraph D[\"Auto-scaling Pod\"]\n            subgraph E[\"MCP Server Container\"]\n                F[FastMCP Framework]\n                G[NetApp API Client]\n                H[17 MCP Tools]\n            end\n        end\n    end\n\n    C --&gt;|HTTPS API Calls| I[NetApp ActiveIQ Unified Manager&lt;br/&gt;External System]\n\n    style A fill:#e1f5fe\n    style B fill:#f3e5f5\n    style C fill:#e8f5e8\n    style D fill:#fff3e0\n    style E fill:#fce4ec\n    style I fill:#fff8e1</code></pre>"},{"location":"deployment/knative-functions-deployment/#prerequisites","title":"Prerequisites","text":""},{"location":"deployment/knative-functions-deployment/#infrastructure-requirements","title":"Infrastructure Requirements","text":"<ul> <li>Kubernetes Cluster: v1.24+ with Knative Serving installed</li> <li>Knative Serving: v1.8+ with networking layer (Istio/Kourier)</li> <li>Container Registry: Docker Hub, GCR, ECR, or private registry access</li> <li>Resource Requirements:</li> <li>Minimum: 2 vCPUs, 4GB RAM per cluster</li> <li>Recommended: 4 vCPUs, 8GB RAM per cluster</li> </ul>"},{"location":"deployment/knative-functions-deployment/#access-requirements","title":"Access Requirements","text":"<ul> <li>Kubernetes Cluster: Admin or sufficient RBAC permissions</li> <li>NetApp ActiveIQ: API access credentials</li> <li>Container Registry: Push/pull permissions</li> </ul>"},{"location":"deployment/knative-functions-deployment/#software-dependencies","title":"Software Dependencies","text":"<ul> <li><code>kubectl</code> &gt;= 1.24</li> <li><code>docker</code> &gt;= 20.10 (for building images)</li> <li>Access to NetApp ActiveIQ Unified Manager</li> </ul>"},{"location":"deployment/knative-functions-deployment/#installation-setup","title":"Installation Setup","text":""},{"location":"deployment/knative-functions-deployment/#step-1-install-knative-cli-tools","title":"Step 1: Install Knative CLI Tools","text":"<pre><code># Install Knative CLI\ncurl -L https://github.com/knative/client/releases/latest/download/kn-darwin-amd64 -o kn\nchmod +x kn &amp;&amp; sudo mv kn /usr/local/bin/\n\n# Install Knative func CLI\ncurl -L https://github.com/knative/func/releases/latest/download/func_darwin_amd64.tar.gz | tar xz\nchmod +x func &amp;&amp; sudo mv func /usr/local/bin/\n\n# Verify installation\nkn version\nfunc version\nkubectl version --client\n</code></pre>"},{"location":"deployment/knative-functions-deployment/#step-2-verify-knative-installation","title":"Step 2: Verify Knative Installation","text":"<pre><code># Check Knative Serving components\nkubectl get pods -n knative-serving\n\n# Verify networking layer\nkubectl get pods -n kourier-system || kubectl get pods -n istio-system\n\n# Check custom resource definitions\nkubectl get crd | grep knative\n</code></pre>"},{"location":"deployment/knative-functions-deployment/#step-3-prepare-environment-variables","title":"Step 3: Prepare Environment Variables","text":"<pre><code># Set deployment configuration\nexport REGISTRY=\"your-registry.com/netapp\"\nexport NAMESPACE=\"netapp-mcp\"\nexport IMAGE_TAG=\"latest\"\n\n# NetApp ActiveIQ Configuration\nexport NETAPP_BASE_URL=\"https://your-netapp-aiqum.example.com/api\"\nexport NETAPP_USERNAME=\"your-username\"\nexport NETAPP_PASSWORD=\"your-password\"\nexport NETAPP_VERIFY_SSL=\"false\"\n\n# MCP Server Configuration\nexport MCP_SERVER_PORT=\"8080\"\nexport MCP_LOG_LEVEL=\"INFO\"\n\n# Optional: Advanced Settings\nexport MCP_TIMEOUT=\"30\"\nexport MCP_MAX_CONNECTIONS=\"100\"\nexport MCP_CACHE_TTL=\"300\"\n</code></pre>"},{"location":"deployment/knative-functions-deployment/#deployment-methods-overview","title":"Deployment Methods Overview","text":""},{"location":"deployment/knative-functions-deployment/#comparison-knative-function-cli-vs-manual-yaml-vs-source-to-url","title":"Comparison: Knative Function CLI vs Manual YAML vs Source-to-URL","text":"Method Build &amp; Deploy Command Source Format Registry Needed Example Use Case Knative Functions CLI <code>kn func deploy --registry &lt;registry&gt;</code> Local project Yes Quick local development Manual YAML <code>kubectl apply -f &lt;manifest&gt;.yaml</code> Container image Yes Custom configuration Source-to-URL (Kaniko/Ko) <code>kubectl apply -f &lt;manifest&gt;.yaml</code> (with build spec) Git repo (source) Yes CI/CD pipelines, Go apps Direct MCP Server <code>npx -y @modelcontextprotocol/server-filesystem /data</code> Container args Yes Standard MCP servers"},{"location":"deployment/knative-functions-deployment/#key-benefits-of-kn-func-approach","title":"Key Benefits of kn func Approach","text":"<ul> <li>Streamlined Development: <code>kn func deploy</code> for one-command deployment from local code</li> <li>Automatic Container Building: Built-in Buildpacks support for Python, Node.js, Go</li> <li>Function Templates: Pre-configured templates for HTTP, CloudEvents</li> <li>Integrated Testing: Built-in function invoke capabilities</li> <li>Auto-scaling: Knative provides concurrency controls and scale-to-zero out of the box</li> <li>Blue-Green Deployments: Native traffic splitting and rollback capabilities</li> </ul>"},{"location":"deployment/knative-functions-deployment/#complete-mcp-server-deployment","title":"Complete MCP Server Deployment","text":""},{"location":"deployment/knative-functions-deployment/#step-1-create-function-structure","title":"Step 1: Create Function Structure","text":"<pre><code># Method 1: Create function using kn func CLI (Recommended)\nkn func create netapp-mcp-server --language python --template http\ncd netapp-mcp-server\n\n# Method 2: Create in existing directory\nmkdir netapp-mcp-server &amp;&amp; cd netapp-mcp-server\nfunc create . --language python --template http\n</code></pre>"},{"location":"deployment/knative-functions-deployment/#step-2-configure-function-specification","title":"Step 2: Configure Function Specification","text":"<p>Create <code>func.yaml</code>:</p> <pre><code>specVersion: 0.35.0\nname: netapp-mcp-server\nruntime: python\nregistry: your-registry.com/netapp\nimage: your-registry.com/netapp/netapp-mcp-server:latest\ncreated: 2025-06-28T14:00:00Z\ninvoke: gunicorn\nbuild:\n  builder: pack\n  buildpacks:\n    - gcr.io/paketo-buildpacks/python\n  env:\n    - name: BP_PIP_VERSION\n      value: \"latest\"\nrun:\n  env:\n    - name: FUNCTION_TARGET\n      value: main\n    - name: PYTHONUNBUFFERED\n      value: \"1\"\ndeploy:\n  namespace: netapp-mcp\n  options:\n    scale:\n      min: 0\n      max: 10\n      metric: concurrency\n      target: 100\n    resources:\n      requests:\n        cpu: 500m\n        memory: 512Mi\n      limits:\n        cpu: 2000m\n        memory: 1Gi\n    annotations:\n      autoscaling.knative.dev/scaleDownDelay: \"60s\"\n      autoscaling.knative.dev/scaleUpDelay: \"0s\"\n      serving.knative.dev/timeout-seconds: \"300\"\n</code></pre>"},{"location":"deployment/knative-functions-deployment/#step-3-prepare-dependencies","title":"Step 3: Prepare Dependencies","text":"<p>Create <code>requirements.txt</code>:</p> <pre><code>fastmcp&gt;=0.5.0\nhttpx&gt;=0.25.0\npydantic&gt;=2.0.0\npython-dotenv&gt;=1.0.0\nuvicorn&gt;=0.24.0\nparliament-functions&gt;=1.0.0\naiofiles&gt;=23.0.0\nasyncio-mqtt&gt;=0.13.0\n</code></pre>"},{"location":"deployment/knative-functions-deployment/#step-4-copy-mcp-server-code","title":"Step 4: Copy MCP Server Code","text":"<pre><code># Copy the main MCP server implementation\ncp ../mcp_server.py func.py\n\n# Ensure the main function is compatible with Knative Functions\n# The func.py should have a main() function that handles HTTP requests\n</code></pre>"},{"location":"deployment/knative-functions-deployment/#step-5-adapt-mcp-server-for-functions","title":"Step 5: Adapt MCP Server for Functions","text":"<p>Create a function-compatible wrapper in <code>func.py</code>:</p> <pre><code>import asyncio\nimport json\nimport os\nfrom datetime import datetime\nfrom parliament import Context\nfrom typing import Dict, Any\n\n# Import your existing MCP server components\ntry:\n    from mcp_server import (\n        get_clusters, get_aggregates, get_volumes, create_volume,\n        delete_volume, modify_volume, create_svm, get_svms,\n        configure_svm, test_connection\n    )\nexcept ImportError:\n    # Fallback implementations for testing\n    async def get_clusters(**kwargs):\n        return {\"records\": [], \"num_records\": 0}\n\n    async def test_connection(**kwargs):\n        return {\"status\": \"connected\", \"timestamp\": datetime.utcnow().isoformat()}\n\n# Function entry point\nasync def main(context: Context) -&gt; Dict[str, Any]:\n    \"\"\"\n    NetApp MCP Server Function main entry point\n    Handles HTTP requests and routes them to appropriate MCP tools\n    \"\"\"\n    try:\n        # Parse request\n        if hasattr(context.request, 'json') and context.request.json:\n            request_data = context.request.json\n        else:\n            # Handle GET requests with query parameters\n            request_data = dict(context.request.query_params) if hasattr(context.request, 'query_params') else {}\n\n        # Extract operation and arguments\n        operation = request_data.get('operation', 'health_check')\n        arguments = request_data.get('arguments', {})\n\n        # Route to appropriate MCP tool\n        result = await route_operation(operation, arguments)\n\n        return {\n            'statusCode': 200,\n            'headers': {'Content-Type': 'application/json'},\n            'body': json.dumps({\n                'success': True,\n                'operation': operation,\n                'timestamp': datetime.utcnow().isoformat(),\n                'function_name': os.getenv('K_SERVICE', 'netapp-mcp-server'),\n                'result': result\n            })\n        }\n\n    except Exception as e:\n        return {\n            'statusCode': 500,\n            'headers': {'Content-Type': 'application/json'},\n            'body': json.dumps({\n                'success': False,\n                'error': str(e),\n                'operation': request_data.get('operation', 'unknown'),\n                'timestamp': datetime.utcnow().isoformat(),\n                'function_name': os.getenv('K_SERVICE', 'netapp-mcp-server')\n            })\n        }\n\nasync def route_operation(operation: str, arguments: Dict[str, Any]) -&gt; Any:\n    \"\"\"Route operations to appropriate MCP tools\"\"\"\n\n    # Health check\n    if operation == 'health_check':\n        return await test_connection()\n\n    # Cluster operations\n    elif operation == 'get_clusters':\n        return await get_clusters(**arguments)\n\n    # Aggregate operations\n    elif operation == 'get_aggregates':\n        return await get_aggregates(**arguments)\n\n    # Volume operations\n    elif operation == 'get_volumes':\n        return await get_volumes(**arguments)\n    elif operation == 'create_volume':\n        return await create_volume(**arguments)\n    elif operation == 'delete_volume':\n        return await delete_volume(**arguments)\n    elif operation == 'modify_volume':\n        return await modify_volume(**arguments)\n\n    # SVM operations\n    elif operation == 'get_svms':\n        return await get_svms(**arguments)\n    elif operation == 'create_svm':\n        return await create_svm(**arguments)\n    elif operation == 'configure_svm':\n        return await configure_svm(**arguments)\n\n    # Connection test\n    elif operation == 'test_connection':\n        return await test_connection(**arguments)\n\n    else:\n        raise ValueError(f\"Unknown operation: {operation}\")\n</code></pre>"},{"location":"deployment/knative-functions-deployment/#step-6-deploy-the-function","title":"Step 6: Deploy the Function","text":"<pre><code># Create namespace if it doesn't exist\nkubectl create namespace netapp-mcp --dry-run=client -o yaml | kubectl apply -f -\n\n# Deploy function with all environment variables\nfunc deploy \\\n  --namespace netapp-mcp \\\n  --env NETAPP_BASE_URL=\"${NETAPP_BASE_URL}\" \\\n  --env NETAPP_USERNAME=\"${NETAPP_USERNAME}\" \\\n  --env NETAPP_PASSWORD=\"${NETAPP_PASSWORD}\" \\\n  --env NETAPP_VERIFY_SSL=\"${NETAPP_VERIFY_SSL}\" \\\n  --env MCP_SERVER_PORT=\"${MCP_SERVER_PORT}\" \\\n  --env MCP_LOG_LEVEL=\"${MCP_LOG_LEVEL}\" \\\n  --env MCP_TIMEOUT=\"${MCP_TIMEOUT}\" \\\n  --env MCP_MAX_CONNECTIONS=\"${MCP_MAX_CONNECTIONS}\" \\\n  --env MCP_CACHE_TTL=\"${MCP_CACHE_TTL}\" \\\n  --build \\\n  --verbose\n</code></pre>"},{"location":"deployment/knative-functions-deployment/#individual-function-components","title":"Individual Function Components","text":""},{"location":"deployment/knative-functions-deployment/#deploy-using-existing-script","title":"Deploy Using Existing Script","text":"<pre><code># Use the provided script for individual function deployment\ncd /Users/brun_s/Documents/veille-technologique/Professionel/donnees-d-entree/PE-AsProduct/netapp\n\n# Set environment variables\nexport NAMESPACE=\"netapp-functions\"\nexport REGISTRY=\"${REGISTRY}\"\nexport NETAPP_BASE_URL=\"${NETAPP_BASE_URL}\"\nexport NETAPP_USERNAME=\"${NETAPP_USERNAME}\"\nexport NETAPP_PASSWORD=\"${NETAPP_PASSWORD}\"\n\n# Deploy individual functions\n./scripts/deploy-knative-functions.sh\n</code></pre>"},{"location":"deployment/knative-functions-deployment/#testing-and-validation","title":"Testing and Validation","text":""},{"location":"deployment/knative-functions-deployment/#step-1-verify-deployment","title":"Step 1: Verify Deployment","text":"<pre><code># Check function status\nkn service list -n netapp-mcp\n\n# Get function details\nkn service describe netapp-mcp-server -n netapp-mcp\n\n# Check pods\nkubectl get pods -n netapp-mcp\n\n# View function logs\nkn func logs netapp-mcp-server --follow\n</code></pre>"},{"location":"deployment/knative-functions-deployment/#step-2-get-function-url","title":"Step 2: Get Function URL","text":"<pre><code># Get the function URL\nMCP_URL=$(kn service describe netapp-mcp-server -n netapp-mcp -o jsonpath='{.status.url}')\necho \"MCP Server URL: $MCP_URL\"\n\n# Store for testing\nexport MCP_FUNCTION_URL=\"$MCP_URL\"\n</code></pre>"},{"location":"deployment/knative-functions-deployment/#step-3-basic-health-check","title":"Step 3: Basic Health Check","text":"<pre><code># Test health endpoint\ncurl -X POST \"$MCP_FUNCTION_URL\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"operation\": \"health_check\"\n  }'\n\n# Expected response:\n# {\n#   \"success\": true,\n#   \"operation\": \"health_check\",\n#   \"timestamp\": \"2025-06-28T14:00:00Z\",\n#   \"function_name\": \"netapp-mcp-server\",\n#   \"result\": {\n#     \"status\": \"connected\",\n#     \"timestamp\": \"2025-06-28T14:00:00Z\"\n#   }\n# }\n</code></pre>"},{"location":"deployment/knative-functions-deployment/#step-4-test-mcp-operations","title":"Step 4: Test MCP Operations","text":"<pre><code># Test cluster information\ncurl -X POST \"$MCP_FUNCTION_URL\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"operation\": \"get_clusters\",\n    \"arguments\": {}\n  }'\n\n# Test volume operations\ncurl -X POST \"$MCP_FUNCTION_URL\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"operation\": \"get_volumes\",\n    \"arguments\": {\n      \"fields\": \"name,size,state\"\n    }\n  }'\n\n# Test SVM operations\ncurl -X POST \"$MCP_FUNCTION_URL\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"operation\": \"get_svms\",\n    \"arguments\": {}\n  }'\n</code></pre>"},{"location":"deployment/knative-functions-deployment/#step-5-load-testing","title":"Step 5: Load Testing","text":"<pre><code># Install hey for load testing (if not installed)\n# brew install hey  # macOS\n# go install github.com/rakyll/hey@latest  # Go\n\n# Perform load test\nhey -n 100 -c 10 -m POST \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"operation\": \"get_clusters\", \"arguments\": {}}' \\\n  \"$MCP_FUNCTION_URL\"\n\n# Monitor scaling during load test\nwatch -n 1 'kubectl get pods -n netapp-mcp'\n</code></pre>"},{"location":"deployment/knative-functions-deployment/#management-operations","title":"Management Operations","text":""},{"location":"deployment/knative-functions-deployment/#update-function-configuration","title":"Update Function Configuration","text":"<pre><code># Update environment variables\nkn service update netapp-mcp-server \\\n  --env MCP_LOG_LEVEL=DEBUG \\\n  --env MCP_TIMEOUT=60 \\\n  -n netapp-mcp\n\n# Update scaling configuration\nkn service update netapp-mcp-server \\\n  --scale-min 1 \\\n  --scale-max 20 \\\n  --scale-target 50 \\\n  -n netapp-mcp\n\n# Update resource limits\nkn service update netapp-mcp-server \\\n  --limit memory=2Gi \\\n  --limit cpu=2000m \\\n  --request memory=1Gi \\\n  --request cpu=1000m \\\n  -n netapp-mcp\n</code></pre>"},{"location":"deployment/knative-functions-deployment/#function-information","title":"Function Information","text":"<pre><code># Get function URL\nkn service describe netapp-mcp-server -n netapp-mcp -o jsonpath='{.status.url}'\n\n# Get function configuration\nkn service describe netapp-mcp-server -n netapp-mcp -o yaml\n\n# List all revisions\nkn revision list -s netapp-mcp-server -n netapp-mcp\n\n# Get revision details\nkn revision describe netapp-mcp-server-&lt;revision&gt; -n netapp-mcp\n</code></pre>"},{"location":"deployment/knative-functions-deployment/#traffic-management","title":"Traffic Management","text":""},{"location":"deployment/knative-functions-deployment/#blue-green-deployment","title":"Blue-Green Deployment","text":"<pre><code># Deploy new version without traffic\nfunc deploy --tag blue --no-traffic\n\n# Test blue version\nBLUE_URL=$(kn service describe netapp-mcp-server -n netapp-mcp -o jsonpath='{.status.traffic[?(@.tag==\"blue\")].url}')\ncurl -X POST \"$BLUE_URL\" -H \"Content-Type: application/json\" -d '{\"operation\": \"health_check\"}'\n\n# Switch traffic to blue version\nkn service update netapp-mcp-server \\\n  --traffic blue=100 \\\n  -n netapp-mcp\n</code></pre>"},{"location":"deployment/knative-functions-deployment/#canary-deployment","title":"Canary Deployment","text":"<pre><code># Split traffic between versions\nkn service update netapp-mcp-server \\\n  --traffic v1=90,v2=10 \\\n  -n netapp-mcp\n\n# Gradually shift traffic\nkn service update netapp-mcp-server \\\n  --traffic v1=70,v2=30 \\\n  -n netapp-mcp\n\n# Full promotion\nkn service update netapp-mcp-server \\\n  --traffic v2=100 \\\n  -n netapp-mcp\n</code></pre>"},{"location":"deployment/knative-functions-deployment/#rollback","title":"Rollback","text":"<pre><code># List revisions\nkn revision list -s netapp-mcp-server -n netapp-mcp\n\n# Rollback to previous revision\nkn service update netapp-mcp-server \\\n  --traffic @latest=0,netapp-mcp-server-00001=100 \\\n  -n netapp-mcp\n</code></pre>"},{"location":"deployment/knative-functions-deployment/#monitoring-and-troubleshooting","title":"Monitoring and Troubleshooting","text":""},{"location":"deployment/knative-functions-deployment/#monitoring","title":"Monitoring","text":"<pre><code># View real-time logs\nkubectl logs -f -l serving.knative.dev/service=netapp-mcp-server -n netapp-mcp\n\n# Monitor function metrics\nkubectl get --raw \"/apis/metrics.k8s.io/v1beta1/namespaces/netapp-mcp/pods\" | jq\n\n# Check autoscaler logs\nkubectl logs -n knative-serving -l app=autoscaler\n\n# Monitor scaling events\nkubectl get events -n netapp-mcp --sort-by='.lastTimestamp'\n</code></pre>"},{"location":"deployment/knative-functions-deployment/#common-issues","title":"Common Issues","text":""},{"location":"deployment/knative-functions-deployment/#function-not-starting","title":"Function Not Starting","text":"<pre><code># Check pod status\nkubectl get pods -n netapp-mcp\n\n# Describe pod for events\nkubectl describe pod &lt;pod-name&gt; -n netapp-mcp\n\n# Check container logs\nkubectl logs &lt;pod-name&gt; -c user-container -n netapp-mcp\n</code></pre>"},{"location":"deployment/knative-functions-deployment/#function-not-scaling","title":"Function Not Scaling","text":"<pre><code># Check Knative serving controller logs\nkubectl logs -n knative-serving -l app=controller\n\n# Check autoscaler configuration\nkubectl get configmap config-autoscaler -n knative-serving -o yaml\n\n# Verify service configuration\nkn service describe netapp-mcp-server -n netapp-mcp\n</code></pre>"},{"location":"deployment/knative-functions-deployment/#network-issues","title":"Network Issues","text":"<pre><code># Test service from within cluster\nkubectl run test-pod --rm -it --image=curlimages/curl -- /bin/sh\n# From inside the pod:\ncurl http://netapp-mcp-server.netapp-mcp.svc.cluster.local\n\n# Check service endpoints\nkubectl get endpoints -n netapp-mcp\n\n# Verify ingress/gateway configuration\nkubectl get gateways,virtualservices -n netapp-mcp\n</code></pre>"},{"location":"deployment/knative-functions-deployment/#debugging-mode","title":"Debugging Mode","text":"<pre><code># Deploy function with debug settings\nfunc deploy \\\n  --env LOG_LEVEL=DEBUG \\\n  --env PYTHONUNBUFFERED=1 \\\n  --env MCP_DEBUG=true \\\n  -n netapp-mcp\n\n# Enable debug annotations\nkn service update netapp-mcp-server \\\n  --annotation serving.knative.dev/creator=debug-user \\\n  -n netapp-mcp\n</code></pre>"},{"location":"deployment/knative-functions-deployment/#production-considerations","title":"Production Considerations","text":""},{"location":"deployment/knative-functions-deployment/#security","title":"Security","text":"<pre><code># Create namespace with security labels\nkubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: netapp-mcp\n  labels:\n    pod-security.kubernetes.io/enforce: restricted\n    pod-security.kubernetes.io/audit: restricted\n    pod-security.kubernetes.io/warn: restricted\nEOF\n\n# Use secrets for credentials\nkubectl create secret generic netapp-credentials \\\n  --from-literal=NETAPP_BASE_URL=\"$NETAPP_BASE_URL\" \\\n  --from-literal=NETAPP_USERNAME=\"$NETAPP_USERNAME\" \\\n  --from-literal=NETAPP_PASSWORD=\"$NETAPP_PASSWORD\" \\\n  -n netapp-mcp\n\n# Deploy with secret reference\nkn service update netapp-mcp-server \\\n  --env-from secret:netapp-credentials \\\n  -n netapp-mcp\n</code></pre>"},{"location":"deployment/knative-functions-deployment/#performance-optimization","title":"Performance Optimization","text":"<pre><code># Optimized func.yaml for production\ndeploy:\n  options:\n    scale:\n      min: 2  # Keep minimum instances warm\n      max: 50  # Allow higher scaling\n      metric: rps  # Requests per second scaling\n      target: 10\n    resources:\n      requests:\n        cpu: 1000m\n        memory: 1Gi\n      limits:\n        cpu: 4000m\n        memory: 2Gi\n    annotations:\n      autoscaling.knative.dev/scaleDownDelay: \"300s\"\n      autoscaling.knative.dev/window: \"60s\"\n      autoscaling.knative.dev/targetUtilizationPercentage: \"70\"\n      run.googleapis.com/cpu-throttling: \"false\"\n</code></pre>"},{"location":"deployment/knative-functions-deployment/#monitoring-setup","title":"Monitoring Setup","text":"<pre><code># Install Prometheus ServiceMonitor\nkubectl apply -f - &lt;&lt;EOF\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: netapp-mcp-server\n  namespace: netapp-mcp\nspec:\n  selector:\n    matchLabels:\n      serving.knative.dev/service: netapp-mcp-server\n  endpoints:\n  - port: http-userland\n    path: /metrics\n    interval: 30s\nEOF\n</code></pre>"},{"location":"deployment/knative-functions-deployment/#backup-and-recovery","title":"Backup and Recovery","text":"<pre><code># Backup function configuration\nkubectl get service.serving.knative.dev/netapp-mcp-server -n netapp-mcp -o yaml &gt; netapp-mcp-server-backup.yaml\n\n# Backup secrets\nkubectl get secret netapp-credentials -n netapp-mcp -o yaml &gt; netapp-credentials-backup.yaml\n\n# Restore from backup\nkubectl apply -f netapp-mcp-server-backup.yaml\nkubectl apply -f netapp-credentials-backup.yaml\n</code></pre>"},{"location":"deployment/knative-functions-deployment/#conclusion","title":"Conclusion","text":"<p>This guide provides comprehensive instructions for deploying the NetApp MCP Server as Knative Functions. The serverless approach offers significant benefits in terms of cost optimization, scalability, and operational efficiency while maintaining full functionality of the MCP server.</p> <p>For additional information, refer to: - Knative Functions Documentation - Architecture Overview - Troubleshooting Guide</p>"},{"location":"examples/basic-usage/","title":"Basic Usage Examples","text":"<p>This guide provides practical examples of using the NetApp ActiveIQ MCP Server with AI assistants and programmatic clients.</p>"},{"location":"examples/basic-usage/#prerequisites","title":"Prerequisites","text":"<ul> <li>NetApp ActiveIQ MCP Server running and accessible</li> <li>Valid NetApp credentials configured</li> <li>MCP client or AI assistant with MCP support</li> </ul>"},{"location":"examples/basic-usage/#getting-started-examples","title":"Getting Started Examples","text":""},{"location":"examples/basic-usage/#1-test-your-connection","title":"1. Test Your Connection","text":"<p>Before starting, verify your MCP server can connect to NetApp:</p>"},{"location":"examples/basic-usage/#with-ai-assistant-claude-desktop","title":"With AI Assistant (Claude Desktop)","text":"<pre><code>\"Test the connection to NetApp and show me the server status\"\n</code></pre>"},{"location":"examples/basic-usage/#programmatic-usage","title":"Programmatic Usage","text":"<pre><code>import asyncio\nfrom mcp_client import MCPClient\n\nasync def test_setup():\n    client = MCPClient(\"http://localhost:8080\")\n\n    # Test connection\n    result = await client.call_tool(\"test_connection\", {\n        \"include_details\": True\n    })\n\n    print(f\"Connection status: {result}\")\n\nasyncio.run(test_setup())\n</code></pre> <p>Expected Response: <pre><code>{\n  \"success\": true,\n  \"connection_status\": \"healthy\",\n  \"netapp_version\": \"9.12.1\",\n  \"response_time_ms\": 150,\n  \"details\": {\n    \"um_host\": \"unified-manager.company.com\",\n    \"api_version\": \"v2\",\n    \"authenticated_user\": \"serviceaccount\"\n  }\n}\n</code></pre></p>"},{"location":"examples/basic-usage/#basic-information-queries","title":"Basic Information Queries","text":""},{"location":"examples/basic-usage/#2-list-all-clusters","title":"2. List All Clusters","text":""},{"location":"examples/basic-usage/#natural-language-ai-assistant","title":"Natural Language (AI Assistant)","text":"<pre><code>\"Show me all the NetApp clusters and their current status\"\n</code></pre>"},{"location":"examples/basic-usage/#programmatic","title":"Programmatic","text":"<pre><code>async def list_clusters():\n    client = MCPClient(\"http://localhost:8080\")\n\n    clusters = await client.call_tool(\"get_clusters\", {\n        \"fields\": [\"name\", \"state\", \"version\", \"nodes\", \"management_ip\"]\n    })\n\n    print(\"Available Clusters:\")\n    for cluster in clusters[\"clusters\"]:\n        print(f\"  - {cluster['name']}: {cluster['state']} (v{cluster['version']['full']})\")\n\nasyncio.run(list_clusters())\n</code></pre> <p>Sample Output: <pre><code>Available Clusters:\n  - prod-cluster-01: up (v9.12.1)\n  - dev-cluster-02: up (v9.11.1)\n  - test-cluster-03: up (v9.12.1)\n</code></pre></p>"},{"location":"examples/basic-usage/#3-check-storage-volumes","title":"3. Check Storage Volumes","text":""},{"location":"examples/basic-usage/#natural-language","title":"Natural Language","text":"<pre><code>\"What volumes do we have and how much space is available?\"\n</code></pre>"},{"location":"examples/basic-usage/#programmatic_1","title":"Programmatic","text":"<pre><code>async def check_volumes():\n    client = MCPClient(\"http://localhost:8080\")\n\n    volumes = await client.call_tool(\"get_volumes\", {\n        \"fields\": [\"name\", \"svm\", \"cluster\", \"size\", \"available\", \"used_percentage\"],\n        \"max_records\": 20\n    })\n\n    print(\"Volume Status:\")\n    print(f\"{'Volume':&lt;20} {'SVM':&lt;15} {'Size (GB)':&lt;10} {'Used %':&lt;8} {'Available (GB)':&lt;15}\")\n    print(\"-\" * 80)\n\n    for vol in volumes[\"volumes\"]:\n        size_gb = vol[\"size\"][\"total\"] / (1024**3)\n        avail_gb = vol[\"size\"][\"available\"] / (1024**3)\n        used_pct = vol[\"used_percentage\"]\n\n        print(f\"{vol['name']:&lt;20} {vol['svm']['name']:&lt;15} {size_gb:&lt;10.1f} {used_pct:&lt;8.1f} {avail_gb:&lt;15.1f}\")\n\nasyncio.run(check_volumes())\n</code></pre>"},{"location":"examples/basic-usage/#4-monitor-storage-virtual-machines","title":"4. Monitor Storage Virtual Machines","text":""},{"location":"examples/basic-usage/#natural-language_1","title":"Natural Language","text":"<pre><code>\"List all SVMs and show which protocols they support\"\n</code></pre>"},{"location":"examples/basic-usage/#programmatic_2","title":"Programmatic","text":"<pre><code>async def list_svms():\n    client = MCPClient(\"http://localhost:8080\")\n\n    svms = await client.call_tool(\"get_svms\", {\n        \"fields\": [\"name\", \"state\", \"cluster\", \"protocols\", \"subtype\"]\n    })\n\n    print(\"Storage Virtual Machines:\")\n    for svm in svms[\"svms\"]:\n        protocols = \", \".join(svm.get(\"protocols\", []))\n        print(f\"  - {svm['name']} ({svm['cluster']['name']}): {svm['state']} | Protocols: {protocols}\")\n\nasyncio.run(list_svms())\n</code></pre>"},{"location":"examples/basic-usage/#monitoring-and-alerting","title":"Monitoring and Alerting","text":""},{"location":"examples/basic-usage/#5-check-for-critical-events","title":"5. Check for Critical Events","text":""},{"location":"examples/basic-usage/#natural-language_2","title":"Natural Language","text":"<pre><code>\"Are there any critical alerts I should know about?\"\n</code></pre>"},{"location":"examples/basic-usage/#programmatic_3","title":"Programmatic","text":"<pre><code>async def check_critical_events():\n    client = MCPClient(\"http://localhost:8080\")\n\n    events = await client.call_tool(\"get_events\", {\n        \"severity\": \"critical\",\n        \"state\": \"new\",\n        \"max_records\": 10\n    })\n\n    if not events[\"events\"]:\n        print(\"\u2705 No critical events found!\")\n        return\n\n    print(\"\ud83d\udea8 Critical Events:\")\n    for event in events[\"events\"]:\n        print(f\"  - {event['name']}: {event['message']}\")\n        print(f\"    Source: {event['source']['name']} | Time: {event['time']}\")\n        print()\n\nasyncio.run(check_critical_events())\n</code></pre>"},{"location":"examples/basic-usage/#6-storage-capacity-monitoring","title":"6. Storage Capacity Monitoring","text":""},{"location":"examples/basic-usage/#natural-language_3","title":"Natural Language","text":"<pre><code>\"Show me volumes that are running low on space (over 80% full)\"\n</code></pre>"},{"location":"examples/basic-usage/#programmatic_4","title":"Programmatic","text":"<pre><code>async def check_low_space():\n    client = MCPClient(\"http://localhost:8080\")\n\n    low_space_volumes = await client.call_tool(\"get_volumes\", {\n        \"utilization_threshold\": 80,\n        \"fields\": [\"name\", \"svm\", \"cluster\", \"used_percentage\", \"size\", \"available\"],\n        \"order_by\": \"used_percentage\"\n    })\n\n    if not low_space_volumes[\"volumes\"]:\n        print(\"\u2705 No volumes with low space!\")\n        return\n\n    print(\"\u26a0\ufe0f  Volumes with Low Available Space (&gt;80% used):\")\n    print(f\"{'Volume':&lt;20} {'SVM':&lt;15} {'Used %':&lt;8} {'Available':&lt;12}\")\n    print(\"-\" * 60)\n\n    for vol in low_space_volumes[\"volumes\"]:\n        avail_gb = vol[\"size\"][\"available\"] / (1024**3)\n        print(f\"{vol['name']:&lt;20} {vol['svm']['name']:&lt;15} {vol['used_percentage']:&lt;8.1f}% {avail_gb:&lt;12.1f} GB\")\n\nasyncio.run(check_low_space())\n</code></pre>"},{"location":"examples/basic-usage/#basic-management-operations","title":"Basic Management Operations","text":""},{"location":"examples/basic-usage/#7-create-a-storage-virtual-machine","title":"7. Create a Storage Virtual Machine","text":""},{"location":"examples/basic-usage/#natural-language_4","title":"Natural Language","text":"<pre><code>\"Create a new SVM called 'dev-environment' on the production cluster with NFS protocol enabled\"\n</code></pre>"},{"location":"examples/basic-usage/#programmatic_5","title":"Programmatic","text":"<pre><code>async def create_development_svm():\n    client = MCPClient(\"http://localhost:8080\")\n\n    # First, get available clusters\n    clusters = await client.call_tool(\"get_clusters\", {\n        \"fields\": [\"name\", \"key\", \"state\"]\n    })\n\n    # Find production cluster\n    prod_cluster = None\n    for cluster in clusters[\"clusters\"]:\n        if \"prod\" in cluster[\"name\"].lower():\n            prod_cluster = cluster[\"name\"]\n            break\n\n    if not prod_cluster:\n        print(\"\u274c No production cluster found\")\n        return\n\n    # Create SVM\n    result = await client.call_tool(\"create_svm\", {\n        \"name\": \"dev-environment\",\n        \"cluster_name\": prod_cluster,\n        \"protocols\": [\"nfs\"],\n        \"security_style\": \"unix\",\n        \"language\": \"c.utf_8\"\n    })\n\n    if result[\"success\"]:\n        print(f\"\u2705 SVM 'dev-environment' created successfully!\")\n        print(f\"   Job ID: {result.get('job_id', 'N/A')}\")\n    else:\n        print(f\"\u274c Failed to create SVM: {result['error']['message']}\")\n\nasyncio.run(create_development_svm())\n</code></pre>"},{"location":"examples/basic-usage/#8-health-check-dashboard","title":"8. Health Check Dashboard","text":""},{"location":"examples/basic-usage/#complete-health-check-script","title":"Complete Health Check Script","text":"<pre><code>import asyncio\nfrom datetime import datetime\nfrom mcp_client import MCPClient\n\nasync def comprehensive_health_check():\n    \"\"\"\n    Comprehensive health check that combines multiple monitoring aspects\n    \"\"\"\n    client = MCPClient(\"http://localhost:8080\")\n\n    print(\"\ud83d\udd0d NetApp Infrastructure Health Check\")\n    print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n    print(\"=\" * 60)\n\n    # 1. Test connection\n    print(\"\\n1. Testing NetApp Connection...\")\n    connection = await client.call_tool(\"test_connection\", {})\n    if connection[\"success\"]:\n        print(\"\u2705 NetApp connection: OK\")\n    else:\n        print(\"\u274c NetApp connection: FAILED\")\n        return\n\n    # 2. Check cluster status\n    print(\"\\n2. Cluster Status...\")\n    clusters = await client.call_tool(\"get_clusters\", {})\n    healthy_clusters = 0\n    for cluster in clusters[\"clusters\"]:\n        status = \"\u2705\" if cluster[\"state\"] == \"up\" else \"\u274c\"\n        print(f\"   {status} {cluster['name']}: {cluster['state']}\")\n        if cluster[\"state\"] == \"up\":\n            healthy_clusters += 1\n\n    print(f\"   Summary: {healthy_clusters}/{len(clusters['clusters'])} clusters healthy\")\n\n    # 3. Check for critical events\n    print(\"\\n3. Critical Events...\")\n    events = await client.call_tool(\"get_events\", {\n        \"severity\": \"critical\",\n        \"state\": \"new\",\n        \"max_records\": 5\n    })\n\n    if not events[\"events\"]:\n        print(\"   \u2705 No critical events\")\n    else:\n        print(f\"   \u26a0\ufe0f  {len(events['events'])} critical events found:\")\n        for event in events[\"events\"][:3]:  # Show first 3\n            print(f\"      - {event['name']}\")\n\n    # 4. Check storage capacity\n    print(\"\\n4. Storage Capacity...\")\n    volumes = await client.call_tool(\"get_volumes\", {\n        \"utilization_threshold\": 85,\n        \"fields\": [\"name\", \"used_percentage\"],\n        \"max_records\": 5\n    })\n\n    if not volumes[\"volumes\"]:\n        print(\"   \u2705 No volumes over 85% capacity\")\n    else:\n        print(f\"   \u26a0\ufe0f  {len(volumes['volumes'])} volumes over 85% capacity:\")\n        for vol in volumes[\"volumes\"]:\n            print(f\"      - {vol['name']}: {vol['used_percentage']:.1f}%\")\n\n    # 5. SVM Status\n    print(\"\\n5. SVM Status...\")\n    svms = await client.call_tool(\"get_svms\", {})\n    running_svms = sum(1 for svm in svms[\"svms\"] if svm[\"state\"] == \"running\")\n    print(f\"   \u2705 {running_svms}/{len(svms['svms'])} SVMs running\")\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Health check completed!\")\n\n# Run the health check\nif __name__ == \"__main__\":\n    asyncio.run(comprehensive_health_check())\n</code></pre>"},{"location":"examples/basic-usage/#ai-assistant-integration-examples","title":"AI Assistant Integration Examples","text":""},{"location":"examples/basic-usage/#9-natural-language-queries","title":"9. Natural Language Queries","text":"<p>Here are examples of natural language queries you can use with AI assistants:</p>"},{"location":"examples/basic-usage/#infrastructure-discovery","title":"Infrastructure Discovery","text":"<ul> <li>\"What NetApp clusters do we have and what versions are they running?\"</li> <li>\"Show me all the storage virtual machines and their protocols\"</li> <li>\"List all volumes larger than 1TB\"</li> </ul>"},{"location":"examples/basic-usage/#monitoring","title":"Monitoring","text":"<ul> <li>\"Are there any critical alerts right now?\"</li> <li>\"Which volumes are running low on space?\"</li> <li>\"Show me the performance metrics for our production cluster\"</li> </ul>"},{"location":"examples/basic-usage/#management","title":"Management","text":"<ul> <li>\"Create a new SVM for the development team with NFS support\"</li> <li>\"Show me the details of cluster 'prod-cluster-01'\"</li> <li>\"What aggregates are available on the production cluster?\"</li> </ul>"},{"location":"examples/basic-usage/#analysis","title":"Analysis","text":"<ul> <li>\"Which SVMs are using the most storage?\"</li> <li>\"Show me all offline volumes\"</li> <li>\"What events happened in the last 24 hours?\"</li> </ul>"},{"location":"examples/basic-usage/#10-error-handling-examples","title":"10. Error Handling Examples","text":"<pre><code>async def robust_volume_check():\n    \"\"\"\n    Example of proper error handling with MCP tools\n    \"\"\"\n    client = MCPClient(\"http://localhost:8080\")\n\n    try:\n        # Attempt to get volume information\n        volumes = await client.call_tool(\"get_volumes\", {\n            \"max_records\": 10\n        })\n\n        if not volumes[\"success\"]:\n            print(f\"API Error: {volumes['error']['message']}\")\n            return\n\n        print(f\"Successfully retrieved {len(volumes['volumes'])} volumes\")\n\n    except ConnectionError:\n        print(\"\u274c Cannot connect to MCP server\")\n    except TimeoutError:\n        print(\"\u274c Request timed out\")\n    except Exception as e:\n        print(f\"\u274c Unexpected error: {e}\")\n\nasyncio.run(robust_volume_check())\n</code></pre>"},{"location":"examples/basic-usage/#configuration-examples","title":"Configuration Examples","text":""},{"location":"examples/basic-usage/#11-mcp-client-configuration","title":"11. MCP Client Configuration","text":""},{"location":"examples/basic-usage/#for-claude-desktop","title":"For Claude Desktop","text":"<pre><code>{\n  \"mcpServers\": {\n    \"netapp-activeiq\": {\n      \"command\": \"docker\",\n      \"args\": [\"exec\", \"-i\", \"netapp-mcp-server\", \"python\", \"-m\", \"netapp_mcp_server\", \"--stdio\"],\n      \"env\": {\n        \"NETAPP_UM_HOST\": \"your-unified-manager.company.com\",\n        \"NETAPP_USERNAME\": \"serviceaccount\",\n        \"NETAPP_PASSWORD\": \"your-secure-password\"\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"examples/basic-usage/#for-custom-python-client","title":"For Custom Python Client","text":"<pre><code>from mcp_client import MCPClient\n\n# Initialize client\nclient = MCPClient(\n    server_url=\"http://localhost:8080\",\n    timeout=30,\n    retry_attempts=3\n)\n\n# Use client\nasync def main():\n    clusters = await client.call_tool(\"get_clusters\", {})\n    print(clusters)\n\nasyncio.run(main())\n</code></pre>"},{"location":"examples/basic-usage/#useful-patterns","title":"Useful Patterns","text":""},{"location":"examples/basic-usage/#12-periodic-monitoring","title":"12. Periodic Monitoring","text":"<pre><code>import asyncio\nimport time\n\nasync def periodic_health_monitor(interval_seconds=300):\n    \"\"\"\n    Run health checks every 5 minutes\n    \"\"\"\n    client = MCPClient(\"http://localhost:8080\")\n\n    while True:\n        try:\n            print(f\"\\n\ud83d\udd0d Health Check - {datetime.now()}\")\n\n            # Quick health check\n            events = await client.call_tool(\"get_events\", {\n                \"severity\": \"critical\",\n                \"state\": \"new\"\n            })\n\n            if events[\"events\"]:\n                print(f\"\u26a0\ufe0f  {len(events['events'])} new critical events!\")\n                # Could send notifications here\n            else:\n                print(\"\u2705 No critical events\")\n\n        except Exception as e:\n            print(f\"\u274c Health check failed: {e}\")\n\n        await asyncio.sleep(interval_seconds)\n\n# Run continuous monitoring\n# asyncio.run(periodic_health_monitor())\n</code></pre>"},{"location":"examples/basic-usage/#13-batch-operations","title":"13. Batch Operations","text":"<pre><code>async def batch_cluster_info():\n    \"\"\"\n    Gather comprehensive cluster information in batch\n    \"\"\"\n    client = MCPClient(\"http://localhost:8080\")\n\n    # Run multiple queries concurrently\n    tasks = [\n        client.call_tool(\"get_clusters\", {}),\n        client.call_tool(\"get_svms\", {}),\n        client.call_tool(\"get_events\", {\"severity\": \"critical\"}),\n        client.call_tool(\"get_volumes\", {\"utilization_threshold\": 80})\n    ]\n\n    results = await asyncio.gather(*tasks, return_exceptions=True)\n\n    clusters, svms, events, low_space = results\n\n    # Process results\n    print(\"Comprehensive Infrastructure Report\")\n    print(f\"Clusters: {len(clusters['clusters']) if isinstance(clusters, dict) else 'Error'}\")\n    print(f\"SVMs: {len(svms['svms']) if isinstance(svms, dict) else 'Error'}\")\n    print(f\"Critical Events: {len(events['events']) if isinstance(events, dict) else 'Error'}\")\n    print(f\"Low Space Volumes: {len(low_space['volumes']) if isinstance(low_space, dict) else 'Error'}\")\n\nasyncio.run(batch_cluster_info())\n</code></pre>"},{"location":"examples/basic-usage/#next-steps","title":"Next Steps","text":"<p>Now that you understand the basics:</p> <ol> <li>Advanced Scenarios - Complex workflows and automation</li> <li>Architecture - Understanding the system design</li> <li>API Reference - Complete tool documentation</li> <li>Deployment - Production deployment guide</li> </ol>"},{"location":"examples/basic-usage/#common-issues","title":"Common Issues","text":""},{"location":"examples/basic-usage/#connection-problems","title":"Connection Problems","text":"<ul> <li>Verify NetApp credentials are correct</li> <li>Check network connectivity to Unified Manager</li> <li>Ensure MCP server is running and accessible</li> </ul>"},{"location":"examples/basic-usage/#authentication-issues","title":"Authentication Issues","text":"<ul> <li>Verify user has required NetApp roles</li> <li>Check password hasn't expired</li> <li>Ensure service account has proper permissions</li> </ul>"},{"location":"examples/basic-usage/#performance-issues","title":"Performance Issues","text":"<ul> <li>Use field filtering to reduce response size</li> <li>Implement caching for frequently accessed data</li> <li>Use appropriate max_records limits</li> </ul>"},{"location":"getting-started/installation/","title":"Installation Guide","text":"<p>Complete installation guide for the NetApp ActiveIQ MCP Server across different environments.</p>"},{"location":"getting-started/installation/#system-requirements","title":"System Requirements","text":""},{"location":"getting-started/installation/#minimum-requirements","title":"Minimum Requirements","text":"<ul> <li>CPU: 2 cores</li> <li>Memory: 4GB RAM</li> <li>Storage: 10GB available space</li> <li>Network: Access to NetApp ActiveIQ Unified Manager</li> <li>OS: Linux, macOS, or Windows (with Docker)</li> </ul>"},{"location":"getting-started/installation/#recommended-requirements","title":"Recommended Requirements","text":"<ul> <li>CPU: 4+ cores</li> <li>Memory: 8GB+ RAM</li> <li>Storage: 50GB+ available space</li> <li>Network: Dedicated network access to NetApp infrastructure</li> </ul>"},{"location":"getting-started/installation/#software-prerequisites","title":"Software Prerequisites","text":"<ul> <li>Docker 20.10+ or Python 3.10+</li> <li>Git (for source installation)</li> <li>curl (for testing)</li> </ul>"},{"location":"getting-started/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"getting-started/installation/#method-1-docker-installation-recommended","title":"Method 1: Docker Installation (Recommended)","text":""},{"location":"getting-started/installation/#pull-pre-built-image","title":"Pull Pre-built Image","text":"<pre><code># Pull the latest stable version\ndocker pull netapp/activeiq-mcp-server:latest\n\n# Or pull a specific version\ndocker pull netapp/activeiq-mcp-server:v1.2.0\n</code></pre>"},{"location":"getting-started/installation/#build-from-source","title":"Build from Source","text":"<pre><code># Clone repository\ngit clone https://github.com/netapp/activeiq-mcp-server.git\ncd activeiq-mcp-server\n\n# Build Docker image\ndocker build -t netapp/activeiq-mcp-server:local .\n</code></pre>"},{"location":"getting-started/installation/#method-2-python-installation","title":"Method 2: Python Installation","text":""},{"location":"getting-started/installation/#from-pypi","title":"From PyPI","text":"<pre><code># Install from PyPI (when available)\npip install netapp-activeiq-mcp-server\n</code></pre>"},{"location":"getting-started/installation/#from-source","title":"From Source","text":"<pre><code># Clone repository\ngit clone https://github.com/netapp/activeiq-mcp-server.git\ncd activeiq-mcp-server\n\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install dependencies\npip install -r requirements.txt\n\n# Install in development mode\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#method-3-kubernetes-installation","title":"Method 3: Kubernetes Installation","text":""},{"location":"getting-started/installation/#using-helm","title":"Using Helm","text":"<pre><code># Add NetApp Helm repository\nhelm repo add netapp https://netapp.github.io/activeiq-mcp-server\nhelm repo update\n\n# Install with default values\nhelm install netapp-mcp-server netapp/activeiq-mcp-server\n\n# Install with custom values\nhelm install netapp-mcp-server netapp/activeiq-mcp-server \\\n  --set netapp.umHost=your-um-host.company.com \\\n  --set netapp.username=admin \\\n  --set netapp.password=your-password\n</code></pre>"},{"location":"getting-started/installation/#using-kubectl","title":"Using kubectl","text":"<pre><code># Apply Kubernetes manifests\nkubectl apply -f https://raw.githubusercontent.com/netapp/activeiq-mcp-server/main/deploy/kubernetes/\n</code></pre>"},{"location":"getting-started/installation/#method-4-knative-functions-serverless","title":"Method 4: Knative Functions (Serverless)","text":""},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<pre><code># Install Knative CLI\ncurl -L https://github.com/knative/client/releases/latest/download/kn-linux-amd64 -o kn\nchmod +x kn &amp;&amp; sudo mv kn /usr/local/bin/\n\n# Install Knative func CLI\ncurl -L https://github.com/knative/func/releases/latest/download/func_linux_amd64.tar.gz | tar xz\nchmod +x func &amp;&amp; sudo mv func /usr/local/bin/\n\n# Verify installation\nkn version\nfunc version\n</code></pre>"},{"location":"getting-started/installation/#option-a-deploy-complete-mcp-server-as-function","title":"Option A: Deploy Complete MCP Server as Function","text":"<pre><code># Create MCP server function\nfunc create netapp-mcp-server --language python --template http\ncd netapp-mcp-server\n\n# Configure func.yaml\ncat &gt; func.yaml &lt;&lt; 'EOF'\nspecVersion: 0.35.0\nname: netapp-mcp-server\nruntime: python\nregistry: your-registry.com/netapp\nimage: your-registry.com/netapp/netapp-mcp-server:latest\ncreated: $(date -u +\"%Y-%m-%dT%H:%M:%SZ\")\ninvoke: gunicorn\nbuild:\n  builder: pack\n  buildpacks:\n    - gcr.io/paketo-buildpacks/python\nrun:\n  env:\n    - name: FUNCTION_TARGET\n      value: main\ndeploy:\n  namespace: netapp-mcp\n  options:\n    scale:\n      min: 0\n      max: 10\n      metric: concurrency\n      target: 100\n    resources:\n      requests:\n        cpu: 500m\n        memory: 512Mi\n      limits:\n        cpu: 2000m\n        memory: 1Gi\n    annotations:\n      autoscaling.knative.dev/scaleDownDelay: \"60s\"\n      autoscaling.knative.dev/scaleUpDelay: \"0s\"\nEOF\n\n# Create requirements.txt\ncat &gt; requirements.txt &lt;&lt; 'EOF'\nfastmcp&gt;=0.5.0\nhttpx&gt;=0.25.0\npydantic&gt;=2.0.0\npython-dotenv&gt;=1.0.0\nuvicorn&gt;=0.24.0\nparliament-functions&gt;=1.0.0\nEOF\n\n# Copy MCP server code\ncp ../mcp_server.py func.py\n\n# Deploy with environment variables\nfunc deploy \\\n  --namespace netapp-mcp \\\n  --env NETAPP_BASE_URL=\"https://your-netapp-aiqum.example.com/api\" \\\n  --env NETAPP_USERNAME=\"your-username\" \\\n  --env NETAPP_PASSWORD=\"your-password\" \\\n  --env NETAPP_VERIFY_SSL=\"false\" \\\n  --env MCP_SERVER_PORT=\"8080\" \\\n  --env MCP_LOG_LEVEL=\"INFO\" \\\n  --build\n</code></pre>"},{"location":"getting-started/installation/#option-b-deploy-individual-function-components","title":"Option B: Deploy Individual Function Components","text":"<pre><code># Set environment variables\nexport NETAPP_BASE_URL=\"https://your-netapp-aiqum.example.com/api\"\nexport NETAPP_USERNAME=\"your-username\"\nexport NETAPP_PASSWORD=\"your-password\"\nexport REGISTRY=\"your-registry.com/netapp\"\n\n# Deploy using the provided script\n./scripts/deploy-knative-functions.sh\n</code></pre>"},{"location":"getting-started/installation/#managing-knative-functions","title":"Managing Knative Functions","text":"<pre><code># List functions\nkn service list -n netapp-mcp\n\n# Get function URL\nMCP_URL=$(kn service describe netapp-mcp-server -n netapp-mcp -o jsonpath='{.status.url}')\necho \"MCP Server URL: $MCP_URL\"\n\n# View function logs\nkn func logs netapp-mcp-server --follow\n\n# Update function configuration\nkn service update netapp-mcp-server \\\n  --env MCP_LOG_LEVEL=DEBUG \\\n  --scale-min 1 \\\n  --scale-max 20 \\\n  -n netapp-mcp\n\n# Test function\ncurl -X POST $MCP_URL/tools/get_clusters \\\n  -H \"Content-Type: application/json\" \\\n  -d '{}'\n</code></pre>"},{"location":"getting-started/installation/#traffic-management","title":"Traffic Management","text":"<pre><code># Deploy new version without traffic\nfunc deploy --tag v2 --no-traffic\n\n# Split traffic between versions\nkn service update netapp-mcp-server \\\n  --traffic v1=90,v2=10 \\\n  -n netapp-mcp\n\n# Promote new version\nkn service update netapp-mcp-server \\\n  --traffic v2=100 \\\n  -n netapp-mcp\n</code></pre>"},{"location":"getting-started/installation/#configuration","title":"Configuration","text":""},{"location":"getting-started/installation/#environment-variables","title":"Environment Variables","text":"<p>Create a configuration file with your environment settings:</p> <pre><code># .env file\n# NetApp ActiveIQ Configuration\nNETAPP_UM_HOST=unified-manager.company.com\nNETAPP_USERNAME=serviceaccount\nNETAPP_PASSWORD=secure-password\nNETAPP_VERIFY_SSL=false\n\n# MCP Server Configuration\nMCP_SERVER_PORT=8080\nMCP_SERVER_HOST=0.0.0.0\nMCP_LOG_LEVEL=INFO\n\n# Optional: Advanced Settings\nMCP_TIMEOUT=30\nMCP_MAX_CONNECTIONS=100\nMCP_CACHE_TTL=300\n\n# Optional: Temporal Integration\nTEMPORAL_HOST=temporal-server:7233\nTEMPORAL_NAMESPACE=netapp-workflows\nTEMPORAL_TASK_QUEUE=netapp-tasks\n\n# Optional: Prometheus Metrics\nPROMETHEUS_ENABLED=true\nPROMETHEUS_PORT=9090\n\n# Optional: Authentication\nAUTH_ENABLED=false\nAUTH_SECRET_KEY=your-secret-key\nAUTH_ALGORITHM=HS256\n</code></pre>"},{"location":"getting-started/installation/#configuration-file","title":"Configuration File","text":"<p>Alternatively, use a YAML configuration file:</p> <pre><code># config.yaml\nnetapp:\n  um_host: \"unified-manager.company.com\"\n  username: \"serviceaccount\"\n  password: \"secure-password\"\n  verify_ssl: false\n  timeout: 30\n\nserver:\n  host: \"0.0.0.0\"\n  port: 8080\n  log_level: \"INFO\"\n  max_connections: 100\n\ncache:\n  enabled: true\n  ttl: 300\n  redis_url: \"redis://localhost:6379\"\n\ntemporal:\n  enabled: false\n  host: \"localhost:7233\"\n  namespace: \"default\"\n  task_queue: \"netapp-tasks\"\n\nmonitoring:\n  prometheus:\n    enabled: true\n    port: 9090\n    path: \"/metrics\"\n\nsecurity:\n  auth_enabled: false\n  secret_key: \"your-secret-key\"\n  ssl_cert: \"/path/to/cert.pem\"\n  ssl_key: \"/path/to/key.pem\"\n</code></pre>"},{"location":"getting-started/installation/#running-the-server","title":"Running the Server","text":""},{"location":"getting-started/installation/#docker-deployment","title":"Docker Deployment","text":""},{"location":"getting-started/installation/#basic-docker-run","title":"Basic Docker Run","text":"<pre><code>docker run -d \\\n  --name netapp-mcp-server \\\n  --env-file .env \\\n  -p 8080:8080 \\\n  netapp/activeiq-mcp-server:latest\n</code></pre>"},{"location":"getting-started/installation/#docker-compose","title":"Docker Compose","text":"<pre><code># docker-compose.yml\nversion: '3.8'\n\nservices:\n  netapp-mcp-server:\n    image: netapp/activeiq-mcp-server:latest\n    container_name: netapp-mcp-server\n    restart: unless-stopped\n    ports:\n      - \"8080:8080\"\n      - \"9090:9090\"  # Prometheus metrics\n    env_file:\n      - .env\n    volumes:\n      - ./config.yaml:/app/config.yaml:ro\n      - ./logs:/app/logs\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8080/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n\n  # Optional: Redis for caching\n  redis:\n    image: redis:7-alpine\n    container_name: netapp-redis\n    restart: unless-stopped\n    ports:\n      - \"6379:6379\"\n    volumes:\n      - redis_data:/data\n\n  # Optional: Temporal server\n  temporal:\n    image: temporalio/auto-setup:latest\n    container_name: netapp-temporal\n    restart: unless-stopped\n    ports:\n      - \"7233:7233\"\n      - \"8088:8080\"  # Temporal Web UI\n    environment:\n      - DB=postgresql\n      - DB_PORT=5432\n      - POSTGRES_USER=temporal\n      - POSTGRES_PWD=temporal\n      - POSTGRES_SEEDS=postgres\n\nvolumes:\n  redis_data:\n</code></pre>"},{"location":"getting-started/installation/#python-deployment","title":"Python Deployment","text":""},{"location":"getting-started/installation/#direct-execution","title":"Direct Execution","text":"<pre><code># Using environment variables\nexport NETAPP_UM_HOST=your-um-host.company.com\nexport NETAPP_USERNAME=admin\nexport NETAPP_PASSWORD=your-password\n\n# Run the server\npython -m netapp_mcp_server\n\n# Or with config file\npython -m netapp_mcp_server --config config.yaml\n</code></pre>"},{"location":"getting-started/installation/#using-systemd-linux","title":"Using systemd (Linux)","text":"<pre><code># /etc/systemd/system/netapp-mcp-server.service\n[Unit]\nDescription=NetApp ActiveIQ MCP Server\nAfter=network.target\n\n[Service]\nType=simple\nUser=netapp\nGroup=netapp\nWorkingDirectory=/opt/netapp-mcp-server\nEnvironment=PATH=/opt/netapp-mcp-server/venv/bin\nEnvironmentFile=/opt/netapp-mcp-server/.env\nExecStart=/opt/netapp-mcp-server/venv/bin/python -m netapp_mcp_server\nRestart=always\nRestartSec=3\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <pre><code># Enable and start service\nsudo systemctl enable netapp-mcp-server\nsudo systemctl start netapp-mcp-server\nsudo systemctl status netapp-mcp-server\n</code></pre>"},{"location":"getting-started/installation/#verification","title":"Verification","text":""},{"location":"getting-started/installation/#health-check","title":"Health Check","text":"<pre><code># Check server health\ncurl http://localhost:8080/health\n\n# Expected response:\n{\n  \"status\": \"healthy\",\n  \"version\": \"1.2.0\",\n  \"netapp_connection\": \"ok\",\n  \"uptime\": \"2h 15m 30s\"\n}\n</code></pre>"},{"location":"getting-started/installation/#mcp-tools-test","title":"MCP Tools Test","text":"<pre><code># List available tools\ncurl http://localhost:8080/tools\n\n# Test a simple tool\ncurl -X POST http://localhost:8080/tools/get_clusters \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"arguments\": {}}'\n</code></pre>"},{"location":"getting-started/installation/#connection-test","title":"Connection Test","text":"<pre><code># Test NetApp connection\ncurl -X POST http://localhost:8080/tools/test_connection \\\n  -H \"Content-Type: application/json\"\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":""},{"location":"getting-started/installation/#cannot-connect-to-netapp-unified-manager","title":"Cannot Connect to NetApp Unified Manager","text":"<pre><code># Test network connectivity\ncurl -k https://your-um-host.company.com/api/v2/datacenter/cluster/clusters\n\n# Check DNS resolution\nnslookup your-um-host.company.com\n\n# Test with explicit credentials\ncurl -k -u \"username:password\" https://your-um-host.company.com/api/v2/datacenter/cluster/clusters\n</code></pre>"},{"location":"getting-started/installation/#server-wont-start","title":"Server Won't Start","text":"<pre><code># Check logs (Docker)\ndocker logs netapp-mcp-server\n\n# Check logs (Python)\ntail -f logs/netapp-mcp-server.log\n\n# Check configuration\npython -m netapp_mcp_server --validate-config\n</code></pre>"},{"location":"getting-started/installation/#permission-issues","title":"Permission Issues","text":"<pre><code># Check file permissions\nls -la config.yaml .env\n\n# Fix permissions\nchmod 600 .env config.yaml\n</code></pre>"},{"location":"getting-started/installation/#log-locations","title":"Log Locations","text":"<ul> <li>Docker: <code>docker logs netapp-mcp-server</code></li> <li>Python: <code>./logs/netapp-mcp-server.log</code></li> <li>systemd: <code>journalctl -u netapp-mcp-server</code></li> </ul>"},{"location":"getting-started/installation/#security-considerations","title":"Security Considerations","text":""},{"location":"getting-started/installation/#credential-management","title":"Credential Management","text":"<ul> <li>Store credentials in secure environment variables or files</li> <li>Use dedicated service accounts with minimal required permissions</li> <li>Rotate passwords regularly</li> <li>Consider using secret management solutions (HashiCorp Vault, Kubernetes Secrets)</li> </ul>"},{"location":"getting-started/installation/#network-security","title":"Network Security","text":"<ul> <li>Use HTTPS for all NetApp API communications</li> <li>Implement proper firewall rules</li> <li>Consider VPN or private network access</li> <li>Enable SSL/TLS for the MCP server</li> </ul>"},{"location":"getting-started/installation/#access-control","title":"Access Control","text":"<ul> <li>Implement authentication for the MCP server</li> <li>Use role-based access control</li> <li>Monitor and audit access logs</li> <li>Implement rate limiting</li> </ul>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>After successful installation:</p> <ol> <li>Configure the Server - Detailed configuration options</li> <li>Explore Architecture - Understand the system design</li> <li>Try Examples - Start with basic examples</li> <li>Deploy to Production - Production deployment guide</li> </ol>"},{"location":"getting-started/knative-functions-quickstart/","title":"Quick Start: NetApp MCP Server with Knative Functions","text":""},{"location":"getting-started/knative-functions-quickstart/#overview","title":"Overview","text":"<p>This guide provides a fast path to deploy NetApp ActiveIQ MCP Server operations as individual Knative functions, enabling serverless, auto-scaling storage management for AI assistants.</p>"},{"location":"getting-started/knative-functions-quickstart/#quick-start-5-minutes","title":"\ud83d\ude80 Quick Start (5 minutes)","text":""},{"location":"getting-started/knative-functions-quickstart/#prerequisites","title":"Prerequisites","text":"<pre><code># Install required CLI tools\n# Knative CLI\ncurl -L https://github.com/knative/client/releases/latest/download/kn-linux-amd64 -o kn\nchmod +x kn &amp;&amp; sudo mv kn /usr/local/bin/\n\n# Knative func CLI\ncurl -L https://github.com/knative/func/releases/latest/download/func_linux_amd64.tar.gz | tar xz\nchmod +x func &amp;&amp; sudo mv func /usr/local/bin/\n\n# Verify installation\nkn version\nfunc version\nkubectl version --client\n</code></pre>"},{"location":"getting-started/knative-functions-quickstart/#deploy-functions","title":"Deploy Functions","text":"<pre><code># Set environment variables\nexport NETAPP_BASE_URL=\"https://your-netapp-aiqum.example.com/api\"\nexport NETAPP_USERNAME=\"your-username\"\nexport NETAPP_PASSWORD=\"your-password\"\nexport REGISTRY=\"your-registry.com/netapp\"\n\n# Deploy all functions\ncd /path/to/netapp\n./scripts/deploy-knative-functions.sh\n</code></pre>"},{"location":"getting-started/knative-functions-quickstart/#test-functions","title":"Test Functions","text":"<pre><code># Test storage monitor function\nkn func invoke netapp-storage-monitor \\\n  --data '{\"operation\": \"get_clusters\"}'\n\n# Test volume operations function\nkn func invoke netapp-volume-ops \\\n  --data '{\"operation\": \"create_volume\", \"volume_config\": {\"name\": \"test_vol\"}}'\n\n# Test SVM manager function\nkn func invoke netapp-svm-manager \\\n  --data '{\"operation\": \"get_svms\"}'\n</code></pre>"},{"location":"getting-started/knative-functions-quickstart/#architecture-overview","title":"\ud83c\udfd7\ufe0f Architecture Overview","text":""},{"location":"getting-started/knative-functions-quickstart/#function-decomposition","title":"Function Decomposition","text":"Function Purpose MCP Tools Scaling <code>netapp-storage-monitor</code> Real-time monitoring <code>get_clusters</code>, <code>get_aggregates</code>, <code>get_volumes</code> High frequency <code>netapp-volume-ops</code> Volume lifecycle <code>create_volume</code>, <code>delete_volume</code>, <code>modify_volume</code> On-demand <code>netapp-svm-manager</code> SVM operations <code>create_svm</code>, <code>get_svms</code>, <code>configure_svm</code> Periodic"},{"location":"getting-started/knative-functions-quickstart/#benefits","title":"Benefits","text":"<ul> <li>\ud83c\udf1f Scale to Zero: No idle resource consumption</li> <li>\u26a1 Instant Scaling: Sub-second response to demand</li> <li>\ud83d\udcb0 Cost Optimized: 90% cost reduction vs traditional deployment</li> <li>\ud83d\udd27 DevOps Friendly: Simple <code>kn</code> CLI deployment</li> <li>\ud83e\udd16 AI Ready: Optimized for AI assistant consumption</li> </ul>"},{"location":"getting-started/knative-functions-quickstart/#detailed-setup","title":"\ud83d\udcd6 Detailed Setup","text":""},{"location":"getting-started/knative-functions-quickstart/#1-environment-setup","title":"1. Environment Setup","text":"<pre><code># Create project directory\nmkdir netapp-functions &amp;&amp; cd netapp-functions\n\n# Set configuration\nexport NAMESPACE=\"netapp-functions\"\nexport REGISTRY=\"your-registry.com/netapp\"\nexport NETAPP_BASE_URL=\"https://your-netapp-aiqum.example.com/api\"\nexport NETAPP_USERNAME=\"your-api-username\"\nexport NETAPP_PASSWORD=\"your-api-password\"\n</code></pre>"},{"location":"getting-started/knative-functions-quickstart/#2-manual-function-creation","title":"2. Manual Function Creation","text":""},{"location":"getting-started/knative-functions-quickstart/#storage-monitor-function","title":"Storage Monitor Function","text":"<pre><code># Create function\nkn func create netapp-storage-monitor --language python --template http\n\n# Navigate to function directory\ncd netapp-storage-monitor\n\n# Update func.yaml\ncat &gt; func.yaml &lt;&lt; 'EOF'\nspecVersion: 0.35.0\nname: netapp-storage-monitor\nruntime: python\nregistry: your-registry.com/netapp\nimage: your-registry.com/netapp/netapp-storage-monitor:latest\ndeploy:\n  namespace: netapp-functions\n  options:\n    scale:\n      min: 0\n      max: 10\n      target: 5\n    resources:\n      requests:\n        cpu: 200m\n        memory: 256Mi\n      limits:\n        cpu: 500m\n        memory: 512Mi\n    annotations:\n      autoscaling.knative.dev/scaleDownDelay: \"30s\"\nEOF\n\n# Deploy function\nkn func deploy --build --namespace netapp-functions\n</code></pre>"},{"location":"getting-started/knative-functions-quickstart/#volume-operations-function","title":"Volume Operations Function","text":"<pre><code># Create function\nkn func create netapp-volume-ops --language python --template http\n\n# Deploy with scaling configuration\nkn func deploy \\\n  --namespace netapp-functions \\\n  --env NETAPP_BASE_URL=\"${NETAPP_BASE_URL}\" \\\n  --env NETAPP_USERNAME=\"${NETAPP_USERNAME}\" \\\n  --env NETAPP_PASSWORD=\"${NETAPP_PASSWORD}\" \\\n  --scale-min 0 \\\n  --scale-max 20 \\\n  --scale-target 10 \\\n  --build\n</code></pre>"},{"location":"getting-started/knative-functions-quickstart/#svm-manager-function","title":"SVM Manager Function","text":"<pre><code># Create and deploy SVM manager\nkn func create netapp-svm-manager --language python --template http\nkn func deploy \\\n  --namespace netapp-functions \\\n  --env NETAPP_BASE_URL=\"${NETAPP_BASE_URL}\" \\\n  --env NETAPP_USERNAME=\"${NETAPP_USERNAME}\" \\\n  --env NETAPP_PASSWORD=\"${NETAPP_PASSWORD}\" \\\n  --scale-min 0 \\\n  --scale-max 5 \\\n  --build\n</code></pre>"},{"location":"getting-started/knative-functions-quickstart/#3-function-management","title":"3. Function Management","text":""},{"location":"getting-started/knative-functions-quickstart/#list-functions","title":"List Functions","text":"<pre><code># List all functions\nkn func list --namespace netapp-functions\nkn service list --namespace netapp-functions\n\n# Get function details\nkn func describe netapp-storage-monitor\nkn service describe netapp-storage-monitor --namespace netapp-functions\n</code></pre>"},{"location":"getting-started/knative-functions-quickstart/#monitor-functions","title":"Monitor Functions","text":"<pre><code># View function logs\nkn func logs netapp-storage-monitor --follow\nkubectl logs -l app=netapp-storage-monitor -n netapp-functions -f\n\n# Monitor scaling\nwatch -n 2 'kubectl get pods -n netapp-functions'\n\n# Check function URLs\nkn service list -n netapp-functions -o custom-columns=NAME:.metadata.name,URL:.status.url\n</code></pre>"},{"location":"getting-started/knative-functions-quickstart/#update-functions","title":"Update Functions","text":"<pre><code># Update environment variables\nkn service update netapp-storage-monitor \\\n  --env MONITORING_INTERVAL=60 \\\n  --env LOG_LEVEL=INFO \\\n  --namespace netapp-functions\n\n# Update scaling configuration\nkn service update netapp-volume-ops \\\n  --scale-min 1 \\\n  --scale-max 25 \\\n  --scale-target 15 \\\n  --namespace netapp-functions\n\n# Update resource limits\nkn service update netapp-svm-manager \\\n  --limit memory=1Gi \\\n  --limit cpu=1000m \\\n  --namespace netapp-functions\n</code></pre>"},{"location":"getting-started/knative-functions-quickstart/#testing-and-validation","title":"\ud83e\uddea Testing and Validation","text":""},{"location":"getting-started/knative-functions-quickstart/#function-testing","title":"Function Testing","text":"<pre><code># Test storage monitor\ncurl -X POST $(kn service describe netapp-storage-monitor -n netapp-functions -o jsonpath='{.status.url}') \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"operation\": \"get_clusters\"}'\n\n# Test volume operations\ncurl -X POST $(kn service describe netapp-volume-ops -n netapp-functions -o jsonpath='{.status.url}') \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"operation\": \"create_volume\",\n    \"volume_config\": {\n      \"name\": \"test_volume\",\n      \"size\": 1000000000,\n      \"svm\": \"test_svm\"\n    }\n  }'\n\n# Test SVM manager\ncurl -X POST $(kn service describe netapp-svm-manager -n netapp-functions -o jsonpath='{.status.url}') \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"operation\": \"get_svms\"}'\n</code></pre>"},{"location":"getting-started/knative-functions-quickstart/#load-testing","title":"Load Testing","text":"<pre><code># Install hey for load testing\ngo install github.com/rakyll/hey@latest\n\n# Load test storage monitor function\nSTORAGE_URL=$(kn service describe netapp-storage-monitor -n netapp-functions -o jsonpath='{.status.url}')\nhey -n 100 -c 10 -m POST \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"operation\": \"get_clusters\"}' \\\n  \"${STORAGE_URL}\"\n\n# Observe auto-scaling\nwatch -n 1 'kubectl get pods -n netapp-functions | grep netapp-storage-monitor'\n</code></pre>"},{"location":"getting-started/knative-functions-quickstart/#advanced-configuration","title":"\ud83d\udd27 Advanced Configuration","text":""},{"location":"getting-started/knative-functions-quickstart/#traffic-splitting","title":"Traffic Splitting","text":"<pre><code># Deploy new version with traffic split\nkn func deploy --tag v2 --traffic v1=90,v2=10\n\n# Gradually shift traffic\nkn service update netapp-storage-monitor \\\n  --traffic v1=70,v2=30 \\\n  --namespace netapp-functions\n\n# Full promotion\nkn service update netapp-storage-monitor \\\n  --traffic v2=100 \\\n  --namespace netapp-functions\n</code></pre>"},{"location":"getting-started/knative-functions-quickstart/#blue-green-deployment","title":"Blue-Green Deployment","text":"<pre><code># Deploy new version without traffic\nkn func deploy --tag blue --no-traffic\n\n# Test blue version\nBLUE_URL=$(kn service describe netapp-storage-monitor -n netapp-functions -o jsonpath='{.status.traffic[?(@.tag==\"blue\")].url}')\ncurl -X POST \"${BLUE_URL}\" -H \"Content-Type: application/json\" -d '{\"operation\": \"get_clusters\"}'\n\n# Switch traffic to blue\nkn service update netapp-storage-monitor \\\n  --traffic blue=100 \\\n  --namespace netapp-functions\n</code></pre>"},{"location":"getting-started/knative-functions-quickstart/#custom-scaling","title":"Custom Scaling","text":"<pre><code># CPU-based scaling\nkn service update netapp-volume-ops \\\n  --annotation autoscaling.knative.dev/metric=cpu \\\n  --annotation autoscaling.knative.dev/target=70 \\\n  --namespace netapp-functions\n\n# Concurrency-based scaling\nkn service update netapp-svm-manager \\\n  --annotation autoscaling.knative.dev/metric=concurrency \\\n  --annotation autoscaling.knative.dev/target=5 \\\n  --namespace netapp-functions\n\n# Custom scaling window\nkn service update netapp-storage-monitor \\\n  --annotation autoscaling.knative.dev/window=30s \\\n  --annotation autoscaling.knative.dev/scaleDownDelay=60s \\\n  --namespace netapp-functions\n</code></pre>"},{"location":"getting-started/knative-functions-quickstart/#monitoring-and-observability","title":"\ud83d\udcca Monitoring and Observability","text":""},{"location":"getting-started/knative-functions-quickstart/#metrics-collection","title":"Metrics Collection","text":"<pre><code># View function metrics\nkubectl get --raw \"/apis/metrics.k8s.io/v1beta1/namespaces/netapp-functions/pods\" | jq\n\n# Monitor requests per second\nkubectl get configmap -n knative-serving config-observability -o yaml\n\n# Check revision metrics\nkubectl get revisions.serving.knative.dev -n netapp-functions\n</code></pre>"},{"location":"getting-started/knative-functions-quickstart/#logging","title":"Logging","text":"<pre><code># Centralized logging\nkubectl logs -l serving.knative.dev/service=netapp-storage-monitor -n netapp-functions --tail=100\n\n# Function-specific logs\nkn func logs netapp-volume-ops --follow --namespace netapp-functions\n\n# Structured logging\nkubectl logs -l app=netapp-svm-manager -n netapp-functions -f | jq\n</code></pre>"},{"location":"getting-started/knative-functions-quickstart/#alerts","title":"Alerts","text":"<pre><code># Prometheus alerting rules\napiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  name: netapp-functions-alerts\n  namespace: netapp-functions\nspec:\n  groups:\n  - name: netapp-functions\n    rules:\n    - alert: NetAppFunctionHighErrorRate\n      expr: sum(rate(function_errors_total[5m])) / sum(rate(function_requests_total[5m])) &gt; 0.05\n      for: 5m\n      labels:\n        severity: warning\n      annotations:\n        summary: \"High error rate in NetApp functions\"\n\n    - alert: NetAppFunctionColdStartHigh\n      expr: histogram_quantile(0.95, function_cold_start_duration_seconds) &gt; 5\n      for: 10m\n      labels:\n        severity: warning\n      annotations:\n        summary: \"High cold start latency for NetApp functions\"\n</code></pre>"},{"location":"getting-started/knative-functions-quickstart/#security-and-compliance","title":"\ud83d\udd10 Security and Compliance","text":""},{"location":"getting-started/knative-functions-quickstart/#secrets-management","title":"Secrets Management","text":"<pre><code># Create secrets\nkubectl create secret generic netapp-credentials \\\n  --from-literal=NETAPP_BASE_URL=\"${NETAPP_BASE_URL}\" \\\n  --from-literal=NETAPP_USERNAME=\"${NETAPP_USERNAME}\" \\\n  --from-literal=NETAPP_PASSWORD=\"${NETAPP_PASSWORD}\" \\\n  --namespace netapp-functions\n\n# Use secret in functions\nkn service update netapp-storage-monitor \\\n  --env-from secret:netapp-credentials \\\n  --namespace netapp-functions\n</code></pre>"},{"location":"getting-started/knative-functions-quickstart/#network-policies","title":"Network Policies","text":"<pre><code># Network policy for function isolation\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: netapp-functions-netpol\n  namespace: netapp-functions\nspec:\n  podSelector:\n    matchLabels:\n      serving.knative.dev/service: netapp-storage-monitor\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          name: knative-serving\n  egress:\n  - to:\n    - namespaceSelector: {}\n    ports:\n    - protocol: TCP\n      port: 443\n</code></pre>"},{"location":"getting-started/knative-functions-quickstart/#service-mesh-integration","title":"Service Mesh Integration","text":"<pre><code># Enable Istio sidecar injection\nkubectl label namespace netapp-functions istio-injection=enabled\n\n# Create virtual service\nkubectl apply -f - &lt;&lt; 'EOF'\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: netapp-functions-vs\n  namespace: netapp-functions\nspec:\n  hosts:\n  - netapp-storage-monitor.netapp-functions.svc.cluster.local\n  http:\n  - match:\n    - headers:\n        authorization:\n          prefix: \"Bearer \"\n    route:\n    - destination:\n        host: netapp-storage-monitor.netapp-functions.svc.cluster.local\n  - route:\n    - destination:\n        host: netapp-storage-monitor.netapp-functions.svc.cluster.local\n      weight: 0\n    fault:\n      abort:\n        percentage:\n          value: 100\n        httpStatus: 401\nEOF\n</code></pre>"},{"location":"getting-started/knative-functions-quickstart/#cost-optimization","title":"\ud83d\udcb0 Cost Optimization","text":""},{"location":"getting-started/knative-functions-quickstart/#resource-right-sizing","title":"Resource Right-Sizing","text":"<pre><code># Monitor resource usage\nkubectl top pods -n netapp-functions\n\n# Adjust resource requests/limits based on usage\nkn service update netapp-storage-monitor \\\n  --request memory=128Mi \\\n  --request cpu=100m \\\n  --limit memory=256Mi \\\n  --limit cpu=200m \\\n  --namespace netapp-functions\n</code></pre>"},{"location":"getting-started/knative-functions-quickstart/#scaling-optimization","title":"Scaling Optimization","text":"<pre><code># Set aggressive scale-to-zero\nkn service update netapp-volume-ops \\\n  --annotation autoscaling.knative.dev/scaleToZeroIdleTimeout=30s \\\n  --namespace netapp-functions\n\n# Optimize concurrency\nkn service update netapp-svm-manager \\\n  --annotation autoscaling.knative.dev/targetUtilizationPercentage=80 \\\n  --namespace netapp-functions\n</code></pre>"},{"location":"getting-started/knative-functions-quickstart/#troubleshooting","title":"\ud83d\udea8 Troubleshooting","text":""},{"location":"getting-started/knative-functions-quickstart/#common-issues","title":"Common Issues","text":""},{"location":"getting-started/knative-functions-quickstart/#function-not-starting","title":"Function Not Starting","text":"<pre><code># Check pod status\nkubectl get pods -n netapp-functions\n\n# Check pod events\nkubectl describe pod &lt;pod-name&gt; -n netapp-functions\n\n# Check container logs\nkubectl logs &lt;pod-name&gt; -c user-container -n netapp-functions\n</code></pre>"},{"location":"getting-started/knative-functions-quickstart/#function-not-scaling","title":"Function Not Scaling","text":"<pre><code># Check autoscaler logs\nkubectl logs -n knative-serving -l app=autoscaler\n\n# Check serving controller logs\nkubectl logs -n knative-serving -l app=controller\n\n# Check revision status\nkubectl get revisions -n netapp-functions\n</code></pre>"},{"location":"getting-started/knative-functions-quickstart/#network-issues","title":"Network Issues","text":"<pre><code># Test service connectivity\nkubectl run test-pod --rm -it --image=curlimages/curl -- /bin/sh\n# From inside the pod:\ncurl http://netapp-storage-monitor.netapp-functions.svc.cluster.local\n\n# Check service endpoints\nkubectl get endpoints -n netapp-functions\n</code></pre>"},{"location":"getting-started/knative-functions-quickstart/#debug-mode","title":"Debug Mode","text":"<pre><code># Deploy function in debug mode\nkn func deploy \\\n  --env LOG_LEVEL=DEBUG \\\n  --env PYTHONUNBUFFERED=1 \\\n  --namespace netapp-functions\n\n# Enable request tracing\nkn service update netapp-storage-monitor \\\n  --annotation serving.knative.dev/creator=debug-user \\\n  --namespace netapp-functions\n</code></pre>"},{"location":"getting-started/knative-functions-quickstart/#cicd-integration","title":"\ud83d\udd04 CI/CD Integration","text":""},{"location":"getting-started/knative-functions-quickstart/#github-actions","title":"GitHub Actions","text":"<pre><code># .github/workflows/deploy-functions.yml\nname: Deploy NetApp Functions\non:\n  push:\n    branches: [main]\n    paths: ['functions/**']\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v3\n\n    - name: Setup Knative CLI\n      run: |\n        curl -L https://github.com/knative/client/releases/latest/download/kn-linux-amd64 -o kn\n        chmod +x kn &amp;&amp; sudo mv kn /usr/local/bin/\n        curl -L https://github.com/knative/func/releases/latest/download/func_linux_amd64.tar.gz | tar xz\n        chmod +x func &amp;&amp; sudo mv func /usr/local/bin/\n\n    - name: Deploy Functions\n      env:\n        KUBECONFIG: ${{ secrets.KUBECONFIG }}\n        NETAPP_BASE_URL: ${{ secrets.NETAPP_BASE_URL }}\n        NETAPP_USERNAME: ${{ secrets.NETAPP_USERNAME }}\n        NETAPP_PASSWORD: ${{ secrets.NETAPP_PASSWORD }}\n      run: |\n        ./scripts/deploy-knative-functions.sh\n</code></pre>"},{"location":"getting-started/knative-functions-quickstart/#gitops-with-argocd","title":"GitOps with ArgoCD","text":"<pre><code># argocd/netapp-functions-app.yaml\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: netapp-functions\n  namespace: argocd\nspec:\n  project: default\n  source:\n    repoURL: https://github.com/your-org/netapp-functions\n    targetRevision: HEAD\n    path: k8s/functions\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: netapp-functions\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n</code></pre>"},{"location":"getting-started/knative-functions-quickstart/#next-steps","title":"\ud83d\udcda Next Steps","text":"<ol> <li>Extend Functions: Add more NetApp operations as separate functions</li> <li>Implement Event-Driven Architecture: Use Knative Eventing for NetApp alerts</li> <li>Add AI Integration: Connect functions to AI assistants via MCP protocol</li> <li>Setup Temporal Workflows: Orchestrate complex multi-function operations</li> <li>Implement Chaos Engineering: Test function resilience and auto-recovery</li> </ol>"},{"location":"getting-started/knative-functions-quickstart/#support","title":"\ud83d\udcde Support","text":"<ul> <li>Documentation: Function-Based Architecture</li> <li>Target Operating Model: Knative Function TOM</li> <li>Issues: Create GitHub issues for problems or feature requests</li> <li>Community: Join NetApp developer community for support</li> </ul>"},{"location":"getting-started/overview/","title":"NetApp ActiveIQ MCP Server Overview","text":""},{"location":"getting-started/overview/#what-is-the-netapp-activeiq-mcp-server","title":"What is the NetApp ActiveIQ MCP Server?","text":"<p>The NetApp ActiveIQ MCP (Model Context Protocol) Server is a specialized server implementation that bridges NetApp's ActiveIQ Unified Manager API with AI assistants and automation tools through the standardized MCP protocol. This server enables AI applications to interact with NetApp storage infrastructure using natural language and structured queries.</p>"},{"location":"getting-started/overview/#key-features","title":"Key Features","text":""},{"location":"getting-started/overview/#mcp-protocol-compliance","title":"\ud83d\udd17 MCP Protocol Compliance","text":"<ul> <li>Full compliance with the Model Context Protocol specification</li> <li>Standardized tool definitions for NetApp operations</li> <li>Seamless integration with MCP-compatible AI assistants</li> </ul>"},{"location":"getting-started/overview/#netapp-integration","title":"\ud83c\udfd7\ufe0f NetApp Integration","text":"<ul> <li>Direct connection to ActiveIQ Unified Manager APIs</li> <li>Real-time access to storage metrics and events</li> <li>Support for all major NetApp storage operations</li> </ul>"},{"location":"getting-started/overview/#high-performance","title":"\u26a1 High Performance","text":"<ul> <li>Async/await architecture for concurrent operations</li> <li>Connection pooling and request optimization</li> <li>Built-in caching for frequently accessed data</li> </ul>"},{"location":"getting-started/overview/#enterprise-security","title":"\ud83d\udd12 Enterprise Security","text":"<ul> <li>Secure credential management</li> <li>Role-based access control integration</li> <li>SSL/TLS encryption for all communications</li> </ul>"},{"location":"getting-started/overview/#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph TB\n    A[AI Assistant] --&gt; B[MCP Client]\n    B --&gt; C[NetApp ActiveIQ MCP Server]\n    C --&gt; D[ActiveIQ Unified Manager API]\n    C --&gt; E[Temporal Workflows]\n    C --&gt; F[Prometheus Metrics]\n\n    D --&gt; G[NetApp Storage Systems]\n    E --&gt; H[Workflow Engine]\n    F --&gt; I[Monitoring Dashboard]\n\n    subgraph \"MCP Server Components\"\n        C1[Tool Registry]\n        C2[Authentication Manager]\n        C3[Request Router]\n        C4[Response Formatter]\n    end\n\n    C --&gt; C1\n    C --&gt; C2\n    C --&gt; C3\n    C --&gt; C4</code></pre>"},{"location":"getting-started/overview/#use-cases","title":"Use Cases","text":""},{"location":"getting-started/overview/#storage-monitoring","title":"\ud83d\udcca Storage Monitoring","text":"<ul> <li>Query cluster health and performance metrics</li> <li>Monitor volume capacity and utilization</li> <li>Track storage efficiency metrics</li> <li>Generate custom reports</li> </ul>"},{"location":"getting-started/overview/#infrastructure-management","title":"\ud83d\udd27 Infrastructure Management","text":"<ul> <li>Automate SVM creation and configuration</li> <li>Manage NFS/CIFS shares</li> <li>Configure network interfaces</li> <li>Handle storage provisioning</li> </ul>"},{"location":"getting-started/overview/#performance-analysis","title":"\ud83d\udcc8 Performance Analysis","text":"<ul> <li>Real-time performance monitoring</li> <li>Historical trend analysis</li> <li>Capacity planning insights</li> <li>Bottleneck identification</li> </ul>"},{"location":"getting-started/overview/#event-management","title":"\ud83d\udea8 Event Management","text":"<ul> <li>Real-time event processing</li> <li>Automated alert handling</li> <li>Incident response workflows</li> <li>Problem resolution tracking</li> </ul>"},{"location":"getting-started/overview/#benefits","title":"Benefits","text":""},{"location":"getting-started/overview/#for-storage-administrators","title":"For Storage Administrators","text":"<ul> <li>Natural language queries for complex storage operations</li> <li>Automated routine tasks and workflows</li> <li>Real-time insights and recommendations</li> <li>Simplified multi-cluster management</li> </ul>"},{"location":"getting-started/overview/#for-developers","title":"For Developers","text":"<ul> <li>Standardized API through MCP protocol</li> <li>Rich tool ecosystem for NetApp operations</li> <li>Easy integration with existing workflows</li> <li>Comprehensive documentation and examples</li> </ul>"},{"location":"getting-started/overview/#for-ai-applications","title":"For AI Applications","text":"<ul> <li>Direct access to storage infrastructure data</li> <li>Context-aware storage operations</li> <li>Automated decision making capabilities</li> <li>Seamless workflow integration</li> </ul>"},{"location":"getting-started/overview/#getting-started","title":"Getting Started","text":"<ol> <li>Quick Start - Get up and running in 5 minutes</li> <li>Installation - Detailed installation guide</li> <li>Configuration - Configure for your environment</li> </ol>"},{"location":"getting-started/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Explore the Architecture for technical details</li> <li>Check out API Reference for available tools</li> <li>See Examples for practical implementations</li> <li>Learn about Deployment options</li> </ul>"},{"location":"getting-started/overview/#support","title":"Support","text":"<ul> <li>Documentation: This comprehensive documentation site</li> <li>GitHub Issues: Report bugs and request features</li> <li>Community: Join our discussion forums</li> <li>Enterprise Support: NetApp support channels for enterprise customers</li> </ul>"},{"location":"getting-started/quick-start/","title":"Quick Start Guide","text":"<p>Get your NetApp ActiveIQ MCP Server up and running in 5 minutes!</p>"},{"location":"getting-started/quick-start/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have:</p> <ul> <li>\u2705 Docker installed and running</li> <li>\u2705 NetApp ActiveIQ Unified Manager accessible</li> <li>\u2705 Valid credentials with appropriate permissions</li> <li>\u2705 Python 3.10+ (for development setup)</li> </ul>"},{"location":"getting-started/quick-start/#option-1-docker-quick-start-recommended","title":"Option 1: Docker Quick Start (Recommended)","text":""},{"location":"getting-started/quick-start/#1-pull-the-docker-image","title":"1. Pull the Docker Image","text":"<pre><code>docker pull netapp/activeiq-mcp-server:latest\n</code></pre>"},{"location":"getting-started/quick-start/#2-create-configuration-file","title":"2. Create Configuration File","text":"<p>Create a <code>.env</code> file with your NetApp credentials:</p> <pre><code># NetApp ActiveIQ Configuration\nNETAPP_UM_HOST=your-unified-manager.company.com\nNETAPP_USERNAME=admin\nNETAPP_PASSWORD=your-secure-password\n\n# MCP Server Configuration\nMCP_SERVER_PORT=8080\nMCP_SERVER_HOST=0.0.0.0\nLOG_LEVEL=INFO\n\n# Optional: Temporal Integration\nTEMPORAL_HOST=localhost:7233\nTEMPORAL_NAMESPACE=default\n</code></pre>"},{"location":"getting-started/quick-start/#3-run-the-server","title":"3. Run the Server","text":"<pre><code>docker run -d \\\n  --name netapp-mcp-server \\\n  --env-file .env \\\n  -p 8080:8080 \\\n  netapp/activeiq-mcp-server:latest\n</code></pre>"},{"location":"getting-started/quick-start/#4-verify-installation","title":"4. Verify Installation","text":"<pre><code># Check server status\ncurl http://localhost:8080/health\n\n# List available MCP tools\ncurl http://localhost:8080/tools\n</code></pre>"},{"location":"getting-started/quick-start/#option-2-development-setup","title":"Option 2: Development Setup","text":""},{"location":"getting-started/quick-start/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/netapp/activeiq-mcp-server.git\ncd activeiq-mcp-server\n</code></pre>"},{"location":"getting-started/quick-start/#2-install-dependencies","title":"2. Install Dependencies","text":"<pre><code># Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install dependencies\npip install -r requirements.txt\n</code></pre>"},{"location":"getting-started/quick-start/#3-configure-environment","title":"3. Configure Environment","text":"<pre><code>cp .env.example .env\n# Edit .env with your NetApp credentials\n</code></pre>"},{"location":"getting-started/quick-start/#4-run-the-server","title":"4. Run the Server","text":"<pre><code>python -m netapp_mcp_server\n</code></pre>"},{"location":"getting-started/quick-start/#first-steps-with-mcp-tools","title":"First Steps with MCP Tools","text":""},{"location":"getting-started/quick-start/#1-test-connection","title":"1. Test Connection","text":"<pre><code># Test NetApp connection\ncurl -X POST http://localhost:8080/tools/test_connection \\\n  -H \"Content-Type: application/json\"\n</code></pre>"},{"location":"getting-started/quick-start/#2-get-cluster-information","title":"2. Get Cluster Information","text":"<pre><code># List all clusters\ncurl -X POST http://localhost:8080/tools/get_clusters \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"arguments\": {\"fields\": [\"name\", \"version\", \"state\"]}}'\n</code></pre>"},{"location":"getting-started/quick-start/#3-monitor-storage","title":"3. Monitor Storage","text":"<pre><code># Get volume information\ncurl -X POST http://localhost:8080/tools/get_volumes \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"arguments\": {\"max_records\": 10}}'\n</code></pre>"},{"location":"getting-started/quick-start/#connect-with-ai-assistant","title":"Connect with AI Assistant","text":""},{"location":"getting-started/quick-start/#using-with-claude-desktop","title":"Using with Claude Desktop","text":"<ol> <li>Add to your Claude Desktop configuration:</li> </ol> <pre><code>{\n  \"mcpServers\": {\n    \"netapp-activeiq\": {\n      \"command\": \"docker\",\n      \"args\": [\"exec\", \"-i\", \"netapp-mcp-server\", \"python\", \"-m\", \"netapp_mcp_server\", \"--stdio\"],\n      \"env\": {\n        \"NETAPP_UM_HOST\": \"your-unified-manager.company.com\",\n        \"NETAPP_USERNAME\": \"admin\",\n        \"NETAPP_PASSWORD\": \"your-secure-password\"\n      }\n    }\n  }\n}\n</code></pre> <ol> <li> <p>Restart Claude Desktop</p> </li> <li> <p>Try natural language queries:</p> </li> <li>\"Show me the health status of all clusters\"</li> <li>\"What volumes are running low on space?\"</li> <li>\"Create a new SVM for the production environment\"</li> </ol>"},{"location":"getting-started/quick-start/#using-with-other-mcp-clients","title":"Using with Other MCP Clients","text":"<p>The server implements the standard MCP protocol and can be used with any MCP-compatible client. See the MCP Protocol documentation for integration details.</p>"},{"location":"getting-started/quick-start/#verification-checklist","title":"Verification Checklist","text":"<p>After setup, verify everything is working:</p> <ul> <li> Server starts without errors</li> <li> Health endpoint returns 200 OK</li> <li> NetApp connection test succeeds</li> <li> MCP tools are listed correctly</li> <li> Sample queries return data</li> <li> AI assistant can communicate with server</li> </ul>"},{"location":"getting-started/quick-start/#common-issues","title":"Common Issues","text":""},{"location":"getting-started/quick-start/#connection-issues","title":"Connection Issues","text":"<p>Problem: Cannot connect to NetApp Unified Manager <pre><code># Check network connectivity\ncurl -k https://your-unified-manager.company.com/api/v2/datacenter/cluster/clusters\n\n# Verify credentials\ncurl -k -u \"username:password\" https://your-unified-manager.company.com/api/v2/datacenter/cluster/clusters\n</code></pre></p>"},{"location":"getting-started/quick-start/#authentication-issues","title":"Authentication Issues","text":"<p>Problem: 401 Unauthorized responses - Verify username/password in .env file - Check user has required roles (Operator, Storage Administrator, or Application Administrator) - Ensure account is not locked</p>"},{"location":"getting-started/quick-start/#docker-issues","title":"Docker Issues","text":"<p>Problem: Container won't start <pre><code># Check logs\ndocker logs netapp-mcp-server\n\n# Check environment variables\ndocker exec netapp-mcp-server env | grep NETAPP\n</code></pre></p>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<p>Now that your server is running:</p> <ol> <li>Learn the Architecture - Understand how it works</li> <li>Explore API Tools - See all available operations</li> <li>Try Examples - Follow guided examples</li> <li>Deploy to Production - Production deployment guide</li> </ol>"},{"location":"getting-started/quick-start/#getting-help","title":"Getting Help","text":"<ul> <li>\ud83d\udcd6 Documentation: Browse this documentation site</li> <li>\ud83d\udc1b Issues: Report problems on GitHub</li> <li>\ud83d\udcac Discussions: Join our community discussions</li> <li>\ud83d\udce7 Support: Contact NetApp support for enterprise assistance</li> </ul>"},{"location":"testing/mermaid-test-page/","title":"Mermaid Diagram Testing Page","text":"<p>This page contains various Mermaid diagrams to test the MkDocs Mermaid integration and identify any syntax issues.</p>"},{"location":"testing/mermaid-test-page/#1-basic-flowchart","title":"1. Basic Flowchart","text":"<pre><code>graph TD\n    A[Start] --&gt; B{Decision}\n    B --&gt;|Yes| C[Process 1]\n    B --&gt;|No| D[Process 2]\n    C --&gt; E[End]\n    D --&gt; E</code></pre>"},{"location":"testing/mermaid-test-page/#2-sequence-diagram","title":"2. Sequence Diagram","text":"<pre><code>sequenceDiagram\n    participant A as Client\n    participant B as Server\n    participant C as Database\n\n    A-&gt;&gt;B: Request\n    B-&gt;&gt;C: Query\n    C--&gt;&gt;B: Response\n    B--&gt;&gt;A: Result</code></pre>"},{"location":"testing/mermaid-test-page/#3-gantt-chart","title":"3. Gantt Chart","text":"<pre><code>gantt\n    title NetApp Project Timeline\n    dateFormat  YYYY-MM-DD\n    section Phase 1\n    Planning     :a1, 2024-01-01, 30d\n    Development  :after a1, 45d\n    section Phase 2\n    Testing      :2024-03-15, 20d\n    Deployment   :2024-04-04, 10d</code></pre>"},{"location":"testing/mermaid-test-page/#4-class-diagram","title":"4. Class Diagram","text":"<pre><code>classDiagram\n    class MCP_Server {\n        +str name\n        +str version\n        +list tools\n        +initialize()\n        +handle_request()\n    }\n\n    class NetApp_API {\n        +str base_url\n        +str auth_token\n        +get_clusters()\n        +get_volumes()\n    }\n\n    MCP_Server --&gt; NetApp_API : uses</code></pre>"},{"location":"testing/mermaid-test-page/#5-state-diagram","title":"5. State Diagram","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; Idle\n    Idle --&gt; Processing : request\n    Processing --&gt; Success : complete\n    Processing --&gt; Error : fail\n    Success --&gt; [*]\n    Error --&gt; Idle : retry</code></pre>"},{"location":"testing/mermaid-test-page/#6-entity-relationship-diagram","title":"6. Entity Relationship Diagram","text":"<pre><code>erDiagram\n    CLUSTER ||--o{ SVM : contains\n    SVM ||--o{ VOLUME : contains\n    VOLUME ||--o{ SNAPSHOT : has\n\n    CLUSTER {\n        string uuid\n        string name\n        string version\n    }\n\n    SVM {\n        string uuid\n        string name\n        string type\n    }\n\n    VOLUME {\n        string uuid\n        string name\n        int size\n    }</code></pre>"},{"location":"testing/mermaid-test-page/#7-user-journey","title":"7. User Journey","text":"<pre><code>journey\n    title NetApp Storage Admin Journey\n    section Discovery\n      Login to ActiveIQ: 5: Admin\n      View Clusters: 4: Admin\n      Check Performance: 3: Admin\n    section Action\n      Identify Issues: 2: Admin\n      Create Volumes: 5: Admin\n      Monitor Results: 4: Admin</code></pre>"},{"location":"testing/mermaid-test-page/#8-git-graph","title":"8. Git Graph","text":"<pre><code>gitgraph\n    commit id: \"Initial\"\n    branch develop\n    checkout develop\n    commit id: \"Feature A\"\n    commit id: \"Feature B\"\n    checkout main\n    merge develop\n    commit id: \"Release 1.0\"</code></pre>"},{"location":"testing/mermaid-test-page/#9-pie-chart","title":"9. Pie Chart","text":"<pre><code>pie title Storage Utilization\n    \"Used\" : 65\n    \"Available\" : 25\n    \"Reserved\" : 10</code></pre>"},{"location":"testing/mermaid-test-page/#10-flowchart-with-subgraphs","title":"10. Flowchart with Subgraphs","text":"<pre><code>flowchart TD\n    subgraph \"NetApp Environment\"\n        A[ActiveIQ Unified Manager]\n        B[Cluster 1]\n        C[Cluster 2]\n    end\n\n    subgraph \"MCP Server\"\n        D[API Gateway]\n        E[Authentication]\n        F[Tools Handler]\n    end\n\n    subgraph \"Client Applications\"\n        G[AI Assistant]\n        H[Automation Tools]\n        I[Monitoring Dashboard]\n    end\n\n    A --&gt; D\n    B --&gt; A\n    C --&gt; A\n    D --&gt; E\n    E --&gt; F\n    F --&gt; G\n    F --&gt; H\n    F --&gt; I</code></pre>"},{"location":"testing/mermaid-test-page/#11-complex-sequence-with-loops","title":"11. Complex Sequence with Loops","text":"<pre><code>sequenceDiagram\n    participant C as Client\n    participant M as MCP Server\n    participant N as NetApp API\n    participant D as Database\n\n    C-&gt;&gt;M: Initialize Connection\n    M-&gt;&gt;N: Authenticate\n    N--&gt;&gt;M: Auth Token\n\n    loop Health Check\n        M-&gt;&gt;N: Get Cluster Status\n        N--&gt;&gt;M: Status Response\n    end\n\n    C-&gt;&gt;M: Get Volume Info\n    M-&gt;&gt;N: Query Volumes\n    N-&gt;&gt;D: Fetch Data\n    D--&gt;&gt;N: Volume Data\n    N--&gt;&gt;M: Volume Response\n    M--&gt;&gt;C: Formatted Result\n\n    Note over C,D: End-to-end data flow</code></pre>"},{"location":"testing/mermaid-test-page/#12-advanced-flowchart-with-styling","title":"12. Advanced Flowchart with Styling","text":"<pre><code>flowchart LR\n    A[User Request] --&gt; B{Authentication}\n    B --&gt;|Valid| C[Process Request]\n    B --&gt;|Invalid| D[Return Error]\n    C --&gt; E{Request Type}\n    E --&gt;|Cluster Info| F[Get Clusters]\n    E --&gt;|Volume Info| G[Get Volumes]\n    E --&gt;|Performance| H[Get Metrics]\n    F --&gt; I[Format Response]\n    G --&gt; I\n    H --&gt; I\n    I --&gt; J[Return Result]\n    D --&gt; K[Log Error]\n\n    classDef successClass fill:#d4edda,stroke:#155724,stroke-width:2px\n    classDef errorClass fill:#f8d7da,stroke:#721c24,stroke-width:2px\n    classDef processClass fill:#fff3cd,stroke:#856404,stroke-width:2px\n\n    class F,G,H,I,J successClass\n    class D,K errorClass\n    class C,E processClass</code></pre>"},{"location":"testing/mermaid-test-page/#13-timeline","title":"13. Timeline","text":"<pre><code>timeline\n    title NetApp MCP Server Development\n\n    2024-Q1 : Planning\n             : Requirements Gathering\n             : API Analysis\n\n    2024-Q2 : Development\n             : Core Implementation\n             : Testing Framework\n\n    2024-Q3 : Integration\n             : AI Assistant Support\n             : Documentation\n\n    2024-Q4 : Production\n             : Deployment\n             : Monitoring Setup</code></pre>"},{"location":"testing/mermaid-test-page/#14-mindmap","title":"14. Mindmap","text":"<pre><code>mindmap\n  root((NetApp MCP))\n    Clusters\n      Performance\n      Health\n      Configuration\n    Volumes\n      Storage\n      Snapshots\n      Quotas\n    SVMs\n      Protocols\n      Security\n      Networking\n    Tools\n      Monitoring\n      Automation\n      Analysis</code></pre>"},{"location":"testing/mermaid-test-page/#test-status","title":"Test Status","text":"<ul> <li>\u2705 Basic diagrams should render correctly</li> <li>\u2705 Complex diagrams with subgraphs</li> <li>\u2705 Styling and theming</li> <li>\u2705 Various diagram types (flowchart, sequence, gantt, etc.)</li> </ul>"},{"location":"testing/mermaid-test-page/#common-issues-to-watch-for","title":"Common Issues to Watch For","text":"<ol> <li>Arrow syntax: Ensure <code>--&gt;</code> not <code>--&amp;gt;</code> or <code>--&gt;</code></li> <li>Quote escaping: Proper handling of quotes in labels</li> <li>Theme compatibility: Mermaid v11.4.0 theme variables</li> <li>Plugin configuration: mermaid2 plugin settings</li> </ol>"},{"location":"testing/mermaid-test-page/#debugging-commands","title":"Debugging Commands","text":"<p>To test this page locally:</p> <pre><code># Activate virtual environment\nsource venv-docs/bin/activate\n\n# Serve the documentation\nmkdocs serve\n\n# Build for production\nmkdocs build\n</code></pre>"},{"location":"testing/mermaid-test-page/#version-information","title":"Version Information","text":"<ul> <li>Mermaid Version: 11.4.0</li> <li>Plugin: mkdocs-mermaid2-plugin 1.2.1</li> <li>MkDocs: 1.6.1</li> <li>Material Theme: 9.6.14</li> </ul>"},{"location":"testing/mermaid-test-page/#mermaid-configuration-test","title":"Mermaid Configuration Test","text":"<p>Current plugin configuration from mkdocs.yml:</p> <pre><code>- mermaid2:\n    version: \"11.4.0\"\n    arguments:\n      startOnLoad: true\n      theme: \"base\"\n      themeVariables:\n        primaryColor: \"#2196f3\"\n        primaryTextColor: \"#000000\"\n        primaryBorderColor: \"#1976d2\"\n        lineColor: \"#333333\"\n        secondaryColor: \"#ff9800\"\n        tertiaryColor: \"#4caf50\"\n        background: \"#ffffff\"\n        mainBkg: \"#ffffff\"\n        secondBkg: \"#f5f5f5\"\n</code></pre>"},{"location":"testing/mermaid-validation-summary/","title":"Mermaid Syntax Validation Summary","text":""},{"location":"testing/mermaid-validation-summary/#resolution-status","title":"\u2705 Resolution Status","text":"<p>The Mermaid integration has been successfully migrated to native MkDocs Material support! All diagrams now render using the built-in Material theme Mermaid integration, providing better performance and compatibility.</p>"},{"location":"testing/mermaid-validation-summary/#what-was-fixed","title":"\ud83d\udd27 What Was Fixed","text":""},{"location":"testing/mermaid-validation-summary/#original-problem","title":"Original Problem","text":"<ul> <li>Arrows were encoded as <code>--&amp;gt;</code> instead of <code>--&gt;</code></li> <li>Sequence arrows were encoded as <code>-&amp;gt;&amp;gt;</code> instead of <code>-&gt;&gt;</code></li> <li>Unicode escapes like <code>--&gt;</code> were causing syntax errors</li> </ul>"},{"location":"testing/mermaid-validation-summary/#applied-fixes","title":"Applied Fixes","text":"<ul> <li>Replaced <code>--&amp;gt;</code> with <code>--&gt;</code></li> <li>Replaced <code>--&gt;</code> with <code>--&gt;</code></li> <li>Replaced <code>-&amp;gt;&amp;gt;</code> with <code>-&gt;&gt;</code></li> <li>Replaced <code>-&gt;&gt;</code> with <code>-&gt;&gt;</code></li> <li>Applied fixes only within mermaid code blocks for safety</li> </ul>"},{"location":"testing/mermaid-validation-summary/#validation-results","title":"\ud83d\udcca Validation Results","text":"<p>The MkDocs build completed successfully with the following Mermaid diagram stats:</p> Page Diagrams Found Status Testing Page 14 \u2705 All rendered Technical Documentation 13 \u2705 All rendered Knative Function TOM 6 \u2705 All rendered Function-Based Architecture 5 \u2705 All rendered Advanced Workflows 4 \u2705 All rendered System Design 4 \u2705 All rendered Target Operating Model 4 \u2705 All rendered Development Setup 2 \u2705 All rendered Various Use Cases 1 each \u2705 All rendered <p>Total Diagrams Processed: 60+ across all documentation files</p>"},{"location":"testing/mermaid-validation-summary/#test-results","title":"\ud83e\uddea Test Results","text":""},{"location":"testing/mermaid-validation-summary/#native-mermaid-configuration","title":"Native Mermaid Configuration","text":"<pre><code>markdown_extensions:\n  - pymdownx.superfences:\n      custom_fences:\n        - name: mermaid\n          class: mermaid\n          format: !!python/name:pymdownx.superfences.fence_code_format\n\ntheme:\n  extra_javascript:\n    - javascripts/mermaid-config.js\n</code></pre>"},{"location":"testing/mermaid-validation-summary/#build-output","title":"Build Output","text":"<pre><code>INFO - Building documentation to directory: site\nINFO - Documentation built in 4.87 seconds\n</code></pre> <p>Performance Improvement: Build time reduced from 6.06s to 4.87s (20% faster!)</p>"},{"location":"testing/mermaid-validation-summary/#next-steps","title":"\ud83d\ude80 Next Steps","text":""},{"location":"testing/mermaid-validation-summary/#1-test-the-documentation-live","title":"1. Test the Documentation Live","text":"<pre><code>cd /Users/brun_s/Documents/veille-technologique/Professionel/donnees-d-entree/PE-AsProduct/netapp\nsource venv-docs/bin/activate\nmkdocs serve\n</code></pre>"},{"location":"testing/mermaid-validation-summary/#2-view-test-page","title":"2. View Test Page","text":"<p>Navigate to: <code>http://127.0.0.1:8000/testing/mermaid-test-page/</code></p>"},{"location":"testing/mermaid-validation-summary/#3-verify-all-diagram-types","title":"3. Verify All Diagram Types","text":"<p>The test page includes:</p> <ul> <li>\u2705 Basic Flowcharts</li> <li>\u2705 Sequence Diagrams</li> <li>\u2705 Gantt Charts</li> <li>\u2705 Class Diagrams</li> <li>\u2705 State Diagrams</li> <li>\u2705 Entity Relationship Diagrams</li> <li>\u2705 User Journey Maps</li> <li>\u2705 Git Graphs</li> <li>\u2705 Pie Charts</li> <li>\u2705 Complex Flowcharts with Subgraphs</li> <li>\u2705 Styled Diagrams</li> <li>\u2705 Timeline Diagrams</li> <li>\u2705 Mindmaps</li> </ul>"},{"location":"testing/mermaid-validation-summary/#4-verify-other-documentation-pages","title":"4. Verify Other Documentation Pages","text":"<p>Check that existing diagrams render correctly:</p> <ul> <li>Architecture documentation</li> <li>Use case workflows</li> <li>Deployment guides</li> <li>Technical specifications</li> </ul>"},{"location":"testing/mermaid-validation-summary/#prevention-measures","title":"\ud83d\udee1\ufe0f Prevention Measures","text":""},{"location":"testing/mermaid-validation-summary/#git-pre-commit-hook-recommended","title":"Git Pre-commit Hook (Recommended)","text":"<pre><code>#!/bin/bash\n# .git/hooks/pre-commit\necho \"\ud83d\udd0d Checking for Mermaid syntax issues...\"\nif grep -r --include=\"*.md\" \"mermaid\" docs/ | grep -E \"(--&amp;gt;|--\\&gt;)\"; then\n    echo \"\u274c Found encoded arrows in Mermaid diagrams\"\n    echo \"Run: ./scripts/fix-mermaid-syntax.sh\"\n    exit 1\nfi\necho \"\u2705 Mermaid syntax looks good\"\n</code></pre>"},{"location":"testing/mermaid-validation-summary/#cicd-integration","title":"CI/CD Integration","text":"<p>Add to your build pipeline:</p> <pre><code>- name: Validate Mermaid Syntax\n  run: |\n    if grep -r --include=\"*.md\" \"mermaid\" docs/ | grep -E \"(--&amp;gt;|--\\&gt;)\"; then\n      echo \"Encoded arrows found in Mermaid diagrams\"\n      exit 1\n    fi\n</code></pre>"},{"location":"testing/mermaid-validation-summary/#troubleshooting-guide","title":"\ud83d\udcdd Troubleshooting Guide","text":""},{"location":"testing/mermaid-validation-summary/#common-issues","title":"Common Issues","text":"<ol> <li>New encoded arrows: Run the fix script again</li> <li>Theme compatibility: Ensure Mermaid version matches plugin</li> <li>Browser rendering: Clear cache and reload</li> <li>Plugin conflicts: Check superfences configuration</li> </ol>"},{"location":"testing/mermaid-validation-summary/#debug-commands","title":"Debug Commands","text":"<pre><code># Test build only\nmkdocs build --clean\n\n# Verbose mode\nmkdocs build --verbose\n\n# Check specific page\nmkdocs serve --dev-addr=127.0.0.1:8001\n</code></pre>"},{"location":"testing/mermaid-validation-summary/#summary","title":"\u2728 Summary","text":"<p>The native MkDocs Material Mermaid integration is now fully operational with:</p> <ul> <li>\u2705 60+ diagrams successfully rendering</li> <li>\u2705 20% faster build times (4.87s vs 6.06s)</li> <li>\u2705 Simplified configuration (no external plugins required)</li> <li>\u2705 Better theme integration with automatic dark/light mode support</li> <li>\u2705 Improved compatibility with future MkDocs Material updates</li> <li>\u2705 Enhanced responsiveness and mobile support</li> <li>\u2705 Prevention measures in place</li> </ul>"},{"location":"testing/mermaid-validation-summary/#migration-benefits","title":"Migration Benefits","text":"<ul> <li>Performance: Faster builds and reduced dependencies</li> <li>Maintainability: Native support reduces plugin conflicts</li> <li>Theming: Better integration with Material theme system</li> <li>Future-proof: Direct support by Material theme maintainers</li> </ul> <p>All diagram types are working correctly with native Material theme integration.</p>"},{"location":"use-cases/annotate_event/","title":"Use Case: Annotating an Event","text":"<p>This sequence diagram illustrates how to authenticate, find an event, and then add or update an annotation.</p>"},{"location":"use-cases/annotate_event/#adding-a-new-annotation","title":"Adding a New Annotation","text":"<pre><code>sequenceDiagram\n    participant AS as Automation Script\n    participant API as NetApp ActiveIQ API\n\n    AS-&gt;&gt;+API: Authenticate (HTTP Basic Auth)\n    API--&gt;&gt;-AS: 200 OK (Authentication Successful)\n\n    Note over AS, API: All subsequent requests are authenticated\n\n    AS-&gt;&gt;+API: GET /management-server/events (Find Event)\n    API--&gt;&gt;-AS: 200 OK (List of Events)\n\n    AS-&gt;&gt;+API: PATCH /management-server/events/{key} (Add Annotation)\n    API--&gt;&gt;-AS: 200 OK (Event with Annotation)</code></pre>"},{"location":"use-cases/annotate_event/#updating-an-existing-annotation","title":"Updating an Existing Annotation","text":"<pre><code>sequenceDiagram\n    participant AS as Automation Script\n    participant API as NetApp ActiveIQ API\n\n    AS-&gt;&gt;+API: Authenticate (HTTP Basic Auth)\n    API--&gt;&gt;-AS: 200 OK (Authentication Successful)\n\n    Note over AS, API: All subsequent requests are authenticated\n\n    AS-&gt;&gt;+API: GET /management-server/events/{key} (Get Current Event)\n    API--&gt;&gt;-AS: 200 OK (Event with Current Annotation)\n\n    Note right of AS: Parse existing annotation&lt;br/&gt;Merge or replace with new values\n\n    AS-&gt;&gt;+API: PATCH /management-server/events/{key} (Update Annotation)\n    API--&gt;&gt;-AS: 200 OK (Event with Updated Annotation)\n\n    AS-&gt;&gt;+API: GET /management-server/events/{key} (Verify Update)\n    API--&gt;&gt;-AS: 200 OK (Event with New Annotation)</code></pre>"},{"location":"use-cases/annotate_event/#error-handling","title":"Error Handling","text":"<ul> <li>Authentication Failure (401 Unauthorized): If authentication fails, the script should log the error and terminate. Ensure that the API credentials are correct and have the necessary permissions.</li> <li>Event Not Found (404 Not Found): If the event to be annotated is not found, the script should handle the error gracefully. This could involve logging the error and moving on to the next event.</li> <li>Invalid Annotation (400 Bad Request): If the annotation format is invalid, the API will return a 400 error. The script should ensure that the annotation is a simple string.</li> <li>Forbidden (403 Forbidden): If the user doesn't have permissions to modify the event, the script should log the error and notify the administrator.</li> <li>Conflict (409 Conflict): If another process is modifying the same event simultaneously, the API may return a 409 error. The script should implement retry logic with backoff to handle this.</li> <li>Network Errors: Implement retry logic with exponential backoff for transient network errors.</li> </ul>"},{"location":"use-cases/attach_metadata_to_object/","title":"Use Case: Attaching Metadata to any Object","text":"<p>This sequence diagram illustrates a generic process for attaching metadata to any object (e.g., SVM, LUN, file share) by creating an associated event with a custom annotation.</p> <pre><code>sequenceDiagram\n    participant User as User/Admin\n    participant Console as ActiveIQ Unified Manager\n    participant API as NetApp ActiveIQ API\n\n    User-&gt;&gt;+Console: 1. Log in to ActiveIQ Console\n    Console--&gt;&gt;-User: Authentication Successful\n\n    User-&gt;&gt;+Console: 2. Navigate to Object Explorer\n    Console--&gt;&gt;-User: Display Object Inventory (SVMs, LUNs, etc.)\n\n    User-&gt;&gt;+Console: 3. Select Object to Tag\n    Console--&gt;&gt;-User: Display Object Details\n\n    User-&gt;&gt;+Console: 4. Initiate \"Add Annotation\" Action\n    Note right of Console: Console prepares event creation request&lt;br/&gt;with object key and user-provided metadata\n    Console-&gt;&gt;+API: 5. POST /management-server/events (Create Event)\n    API--&gt;&gt;-Console: 202 Accepted (Job Key)\n\n    loop Poll Job Status\n        Console-&gt;&gt;+API: GET /management-server/jobs/{job_key}\n        API--&gt;&gt;-Console: 200 OK (Job Status)\n    end\n\n    Console-&gt;&gt;+User: 6. Display Confirmation (Annotation Added)\n\n    User-&gt;&gt;+Console: 7. View Object Annotations\n    Console-&gt;&gt;+API: 8. GET /management-server/events?resource.key={object_key}\n    API--&gt;&gt;-Console: 200 OK (List of Events with Annotations)\n    Console--&gt;&gt;-User: Display Associated Annotations</code></pre>"},{"location":"use-cases/attach_metadata_to_object/#error-handling","title":"Error Handling","text":"<ul> <li>Object Not Found (404 Not Found): If the target object for annotation does not exist, the console should provide a user-friendly error message.</li> <li>Invalid Metadata (400 Bad Request): The console should validate the metadata format (e.g., key-value pairs) before submitting the request to the API.</li> <li>Permission Denied (403 Forbidden): If the user does not have the necessary permissions to create events or annotate objects, the console should display an access denied message.</li> <li>Job Failure: If the event creation job fails, the console should provide detailed error information to help the user diagnose the issue.</li> <li>Concurrent Modifications (409 Conflict): If the object is being modified by another user, the console should handle the conflict gracefully, perhaps by asking the user to retry.</li> </ul>"},{"location":"use-cases/backup-recovery/","title":"Backup &amp; Recovery","text":""},{"location":"use-cases/backup-recovery/#overview","title":"Overview","text":"<p>Backup &amp; Recovery is a critical DevOps use case that leverages the NetApp ActiveIQ MCP server through APIM to ensure data protection, disaster recovery, and business continuity. This use case demonstrates how DevOps teams can implement automated, intelligent, and reliable backup and recovery operations through orchestrated workflows and AI-enhanced monitoring.</p>"},{"location":"use-cases/backup-recovery/#architecture-flow","title":"Architecture Flow","text":"<pre><code>sequenceDiagram\n    participant DevOps as DevOps GUI\n    participant APIM as API Management (APIM)\n    participant Temporal as Temporal Workflows\n    participant MCP as MCP Server (Optional)\n    participant NetApp as NetApp ActiveIQ APIs\n    participant AI as AI Assistant (Day-2)\n    participant Storage as Storage Systems\n\n    DevOps-&gt;&gt;APIM: Initiate Backup Operation\n    APIM-&gt;&gt;Temporal: Trigger Backup Workflow\n    Temporal-&gt;&gt;MCP: Optional: Enhanced Backup Context\n    Temporal-&gt;&gt;NetApp: Execute Backup Commands\n    NetApp-&gt;&gt;Storage: Perform Backup Operations\n    Storage--&gt;&gt;NetApp: Backup Status\n    NetApp--&gt;&gt;Temporal: Operation Results\n    Temporal-&gt;&gt;APIM: Backup Completion Status\n    APIM--&gt;&gt;DevOps: Operation Summary\n\n    Note over AI: Day-2 Operations\n    AI-&gt;&gt;Temporal: Backup Health Analysis\n    AI-&gt;&gt;DevOps: Recovery Recommendations\n    AI-&gt;&gt;APIM: Predictive Failure Alerts</code></pre>"},{"location":"use-cases/backup-recovery/#backup-recovery-categories","title":"Backup &amp; Recovery Categories","text":""},{"location":"use-cases/backup-recovery/#1-data-protection-levels","title":"1. Data Protection Levels","text":"<ul> <li>Local Snapshots: Point-in-time copies within the same system</li> <li>Local Replication: Mirrored copies to local secondary storage</li> <li>Remote Replication: Copies to geographically distributed sites</li> <li>Cloud Backup: Long-term retention in cloud storage</li> <li>Archive Storage: Compliance and long-term retention</li> </ul>"},{"location":"use-cases/backup-recovery/#2-recovery-types","title":"2. Recovery Types","text":"<ul> <li>File-Level Recovery: Individual file restoration</li> <li>Volume Recovery: Complete volume restoration</li> <li>Database Recovery: Application-consistent database recovery</li> <li>System Recovery: Full system restoration</li> <li>Disaster Recovery: Cross-site failover and recovery</li> </ul>"},{"location":"use-cases/backup-recovery/#3-protection-policies","title":"3. Protection Policies","text":"<ul> <li>RPO (Recovery Point Objective): Maximum acceptable data loss</li> <li>RTO (Recovery Time Objective): Maximum acceptable downtime</li> <li>Retention Policies: How long backups are retained</li> <li>Compliance Requirements: Regulatory and industry standards</li> </ul>"},{"location":"use-cases/backup-recovery/#apim-managed-backup-workflows","title":"APIM-Managed Backup Workflows","text":""},{"location":"use-cases/backup-recovery/#1-automated-backup-scheduling","title":"1. Automated Backup Scheduling","text":"<pre><code>workflow_name: scheduled_backup\ntrigger: cron_schedule\nschedules:\n  - daily_backup:\n      time: \"02:00\"\n      frequency: daily\n      retention: 30_days\n  - weekly_backup:\n      time: \"01:00\"\n      day: sunday\n      retention: 12_weeks\n  - monthly_backup:\n      time: \"00:00\"\n      day: first_sunday\n      retention: 12_months\nsteps:\n  - pre_backup_validation:\n      check_space: true\n      verify_connectivity: true\n      validate_snapmirror: true\n  - execute_backup:\n      type: snapshot\n      consistency_group: true\n      application_quiesce: true\n  - post_backup_verification:\n      verify_integrity: true\n      update_catalog: true\n      send_notifications: true\n</code></pre>"},{"location":"use-cases/backup-recovery/#2-disaster-recovery-orchestration","title":"2. Disaster Recovery Orchestration","text":"<pre><code>workflow_name: disaster_recovery\ntrigger: manual_or_automated\nfailover_types:\n  - planned_maintenance\n  - unplanned_disaster\n  - testing_scenario\nsteps:\n  - assessment_phase:\n      evaluate_primary_site: true\n      check_secondary_readiness: true\n      calculate_rpo_rto: true\n  - failover_execution:\n      break_snapmirror: conditional\n      promote_secondary: true\n      update_dns_records: true\n      restart_applications: true\n  - validation_phase:\n      verify_data_integrity: true\n      test_application_functionality: true\n      confirm_user_access: true\n  - notification_phase:\n      notify_stakeholders: true\n      update_monitoring_systems: true\n      document_actions_taken: true\n</code></pre>"},{"location":"use-cases/backup-recovery/#3-recovery-testing-automation","title":"3. Recovery Testing Automation","text":"<pre><code>workflow_name: recovery_testing\ntrigger: scheduled_monthly\ntest_scenarios:\n  - file_recovery_test\n  - volume_recovery_test\n  - database_recovery_test\n  - full_disaster_recovery_test\nsteps:\n  - test_environment_setup:\n      clone_production_data: true\n      isolate_test_environment: true\n      prepare_test_systems: true\n  - recovery_simulation:\n      execute_recovery_procedures: true\n      measure_recovery_times: true\n      validate_data_consistency: true\n  - test_cleanup:\n      remove_test_clones: true\n      restore_test_environment: true\n      archive_test_results: true\n  - reporting:\n      generate_test_report: true\n      update_rpo_rto_metrics: true\n      recommend_improvements: true\n</code></pre>"},{"location":"use-cases/backup-recovery/#devops-integration-patterns","title":"DevOps Integration Patterns","text":""},{"location":"use-cases/backup-recovery/#backup-management-interface","title":"Backup Management Interface","text":"<pre><code># Example: Backup and recovery integration\nfrom netapp_mcp_client import NetAppMCPClient\nfrom apim_client import APIMClient\nfrom datetime import datetime, timedelta\n\nclass BackupRecoveryManager:\n    def __init__(self):\n        self.apim = APIMClient()\n        self.mcp_client = NetAppMCPClient()\n\n    async def schedule_backup(self, volume_id: str, backup_policy: dict):\n        \"\"\"Schedule a backup operation with specified policy\"\"\"\n        backup_request = {\n            \"workflow\": \"schedule_backup\",\n            \"parameters\": {\n                \"volume_id\": volume_id,\n                \"backup_type\": backup_policy.get(\"type\", \"snapshot\"),\n                \"retention_period\": backup_policy.get(\"retention\", \"30_days\"),\n                \"consistency_group\": backup_policy.get(\"consistency_group\", True),\n                \"schedule\": backup_policy.get(\"schedule\", \"daily\")\n            }\n        }\n\n        response = await self.apim.execute_temporal_workflow(backup_request)\n        return response.backup_schedule\n\n    async def initiate_recovery(self, recovery_request: dict):\n        \"\"\"Initiate a data recovery operation\"\"\"\n        recovery_workflow = {\n            \"workflow\": \"data_recovery\",\n            \"parameters\": {\n                \"recovery_type\": recovery_request[\"type\"],\n                \"source_backup\": recovery_request[\"backup_id\"],\n                \"target_location\": recovery_request[\"target\"],\n                \"recovery_point\": recovery_request.get(\"point_in_time\"),\n                \"validate_integrity\": True,\n                \"notify_completion\": True\n            }\n        }\n\n        return await self.apim.execute_temporal_workflow(recovery_workflow)\n\n    async def get_backup_status(self, cluster_id: str, timeframe_days: int = 7):\n        \"\"\"Get comprehensive backup status for cluster\"\"\"\n        status_request = {\n            \"workflow\": \"backup_status_report\",\n            \"parameters\": {\n                \"cluster_id\": cluster_id,\n                \"start_date\": (datetime.utcnow() - timedelta(days=timeframe_days)).isoformat(),\n                \"end_date\": datetime.utcnow().isoformat(),\n                \"include_metrics\": True,\n                \"include_failures\": True\n            }\n        }\n\n        response = await self.apim.execute_temporal_workflow(status_request)\n        return response.backup_status\n</code></pre>"},{"location":"use-cases/backup-recovery/#automated-recovery-workflows","title":"Automated Recovery Workflows","text":"<pre><code>class AutomatedRecovery:\n    async def setup_recovery_automation(self):\n        \"\"\"Configure automated recovery triggers\"\"\"\n\n        # File-level recovery automation\n        await self.apim.register_recovery_handler({\n            \"trigger_type\": \"file_corruption_detected\",\n            \"recovery_action\": \"automatic_file_restore\",\n            \"approval_required\": False,\n            \"max_file_size\": \"1GB\",\n            \"notification_channels\": [\"slack\", \"email\"]\n        })\n\n        # Volume recovery automation\n        await self.apim.register_recovery_handler({\n            \"trigger_type\": \"volume_failure\",\n            \"recovery_action\": \"volume_restore_from_snapshot\",\n            \"approval_required\": True,\n            \"approver_role\": \"storage_admin\",\n            \"escalation_timeout\": \"15_minutes\"\n        })\n\n        # Disaster recovery automation\n        await self.apim.register_recovery_handler({\n            \"trigger_type\": \"site_failure\",\n            \"recovery_action\": \"disaster_recovery_failover\",\n            \"approval_required\": True,\n            \"approver_role\": \"disaster_recovery_manager\",\n            \"escalation_chain\": [\"team_lead\", \"operations_manager\", \"cto\"]\n        })\n\n    async def execute_smart_recovery(self, failure_context):\n        \"\"\"Execute AI-recommended recovery strategy\"\"\"\n        recovery_analysis = await self.ai_assistant.analyze_failure(\n            failure_context=failure_context,\n            available_backups=await self.get_available_backups(),\n            business_impact=await self.assess_business_impact()\n        )\n\n        recommended_strategy = recovery_analysis.recommended_strategy\n\n        if recommended_strategy.confidence_score &gt; 0.8:\n            # High confidence - execute automatically\n            return await self.apim.execute_temporal_workflow({\n                \"workflow\": recommended_strategy.workflow,\n                \"parameters\": recommended_strategy.parameters,\n                \"auto_approve\": True\n            })\n        else:\n            # Lower confidence - require approval\n            return await self.apim.submit_for_approval({\n                \"workflow\": recommended_strategy.workflow,\n                \"parameters\": recommended_strategy.parameters,\n                \"recommendation_context\": recovery_analysis,\n                \"approver_role\": \"storage_admin\"\n            })\n</code></pre>"},{"location":"use-cases/backup-recovery/#ai-enhanced-day-2-operations","title":"AI-Enhanced Day-2 Operations","text":""},{"location":"use-cases/backup-recovery/#intelligent-backup-optimization","title":"Intelligent Backup Optimization","text":"<p>The AI Assistant provides advanced backup and recovery capabilities:</p> <ul> <li>Backup Optimization: Analyze backup patterns to optimize schedules and retention</li> <li>Failure Prediction: Predict potential backup failures before they occur</li> <li>Recovery Planning: Generate optimal recovery strategies based on failure scenarios</li> <li>Compliance Monitoring: Ensure backup policies meet regulatory requirements</li> </ul>"},{"location":"use-cases/backup-recovery/#ai-backup-analytics-pipeline","title":"AI Backup Analytics Pipeline","text":"<pre><code>class AIBackupAnalytics:\n    async def optimize_backup_strategy(self, cluster_metrics):\n        \"\"\"AI-driven backup strategy optimization\"\"\"\n\n        # Analyze current backup performance\n        backup_analysis = await self.ai_assistant.analyze_backup_patterns(\n            cluster_metrics=cluster_metrics,\n            historical_data=\"90_days\",\n            include_failures=True\n        )\n\n        # Generate optimization recommendations\n        optimizations = await self.ai_assistant.generate_backup_optimizations(\n            current_analysis=backup_analysis,\n            business_requirements=await self.get_business_requirements(),\n            compliance_requirements=await self.get_compliance_requirements()\n        )\n\n        # Implement approved optimizations\n        for optimization in optimizations.approved_recommendations:\n            await self.apim.execute_temporal_workflow({\n                \"workflow\": \"update_backup_policy\",\n                \"parameters\": {\n                    \"policy_changes\": optimization.policy_changes,\n                    \"effective_date\": optimization.implementation_date,\n                    \"rollback_plan\": optimization.rollback_procedure\n                }\n            })\n\n        return optimizations\n\n    async def predict_backup_failures(self, system_metrics):\n        \"\"\"Predict potential backup failures\"\"\"\n        failure_prediction = await self.ai_assistant.predict_failures(\n            system_metrics=system_metrics,\n            backup_history=await self.get_backup_history(),\n            environmental_factors=await self.get_system_health()\n        )\n\n        # Proactive remediation for high-risk scenarios\n        for prediction in failure_prediction.high_risk_scenarios:\n            if prediction.confidence_score &gt; 0.75:\n                await self.apim.execute_temporal_workflow({\n                    \"workflow\": \"preventive_backup_maintenance\",\n                    \"parameters\": {\n                        \"target_system\": prediction.affected_system,\n                        \"remediation_actions\": prediction.recommended_actions,\n                        \"urgency\": prediction.urgency_level\n                    }\n                })\n\n        return failure_prediction\n</code></pre>"},{"location":"use-cases/backup-recovery/#predictive-recovery-planning","title":"Predictive Recovery Planning","text":"<pre><code>predictive_recovery_workflows:\n  - name: failure_scenario_modeling\n    trigger: weekly\n    ai_model: scenario_simulation\n    inputs:\n      - system_topology\n      - backup_inventory\n      - business_criticality_matrix\n    outputs:\n      - recovery_time_estimates\n      - resource_requirement_projections\n      - risk_mitigation_strategies\n\n  - name: recovery_plan_optimization\n    trigger: configuration_change\n    ai_model: optimization_engine\n    optimization_goals:\n      - minimize_rto\n      - minimize_rpo\n      - optimize_resource_utilization\n    constraints:\n      - budget_limitations\n      - compliance_requirements\n      - business_continuity_needs\n</code></pre>"},{"location":"use-cases/backup-recovery/#backup-recovery-best-practices","title":"Backup &amp; Recovery Best Practices","text":""},{"location":"use-cases/backup-recovery/#1-backup-strategy-design","title":"1. Backup Strategy Design","text":"<ul> <li>3-2-1 Rule: 3 copies of data, 2 different media types, 1 offsite copy</li> <li>Tiered Backup: Different protection levels based on data criticality</li> <li>Application Consistency: Ensure backups are application-consistent</li> <li>Regular Testing: Regularly test backup and recovery procedures</li> </ul>"},{"location":"use-cases/backup-recovery/#2-recovery-planning","title":"2. Recovery Planning","text":"<ul> <li>RTO/RPO Definition: Clearly define recovery objectives for each application</li> <li>Recovery Prioritization: Establish recovery priority based on business impact</li> <li>Automation: Automate recovery procedures where possible</li> <li>Documentation: Maintain comprehensive recovery documentation</li> </ul>"},{"location":"use-cases/backup-recovery/#3-monitoring-and-alerting","title":"3. Monitoring and Alerting","text":"<ul> <li>Backup Success Monitoring: Monitor all backup operations for success/failure</li> <li>Performance Tracking: Track backup and recovery performance metrics</li> <li>Capacity Planning: Monitor backup storage capacity and growth trends</li> <li>Compliance Reporting: Generate reports for compliance and audit purposes</li> </ul>"},{"location":"use-cases/backup-recovery/#compliance-and-governance","title":"Compliance and Governance","text":""},{"location":"use-cases/backup-recovery/#regulatory-compliance","title":"Regulatory Compliance","text":"<pre><code>compliance_frameworks:\n  - framework: SOX\n    requirements:\n      - backup_retention: 7_years\n      - audit_trail: complete\n      - recovery_testing: quarterly\n      - documentation: comprehensive\n\n  - framework: HIPAA\n    requirements:\n      - encryption: aes_256\n      - access_controls: role_based\n      - audit_logging: detailed\n      - business_associate_agreements: required\n\n  - framework: GDPR\n    requirements:\n      - data_minimization: enforced\n      - right_to_erasure: supported\n      - breach_notification: 72_hours\n      - privacy_by_design: implemented\n</code></pre>"},{"location":"use-cases/backup-recovery/#audit-and-reporting","title":"Audit and Reporting","text":"<pre><code>class ComplianceReporting:\n    async def generate_compliance_report(self, framework: str, period: str):\n        \"\"\"Generate compliance report for specified framework\"\"\"\n        report_request = {\n            \"workflow\": \"compliance_report_generation\",\n            \"parameters\": {\n                \"compliance_framework\": framework,\n                \"reporting_period\": period,\n                \"include_metrics\": True,\n                \"include_exceptions\": True,\n                \"include_remediation_plans\": True\n            }\n        }\n\n        response = await self.apim.execute_temporal_workflow(report_request)\n        return response.compliance_report\n\n    async def audit_backup_policies(self, audit_scope: dict):\n        \"\"\"Audit backup policies for compliance\"\"\"\n        audit_request = {\n            \"workflow\": \"backup_policy_audit\",\n            \"parameters\": {\n                \"audit_scope\": audit_scope,\n                \"compliance_standards\": audit_scope.get(\"standards\"),\n                \"automated_remediation\": True,\n                \"generate_findings_report\": True\n            }\n        }\n\n        return await self.apim.execute_temporal_workflow(audit_request)\n</code></pre>"},{"location":"use-cases/backup-recovery/#recovery-testing-framework","title":"Recovery Testing Framework","text":""},{"location":"use-cases/backup-recovery/#automated-testing-scenarios","title":"Automated Testing Scenarios","text":"<pre><code>recovery_test_scenarios:\n  - scenario: file_level_recovery\n    frequency: weekly\n    automation_level: full\n    test_cases:\n      - single_file_recovery\n      - multiple_file_recovery\n      - large_file_recovery\n      - cross_volume_recovery\n    success_criteria:\n      - recovery_time: less_than_5_minutes\n      - data_integrity: 100%\n      - application_functionality: verified\n\n  - scenario: volume_recovery\n    frequency: monthly\n    automation_level: semi_automated\n    test_cases:\n      - full_volume_restore\n      - partial_volume_restore\n      - cross_cluster_restore\n    success_criteria:\n      - recovery_time: less_than_30_minutes\n      - data_consistency: verified\n      - application_startup: successful\n\n  - scenario: disaster_recovery\n    frequency: quarterly\n    automation_level: orchestrated\n    test_cases:\n      - site_failover\n      - application_failover\n      - data_center_evacuation\n    success_criteria:\n      - rto_compliance: verified\n      - rpo_compliance: verified\n      - business_continuity: maintained\n</code></pre>"},{"location":"use-cases/backup-recovery/#troubleshooting-guide","title":"Troubleshooting Guide","text":""},{"location":"use-cases/backup-recovery/#common-backup-issues","title":"Common Backup Issues","text":"<ol> <li>Backup Failures</li> <li>Check storage space availability</li> <li>Verify network connectivity</li> <li>Review SnapMirror relationships</li> <li> <p>Analyze error logs and messages</p> </li> <li> <p>Slow Backup Performance</p> </li> <li>Optimize backup schedules</li> <li>Review network bandwidth</li> <li>Check storage performance</li> <li> <p>Consider backup deduplication</p> </li> <li> <p>Recovery Issues</p> </li> <li>Verify backup integrity</li> <li>Check target system capacity</li> <li>Review recovery procedures</li> <li>Validate application dependencies</li> </ol>"},{"location":"use-cases/backup-recovery/#recovery-validation","title":"Recovery Validation","text":"<ul> <li>Data Integrity Checks: Verify restored data matches original</li> <li>Application Testing: Ensure applications function correctly</li> <li>Performance Validation: Confirm system performance meets requirements</li> <li>User Acceptance Testing: Validate user functionality and access</li> </ul>"},{"location":"use-cases/backup-recovery/#success-metrics","title":"Success Metrics","text":"<ul> <li>Backup Success Rate: Percentage of successful backup operations</li> <li>Recovery Time Objective (RTO): Actual vs. target recovery times</li> <li>Recovery Point Objective (RPO): Actual vs. target data loss</li> <li>Mean Time to Recovery (MTTR): Average time to complete recoveries</li> <li>Backup Storage Efficiency: Deduplication and compression ratios</li> <li>Compliance Score: Percentage of compliance requirements met</li> <li>Test Success Rate: Percentage of successful recovery tests</li> </ul> <p>This comprehensive backup and recovery framework enables DevOps teams to ensure robust data protection, efficient recovery operations, and compliance with business and regulatory requirements through automated, intelligent, and thoroughly tested procedures.</p>"},{"location":"use-cases/capacity-planning/","title":"Capacity Planning","text":""},{"location":"use-cases/capacity-planning/#overview","title":"Overview","text":"<p>Capacity Planning is a strategic DevOps use case that leverages the NetApp ActiveIQ MCP server through APIM to proactively manage storage capacity, predict future growth, and optimize resource allocation. This use case demonstrates how DevOps teams can implement data-driven capacity planning through AI-enhanced analytics, automated monitoring, and predictive modeling.</p>"},{"location":"use-cases/capacity-planning/#architecture-flow","title":"Architecture Flow","text":"<pre><code>sequenceDiagram\n    participant DevOps as DevOps GUI\n    participant APIM as API Management (APIM)\n    participant Temporal as Temporal Workflows\n    participant MCP as MCP Server (Optional)\n    participant NetApp as NetApp ActiveIQ APIs\n    participant AI as AI Assistant (Day-2)\n    participant Analytics as Analytics Engine\n\n    DevOps-&gt;&gt;APIM: Request Capacity Analysis\n    APIM-&gt;&gt;Temporal: Trigger Capacity Planning Workflow\n    Temporal-&gt;&gt;MCP: Optional: Enhanced Capacity Context\n    Temporal-&gt;&gt;NetApp: Collect Capacity Metrics\n    NetApp--&gt;&gt;Temporal: Historical &amp; Current Data\n    Temporal-&gt;&gt;Analytics: Process Capacity Trends\n    Analytics--&gt;&gt;Temporal: Trend Analysis Results\n    Temporal-&gt;&gt;AI: Generate Capacity Predictions\n    AI--&gt;&gt;Temporal: Growth Forecasts &amp; Recommendations\n    Temporal-&gt;&gt;APIM: Capacity Planning Report\n    APIM--&gt;&gt;DevOps: Planning Dashboard &amp; Alerts\n\n    Note over AI: Continuous Learning\n    AI-&gt;&gt;Temporal: Updated Prediction Models\n    AI-&gt;&gt;DevOps: Proactive Capacity Alerts</code></pre>"},{"location":"use-cases/capacity-planning/#capacity-planning-categories","title":"Capacity Planning Categories","text":""},{"location":"use-cases/capacity-planning/#1-storage-capacity-types","title":"1. Storage Capacity Types","text":"<ul> <li>Raw Capacity: Physical storage space available</li> <li>Usable Capacity: Space available after RAID overhead</li> <li>Allocated Capacity: Space assigned to volumes and LUNs</li> <li>Used Capacity: Actual data stored</li> <li>Available Capacity: Remaining usable space</li> </ul>"},{"location":"use-cases/capacity-planning/#2-planning-horizons","title":"2. Planning Horizons","text":"<ul> <li>Short-term (1-3 months): Immediate capacity needs</li> <li>Medium-term (3-12 months): Budget planning and procurement</li> <li>Long-term (1-3 years): Strategic capacity roadmap</li> <li>Emergency planning: Rapid capacity expansion scenarios</li> </ul>"},{"location":"use-cases/capacity-planning/#3-growth-patterns","title":"3. Growth Patterns","text":"<ul> <li>Linear Growth: Steady, predictable capacity increase</li> <li>Exponential Growth: Accelerating capacity consumption</li> <li>Seasonal Growth: Cyclical capacity usage patterns</li> <li>Event-driven Growth: Capacity spikes due to specific events</li> </ul>"},{"location":"use-cases/capacity-planning/#apim-managed-capacity-workflows","title":"APIM-Managed Capacity Workflows","text":""},{"location":"use-cases/capacity-planning/#1-automated-capacity-monitoring","title":"1. Automated Capacity Monitoring","text":"<pre><code>workflow_name: capacity_monitoring\ntrigger: scheduled\nfrequency: hourly\nscope: all_clusters\nsteps:\n  - data_collection:\n      metrics: [used_capacity, available_capacity, growth_rate]\n      aggregation_levels: [cluster, svm, volume, lun]\n      historical_depth: 90_days\n  - trend_analysis:\n      growth_rate_calculation: true\n      seasonal_pattern_detection: true\n      anomaly_identification: true\n  - threshold_evaluation:\n      warning_threshold: 75%\n      critical_threshold: 85%\n      emergency_threshold: 95%\n  - alert_generation:\n      immediate_alerts: critical_thresholds\n      forecast_alerts: projected_exhaustion\n      trend_alerts: unusual_patterns\n</code></pre>"},{"location":"use-cases/capacity-planning/#2-predictive-capacity-analysis","title":"2. Predictive Capacity Analysis","text":"<pre><code>workflow_name: predictive_capacity_analysis\ntrigger: daily\nai_integration: true\nprediction_models:\n  - linear_regression\n  - exponential_smoothing\n  - seasonal_decomposition\n  - machine_learning_ensemble\nsteps:\n  - feature_engineering:\n      time_series_features: true\n      external_factors: [business_events, seasonal_patterns]\n      growth_acceleration: true\n  - model_execution:\n      ensemble_prediction: true\n      confidence_intervals: true\n      scenario_modeling: [best_case, worst_case, most_likely]\n  - recommendation_generation:\n      procurement_timing: true\n      capacity_optimization: true\n      cost_analysis: true\n</code></pre>"},{"location":"use-cases/capacity-planning/#3-capacity-optimization-workflows","title":"3. Capacity Optimization Workflows","text":"<pre><code>workflow_name: capacity_optimization\ntrigger: weekly\noptimization_targets:\n  - storage_efficiency\n  - cost_optimization\n  - performance_balance\nsteps:\n  - efficiency_analysis:\n      deduplication_opportunities: true\n      compression_benefits: true\n      thin_provisioning_optimization: true\n  - rebalancing_recommendations:\n      volume_migration_suggestions: true\n      tier_optimization: true\n      aggregate_rebalancing: true\n  - cost_optimization:\n      license_optimization: true\n      hardware_utilization: true\n      cloud_tier_opportunities: true\n</code></pre>"},{"location":"use-cases/capacity-planning/#devops-integration-patterns","title":"DevOps Integration Patterns","text":""},{"location":"use-cases/capacity-planning/#capacity-planning-dashboard","title":"Capacity Planning Dashboard","text":"<pre><code># Example: Capacity planning integration\nfrom netapp_mcp_client import NetAppMCPClient\nfrom apim_client import APIMClient\nfrom datetime import datetime, timedelta\n\nclass CapacityPlanner:\n    def __init__(self):\n        self.apim = APIMClient()\n        self.mcp_client = NetAppMCPClient()\n\n    async def get_capacity_forecast(self, cluster_id: str, forecast_days: int = 90):\n        \"\"\"Generate capacity forecast for specified timeframe\"\"\"\n        forecast_request = {\n            \"workflow\": \"capacity_forecasting\",\n            \"parameters\": {\n                \"cluster_id\": cluster_id,\n                \"forecast_horizon\": f\"{forecast_days}_days\",\n                \"prediction_models\": [\"linear\", \"exponential\", \"ml_ensemble\"],\n                \"confidence_level\": 95,\n                \"include_scenarios\": True\n            }\n        }\n\n        response = await self.apim.execute_temporal_workflow(forecast_request)\n        return response.capacity_forecast\n\n    async def analyze_capacity_trends(self, timeframe_days: int = 30):\n        \"\"\"Analyze capacity trends across all monitored systems\"\"\"\n        trend_request = {\n            \"workflow\": \"capacity_trend_analysis\",\n            \"parameters\": {\n                \"analysis_period\": f\"{timeframe_days}_days\",\n                \"aggregation_levels\": [\"cluster\", \"svm\", \"volume\"],\n                \"include_growth_rates\": True,\n                \"detect_anomalies\": True,\n                \"generate_insights\": True\n            }\n        }\n\n        response = await self.apim.execute_temporal_workflow(trend_request)\n        return response.trend_analysis\n\n    async def get_optimization_recommendations(self, cluster_id: str):\n        \"\"\"Get AI-powered capacity optimization recommendations\"\"\"\n        optimization_request = {\n            \"workflow\": \"capacity_optimization_analysis\",\n            \"parameters\": {\n                \"cluster_id\": cluster_id,\n                \"optimization_goals\": [\"efficiency\", \"cost\", \"performance\"],\n                \"include_migration_suggestions\": True,\n                \"include_cost_analysis\": True,\n                \"risk_tolerance\": \"medium\"\n            }\n        }\n\n        response = await self.apim.execute_temporal_workflow(optimization_request)\n        return response.optimization_recommendations\n</code></pre>"},{"location":"use-cases/capacity-planning/#automated-capacity-management","title":"Automated Capacity Management","text":"<pre><code>class AutomatedCapacityManager:\n    async def setup_capacity_automation(self):\n        \"\"\"Configure automated capacity management\"\"\"\n\n        # Automated thin provisioning optimization\n        await self.apim.register_capacity_handler({\n            \"trigger_type\": \"low_efficiency_detected\",\n            \"action\": \"thin_provisioning_optimization\",\n            \"auto_execute\": True,\n            \"approval_required\": False,\n            \"notification_channels\": [\"slack\", \"email\"]\n        })\n\n        # Automated volume migration for balance\n        await self.apim.register_capacity_handler({\n            \"trigger_type\": \"aggregate_imbalance\",\n            \"action\": \"volume_migration_recommendation\",\n            \"auto_execute\": False,\n            \"approval_required\": True,\n            \"approver_role\": \"storage_admin\"\n        })\n\n        # Emergency capacity provisioning\n        await self.apim.register_capacity_handler({\n            \"trigger_type\": \"critical_capacity_threshold\",\n            \"action\": \"emergency_capacity_provisioning\",\n            \"auto_execute\": True,\n            \"approval_required\": True,\n            \"approver_role\": \"capacity_manager\",\n            \"escalation_timeout\": \"10_minutes\"\n        })\n\n    async def execute_capacity_expansion(self, expansion_plan):\n        \"\"\"Execute automated capacity expansion\"\"\"\n        expansion_workflow = {\n            \"workflow\": \"capacity_expansion\",\n            \"parameters\": {\n                \"expansion_type\": expansion_plan[\"type\"],\n                \"target_capacity\": expansion_plan[\"target_capacity\"],\n                \"expansion_timeline\": expansion_plan[\"timeline\"],\n                \"validation_checks\": True,\n                \"rollback_plan\": expansion_plan.get(\"rollback_plan\")\n            }\n        }\n\n        return await self.apim.execute_temporal_workflow(expansion_workflow)\n</code></pre>"},{"location":"use-cases/capacity-planning/#ai-enhanced-day-2-operations","title":"AI-Enhanced Day-2 Operations","text":""},{"location":"use-cases/capacity-planning/#intelligent-capacity-forecasting","title":"Intelligent Capacity Forecasting","text":"<p>The AI Assistant provides advanced capacity planning capabilities:</p> <ul> <li>Growth Pattern Recognition: Automatically identify capacity growth patterns</li> <li>Anomaly Detection: Detect unusual capacity consumption patterns</li> <li>Predictive Modeling: Generate accurate capacity forecasts using ML models</li> <li>Optimization Recommendations: Suggest capacity optimization strategies</li> </ul>"},{"location":"use-cases/capacity-planning/#ai-capacity-analytics-pipeline","title":"AI Capacity Analytics Pipeline","text":"<pre><code>class AICapacityAnalytics:\n    async def predict_capacity_needs(self, cluster_metrics):\n        \"\"\"AI-driven capacity need prediction\"\"\"\n\n        # Analyze historical capacity patterns\n        capacity_analysis = await self.ai_assistant.analyze_capacity_patterns(\n            cluster_metrics=cluster_metrics,\n            historical_period=\"12_months\",\n            include_seasonal_patterns=True\n        )\n\n        # Generate capacity predictions\n        predictions = await self.ai_assistant.generate_capacity_predictions(\n            capacity_analysis=capacity_analysis,\n            prediction_horizons=[\"30_days\", \"90_days\", \"365_days\"],\n            confidence_levels=[80, 90, 95]\n        )\n\n        # Optimize capacity allocation\n        optimizations = await self.ai_assistant.optimize_capacity_allocation(\n            current_state=cluster_metrics,\n            predictions=predictions,\n            business_constraints=await self.get_business_constraints()\n        )\n\n        # Execute approved optimizations\n        for optimization in optimizations.approved_actions:\n            await self.apim.execute_temporal_workflow({\n                \"workflow\": optimization.workflow,\n                \"parameters\": optimization.parameters,\n                \"ai_confidence\": optimization.confidence_score\n            })\n\n        return {\n            \"capacity_analysis\": capacity_analysis,\n            \"predictions\": predictions,\n            \"optimizations\": optimizations\n        }\n\n    async def detect_capacity_anomalies(self, system_metrics):\n        \"\"\"Detect capacity usage anomalies\"\"\"\n        anomaly_detection = await self.ai_assistant.detect_anomalies(\n            metrics=system_metrics,\n            detection_algorithms=[\"statistical\", \"ml_based\", \"pattern_matching\"],\n            sensitivity_level=\"medium\"\n        )\n\n        # Investigate detected anomalies\n        for anomaly in anomaly_detection.high_priority_anomalies:\n            investigation_result = await self.ai_assistant.investigate_anomaly(\n                anomaly=anomaly,\n                context_data=await self.get_system_context(),\n                root_cause_analysis=True\n            )\n\n            # Take appropriate action based on investigation\n            if investigation_result.requires_immediate_action:\n                await self.apim.execute_temporal_workflow({\n                    \"workflow\": \"anomaly_response\",\n                    \"parameters\": {\n                        \"anomaly_context\": investigation_result,\n                        \"response_urgency\": \"high\",\n                        \"auto_approve\": investigation_result.confidence &gt; 0.9\n                    }\n                })\n\n        return anomaly_detection\n</code></pre>"},{"location":"use-cases/capacity-planning/#predictive-capacity-planning","title":"Predictive Capacity Planning","text":"<pre><code>predictive_capacity_workflows:\n  - name: growth_pattern_analysis\n    trigger: weekly\n    ai_model: time_series_decomposition\n    features:\n      - historical_usage_data\n      - business_activity_metrics\n      - seasonal_indicators\n    outputs:\n      - growth_trend_analysis\n      - seasonal_pattern_identification\n      - anomaly_detection_results\n\n  - name: capacity_demand_forecasting\n    trigger: monthly\n    ai_model: ensemble_forecasting\n    prediction_horizons: [30, 90, 180, 365]\n    confidence_levels: [80, 90, 95]\n    scenarios: [conservative, optimistic, pessimistic]\n    outputs:\n      - capacity_demand_forecasts\n      - procurement_recommendations\n      - budget_planning_data\n\n  - name: optimization_strategy_generation\n    trigger: configuration_change\n    ai_model: multi_objective_optimization\n    objectives:\n      - minimize_cost\n      - maximize_efficiency\n      - maintain_performance\n    constraints:\n      - budget_limitations\n      - hardware_availability\n      - business_requirements\n</code></pre>"},{"location":"use-cases/capacity-planning/#capacity-planning-best-practices","title":"Capacity Planning Best Practices","text":""},{"location":"use-cases/capacity-planning/#1-data-collection-strategy","title":"1. Data Collection Strategy","text":"<ul> <li>Comprehensive Monitoring: Monitor all capacity metrics across all systems</li> <li>Historical Data Retention: Maintain sufficient historical data for trend analysis</li> <li>Granular Metrics: Collect data at appropriate granularity levels</li> <li>Real-time Alerting: Implement real-time capacity threshold monitoring</li> </ul>"},{"location":"use-cases/capacity-planning/#2-forecasting-methodology","title":"2. Forecasting Methodology","text":"<ul> <li>Multiple Models: Use ensemble forecasting with multiple prediction models</li> <li>Scenario Planning: Consider best-case, worst-case, and most-likely scenarios</li> <li>Regular Model Updates: Continuously update and retrain prediction models</li> <li>Validation and Calibration: Regularly validate forecast accuracy</li> </ul>"},{"location":"use-cases/capacity-planning/#3-optimization-strategies","title":"3. Optimization Strategies","text":"<ul> <li>Efficiency Maximization: Optimize storage efficiency through deduplication and compression</li> <li>Cost Optimization: Balance capacity costs with performance requirements</li> <li>Performance Considerations: Ensure capacity planning doesn't compromise performance</li> <li>Scalability Planning: Plan for both vertical and horizontal scaling options</li> </ul>"},{"location":"use-cases/capacity-planning/#capacity-thresholds-and-alerting","title":"Capacity Thresholds and Alerting","text":""},{"location":"use-cases/capacity-planning/#threshold-configuration","title":"Threshold Configuration","text":"<pre><code>capacity_thresholds:\n  warning_levels:\n    - threshold: 75%\n      alert_frequency: daily\n      notification_channels: [email]\n      recipients: [storage_team]\n\n    - threshold: 80%\n      alert_frequency: every_4_hours\n      notification_channels: [slack, email]\n      recipients: [storage_team, devops_team]\n\n  critical_levels:\n    - threshold: 85%\n      alert_frequency: hourly\n      notification_channels: [pagerduty, slack, email]\n      recipients: [oncall_engineer, storage_team]\n\n    - threshold: 90%\n      alert_frequency: every_15_minutes\n      notification_channels: [pagerduty, phone, slack]\n      recipients: [oncall_engineer, storage_manager]\n\n  emergency_levels:\n    - threshold: 95%\n      alert_frequency: immediate\n      notification_channels: [pagerduty, phone, slack, email]\n      recipients: [oncall_engineer, storage_manager, it_director]\n      auto_actions: [emergency_capacity_provisioning]\n</code></pre>"},{"location":"use-cases/capacity-planning/#predictive-alerting","title":"Predictive Alerting","text":"<pre><code>predictive_alerts:\n  - alert_name: capacity_exhaustion_forecast\n    trigger: predicted_exhaustion_within_30_days\n    confidence_threshold: 80%\n    notification:\n      channels: [email, slack]\n      recipients: [capacity_planning_team]\n      escalation: 7_days_no_action\n\n  - alert_name: growth_acceleration_detected\n    trigger: growth_rate_increase_above_baseline\n    threshold: 50%_increase\n    notification:\n      channels: [slack, email]\n      recipients: [storage_team, business_analysts]\n\n  - alert_name: seasonal_capacity_peak_approaching\n    trigger: seasonal_pattern_analysis\n    advance_notice: 45_days\n    notification:\n      channels: [email]\n      recipients: [capacity_planning_team, procurement_team]\n</code></pre>"},{"location":"use-cases/capacity-planning/#cost-optimization-framework","title":"Cost Optimization Framework","text":""},{"location":"use-cases/capacity-planning/#cost-analysis-components","title":"Cost Analysis Components","text":"<pre><code>class CapacityCostAnalyzer:\n    async def analyze_capacity_costs(self, time_period: str = \"12_months\"):\n        \"\"\"Comprehensive capacity cost analysis\"\"\"\n        cost_analysis_request = {\n            \"workflow\": \"capacity_cost_analysis\",\n            \"parameters\": {\n                \"analysis_period\": time_period,\n                \"cost_components\": [\n                    \"hardware_acquisition\",\n                    \"maintenance_contracts\",\n                    \"power_cooling\",\n                    \"datacenter_space\",\n                    \"management_overhead\"\n                ],\n                \"optimization_opportunities\": True,\n                \"roi_calculations\": True\n            }\n        }\n\n        response = await self.apim.execute_temporal_workflow(cost_analysis_request)\n        return response.cost_analysis\n\n    async def generate_procurement_plan(self, capacity_forecast):\n        \"\"\"Generate optimal procurement plan based on forecast\"\"\"\n        procurement_request = {\n            \"workflow\": \"procurement_planning\",\n            \"parameters\": {\n                \"capacity_requirements\": capacity_forecast,\n                \"budget_constraints\": await self.get_budget_constraints(),\n                \"vendor_options\": await self.get_vendor_catalog(),\n                \"optimization_criteria\": [\"cost\", \"performance\", \"scalability\"],\n                \"procurement_timing\": \"optimal\"\n            }\n        }\n\n        return await self.apim.execute_temporal_workflow(procurement_request)\n</code></pre>"},{"location":"use-cases/capacity-planning/#roi-and-business-impact","title":"ROI and Business Impact","text":"<pre><code>cost_optimization_metrics:\n  - metric: cost_per_gb_stored\n    calculation: total_storage_cost / total_stored_capacity\n    target: minimize\n    benchmark: industry_average\n\n  - metric: efficiency_ratio\n    calculation: logical_capacity / physical_capacity\n    target: maximize\n    benchmark: best_practice_ratio\n\n  - metric: capacity_utilization\n    calculation: used_capacity / allocated_capacity\n    target: optimize_range\n    optimal_range: [70%, 85%]\n\n  - metric: growth_cost_efficiency\n    calculation: incremental_cost / incremental_capacity\n    target: minimize\n    trend_monitoring: true\n</code></pre>"},{"location":"use-cases/capacity-planning/#troubleshooting-guide","title":"Troubleshooting Guide","text":""},{"location":"use-cases/capacity-planning/#common-capacity-issues","title":"Common Capacity Issues","text":"<ol> <li>Rapid Capacity Consumption</li> <li>Investigate unexpected data growth</li> <li>Check for data duplication opportunities</li> <li>Review application behavior changes</li> <li> <p>Analyze user access patterns</p> </li> <li> <p>Inefficient Space Utilization</p> </li> <li>Optimize thin provisioning settings</li> <li>Implement or tune deduplication</li> <li>Review compression configurations</li> <li> <p>Consolidate fragmented volumes</p> </li> <li> <p>Inaccurate Capacity Forecasts</p> </li> <li>Validate data collection accuracy</li> <li>Review prediction model parameters</li> <li>Update seasonal pattern definitions</li> <li>Incorporate business change factors</li> </ol>"},{"location":"use-cases/capacity-planning/#capacity-optimization-techniques","title":"Capacity Optimization Techniques","text":"<ul> <li>Storage Efficiency: Enable and optimize deduplication and compression</li> <li>Thin Provisioning: Implement efficient thin provisioning strategies</li> <li>Data Tiering: Optimize data placement across storage tiers</li> <li>Volume Right-sizing: Regularly review and adjust volume sizes</li> </ul>"},{"location":"use-cases/capacity-planning/#success-metrics","title":"Success Metrics","text":"<ul> <li>Forecast Accuracy: Percentage accuracy of capacity predictions</li> <li>Capacity Utilization: Optimal utilization of available storage</li> <li>Cost per GB: Cost efficiency of storage capacity</li> <li>Planning Lead Time: Advance notice for capacity planning decisions</li> <li>Storage Efficiency Ratio: Logical vs. physical capacity efficiency</li> <li>Threshold Breach Frequency: Number of unexpected capacity threshold breaches</li> <li>Procurement Timing Accuracy: Accuracy of procurement timing predictions</li> </ul> <p>This comprehensive capacity planning framework enables DevOps teams to proactively manage storage capacity through data-driven forecasting, intelligent optimization, and automated monitoring, ensuring optimal resource utilization and cost efficiency.</p>"},{"location":"use-cases/decommission_fileshare/","title":"Use Case: Decommissioning a File Share","text":"<p>This sequence diagram shows the process of authenticating, finding a file share, and then deleting it.</p> <pre><code>sequenceDiagram\n    participant AS as Automation Script\n    participant API as NetApp ActiveIQ API\n\n    AS-&gt;&gt;+API: Authenticate (HTTP Basic Auth)\n    API--&gt;&gt;-AS: 200 OK (Authentication Successful)\n\n    Note over AS, API: All subsequent requests are authenticated\n\n    AS-&gt;&gt;+API: GET /storage-provider/file-shares (Find File Share)\n    API--&gt;&gt;-AS: 200 OK (List of File Shares)\n\n    AS-&gt;&gt;+API: DELETE /storage-provider/file-shares/{key} (Delete File Share)\n    API--&gt;&gt;-AS: 202 Accepted (Job Key)\n\n    loop Poll Job Status\n        AS-&gt;&gt;+API: GET /management-server/jobs/{job_key}\n        API--&gt;&gt;-AS: 200 OK (Job Status)\n    end</code></pre>"},{"location":"use-cases/decommission_fileshare/#error-handling","title":"Error Handling","text":"<ul> <li>Authentication Failure (401 Unauthorized): If authentication fails, the script should log the error and terminate. Ensure that the API credentials are correct and have the necessary permissions.</li> <li>File Share Not Found (404 Not Found): If the file share to be deleted is not found, the script should handle the error appropriately. This could mean the file share has already been deleted, which may be acceptable depending on the use case.</li> <li>Forbidden (403 Forbidden): If the user doesn't have permissions to delete the file share, the script should log the error and notify the administrator.</li> <li>File Share In Use (400 Bad Request): If the file share is currently in use (e.g., has active connections), the API may return a 400 error. The script should handle this by either forcing the deletion (if appropriate) or notifying the user to disconnect clients first.</li> <li>Job Failure: The deletion job may fail for various reasons (e.g., dependencies, system constraints). The script should monitor the job status and provide detailed error information if the job fails.</li> <li>Network Errors: Implement retry logic with exponential backoff for transient network errors.</li> </ul>"},{"location":"use-cases/event-management/","title":"Event Management","text":""},{"location":"use-cases/event-management/#overview","title":"Overview","text":"<p>Event Management is a cornerstone DevOps use case that leverages the NetApp ActiveIQ MCP server through APIM to monitor, process, and respond to storage system events. This comprehensive approach enables proactive incident management, automated response workflows, and intelligent event correlation through AI-enhanced day-2 operations.</p>"},{"location":"use-cases/event-management/#architecture-flow","title":"Architecture Flow","text":"<pre><code>sequenceDiagram\n    participant Storage as NetApp Storage Systems\n    participant APIM as API Management (APIM)\n    participant Temporal as Temporal Workflows\n    participant MCP as MCP Server (Optional)\n    participant DevOps as DevOps GUI\n    participant AI as AI Assistant (Day-2)\n    participant Alert as Alert Systems\n\n    Storage-&gt;&gt;APIM: Event Notification\n    APIM-&gt;&gt;Temporal: Trigger Event Processing Workflow\n    Temporal-&gt;&gt;MCP: Optional: Enhanced Event Context\n    Temporal-&gt;&gt;AI: Event Classification &amp; Correlation\n    AI--&gt;&gt;Temporal: Event Severity &amp; Recommendations\n    Temporal-&gt;&gt;Alert: Send Notifications\n    Temporal-&gt;&gt;DevOps: Event Dashboard Update\n\n    Note over DevOps: Manual Intervention (if required)\n    DevOps-&gt;&gt;APIM: Acknowledge/Resolve Event\n    APIM-&gt;&gt;Temporal: Update Event Status\n\n    Note over AI: Continuous Learning\n    AI-&gt;&gt;Temporal: Pattern Recognition Updates\n    AI-&gt;&gt;DevOps: Predictive Event Insights</code></pre>"},{"location":"use-cases/event-management/#event-categories","title":"Event Categories","text":""},{"location":"use-cases/event-management/#1-critical-events","title":"1. Critical Events","text":"<ul> <li>Hardware Failures: Disk failures, controller issues, network problems</li> <li>Data Protection Issues: Backup failures, replication errors</li> <li>Security Events: Unauthorized access attempts, configuration changes</li> <li>Service Outages: Complete system unavailability</li> </ul>"},{"location":"use-cases/event-management/#2-warning-events","title":"2. Warning Events","text":"<ul> <li>Performance Degradation: High latency, reduced throughput</li> <li>Capacity Issues: Low disk space, approaching limits</li> <li>Configuration Changes: Unauthorized or risky modifications</li> <li>Maintenance Windows: Scheduled maintenance notifications</li> </ul>"},{"location":"use-cases/event-management/#3-informational-events","title":"3. Informational Events","text":"<ul> <li>System Status Updates: Normal operation confirmations</li> <li>Scheduled Tasks: Backup completions, maintenance tasks</li> <li>Performance Reports: Regular performance summaries</li> <li>Configuration Backups: Successful configuration saves</li> </ul>"},{"location":"use-cases/event-management/#apim-managed-event-workflows","title":"APIM-Managed Event Workflows","text":""},{"location":"use-cases/event-management/#1-event-ingestion-and-processing","title":"1. Event Ingestion and Processing","text":"<pre><code>workflow_name: event_processing\ntrigger: webhook\nsource: netapp_storage_systems\nsteps:\n  - event_validation:\n      schema_validation: true\n      event_enrichment: true\n  - severity_classification:\n      ai_classification: true\n      predefined_rules: true\n  - correlation_analysis:\n      temporal_window: 5_minutes\n      pattern_matching: true\n  - response_routing:\n      immediate_action: critical_events\n      scheduled_action: warning_events\n      notification_only: informational_events\n</code></pre>"},{"location":"use-cases/event-management/#2-automated-response-workflows","title":"2. Automated Response Workflows","text":"<pre><code>workflow_name: automated_response\ntrigger: event_classified\nconditions:\n  - event_severity: [critical, warning]\n  - auto_response_enabled: true\nsteps:\n  - immediate_actions:\n      critical:\n        - notify_oncall_engineer\n        - create_incident_ticket\n        - execute_remediation_runbook\n      warning:\n        - notify_devops_team\n        - log_event_details\n        - schedule_investigation\n  - escalation_paths:\n      no_acknowledgment_timeout: 15_minutes\n      escalation_levels: [team_lead, manager, director]\n</code></pre>"},{"location":"use-cases/event-management/#3-event-correlation-and-analysis","title":"3. Event Correlation and Analysis","text":"<pre><code>workflow_name: event_correlation\ntrigger: multiple_events\nai_integration: true\nsteps:\n  - pattern_recognition:\n      temporal_correlation: true\n      spatial_correlation: true\n      causality_analysis: true\n  - root_cause_analysis:\n      dependency_mapping: true\n      impact_assessment: true\n  - predictive_analytics:\n      failure_prediction: true\n      cascading_effect_analysis: true\n  - recommendation_generation:\n      preventive_measures: true\n      optimization_suggestions: true\n</code></pre>"},{"location":"use-cases/event-management/#devops-integration-patterns","title":"DevOps Integration Patterns","text":""},{"location":"use-cases/event-management/#event-dashboard-integration","title":"Event Dashboard Integration","text":"<pre><code># Example: Event management integration\nfrom netapp_mcp_client import NetAppMCPClient\nfrom apim_client import APIMClient\nfrom datetime import datetime, timedelta\n\nclass EventManager:\n    def __init__(self):\n        self.apim = APIMClient()\n        self.mcp_client = NetAppMCPClient()\n\n    async def get_active_events(self, severity_filter: list = None):\n        \"\"\"Fetch active events with optional severity filtering\"\"\"\n        workflow_request = {\n            \"workflow\": \"get_active_events\",\n            \"parameters\": {\n                \"severity_filter\": severity_filter or [\"critical\", \"warning\"],\n                \"status\": \"active\",\n                \"include_details\": True\n            }\n        }\n\n        response = await self.apim.execute_temporal_workflow(workflow_request)\n        return response.events\n\n    async def acknowledge_event(self, event_id: str, user_id: str, notes: str = None):\n        \"\"\"Acknowledge an event with user context\"\"\"\n        acknowledgment_request = {\n            \"workflow\": \"acknowledge_event\",\n            \"parameters\": {\n                \"event_id\": event_id,\n                \"acknowledged_by\": user_id,\n                \"acknowledgment_time\": datetime.utcnow().isoformat(),\n                \"notes\": notes\n            }\n        }\n\n        return await self.apim.execute_temporal_workflow(acknowledgment_request)\n\n    async def get_event_timeline(self, cluster_id: str, timeframe_hours: int = 24):\n        \"\"\"Get event timeline for specific cluster\"\"\"\n        timeline_request = {\n            \"workflow\": \"event_timeline_analysis\",\n            \"parameters\": {\n                \"cluster_id\": cluster_id,\n                \"start_time\": (datetime.utcnow() - timedelta(hours=timeframe_hours)).isoformat(),\n                \"end_time\": datetime.utcnow().isoformat(),\n                \"include_correlations\": True\n            }\n        }\n\n        response = await self.apim.execute_temporal_workflow(timeline_request)\n        return response.timeline_data\n</code></pre>"},{"location":"use-cases/event-management/#event-driven-automation","title":"Event-Driven Automation","text":"<pre><code>class EventAutomation:\n    async def setup_event_handlers(self):\n        \"\"\"Configure automated event response handlers\"\"\"\n\n        # Critical event handler\n        await self.apim.register_event_handler({\n            \"event_type\": \"hardware_failure\",\n            \"severity\": \"critical\",\n            \"handler\": \"emergency_response_workflow\",\n            \"auto_execute\": True,\n            \"approval_required\": False\n        })\n\n        # Warning event handler\n        await self.apim.register_event_handler({\n            \"event_type\": \"performance_degradation\",\n            \"severity\": \"warning\",\n            \"handler\": \"performance_investigation_workflow\",\n            \"auto_execute\": True,\n            \"approval_required\": True,\n            \"approver_role\": \"devops_lead\"\n        })\n\n        # Informational event handler\n        await self.apim.register_event_handler({\n            \"event_type\": \"maintenance_completion\",\n            \"severity\": \"info\",\n            \"handler\": \"update_maintenance_log\",\n            \"auto_execute\": True,\n            \"approval_required\": False\n        })\n\n    async def execute_remediation_runbook(self, event_data):\n        \"\"\"Execute automated remediation based on event type\"\"\"\n        runbook_mapping = {\n            \"disk_failure\": \"disk_replacement_workflow\",\n            \"high_cpu\": \"cpu_optimization_workflow\",\n            \"network_issue\": \"network_diagnostics_workflow\",\n            \"backup_failure\": \"backup_retry_workflow\"\n        }\n\n        runbook = runbook_mapping.get(event_data.event_type)\n        if runbook:\n            return await self.apim.execute_temporal_workflow({\n                \"workflow\": runbook,\n                \"parameters\": {\n                    \"event_context\": event_data,\n                    \"cluster_id\": event_data.cluster_id,\n                    \"auto_approve\": event_data.severity == \"critical\"\n                }\n            })\n</code></pre>"},{"location":"use-cases/event-management/#ai-enhanced-day-2-operations","title":"AI-Enhanced Day-2 Operations","text":""},{"location":"use-cases/event-management/#intelligent-event-analysis","title":"Intelligent Event Analysis","text":"<p>The AI Assistant provides advanced event management capabilities:</p> <ul> <li>Event Correlation: Automatically correlate related events across systems</li> <li>Anomaly Detection: Identify unusual event patterns that may indicate issues</li> <li>Predictive Analytics: Predict potential failures based on event history</li> <li>Root Cause Analysis: AI-powered investigation of complex event chains</li> </ul>"},{"location":"use-cases/event-management/#ai-event-processing-pipeline","title":"AI Event Processing Pipeline","text":"<pre><code>class AIEventProcessor:\n    async def process_event_with_ai(self, event_data):\n        \"\"\"AI-enhanced event processing\"\"\"\n\n        # Event classification and enrichment\n        classified_event = await self.ai_assistant.classify_event(\n            event_data=event_data,\n            historical_context=True,\n            system_topology=True\n        )\n\n        # Correlation analysis\n        correlations = await self.ai_assistant.find_correlations(\n            target_event=classified_event,\n            time_window=\"30_minutes\",\n            similarity_threshold=0.7\n        )\n\n        # Impact assessment\n        impact_analysis = await self.ai_assistant.assess_impact(\n            event=classified_event,\n            correlations=correlations,\n            business_context=True\n        )\n\n        # Generate recommendations\n        recommendations = await self.ai_assistant.generate_recommendations(\n            event_analysis=impact_analysis,\n            available_actions=self.get_available_actions(),\n            risk_tolerance=\"medium\"\n        )\n\n        # Execute approved automated responses\n        for recommendation in recommendations.auto_approved:\n            await self.apim.execute_temporal_workflow({\n                \"workflow\": recommendation.workflow,\n                \"parameters\": recommendation.parameters,\n                \"ai_confidence\": recommendation.confidence_score\n            })\n\n        return {\n            \"processed_event\": classified_event,\n            \"correlations\": correlations,\n            \"impact_analysis\": impact_analysis,\n            \"recommendations\": recommendations\n        }\n</code></pre>"},{"location":"use-cases/event-management/#predictive-event-management","title":"Predictive Event Management","text":"<pre><code>predictive_workflows:\n  - name: failure_prediction\n    trigger: daily\n    ai_model: time_series_anomaly_detection\n    features:\n      - hardware_metrics\n      - performance_trends\n      - event_patterns\n    prediction_horizon: 7_days\n    actions:\n      - preventive_maintenance_scheduling\n      - proactive_component_replacement\n      - capacity_planning_updates\n\n  - name: cascade_prevention\n    trigger: critical_event\n    ai_model: dependency_graph_analysis\n    analysis:\n      - impact_propagation_modeling\n      - containment_strategy_generation\n      - resource_allocation_optimization\n    actions:\n      - automated_isolation_procedures\n      - backup_system_activation\n      - stakeholder_notifications\n</code></pre>"},{"location":"use-cases/event-management/#event-response-playbooks","title":"Event Response Playbooks","text":""},{"location":"use-cases/event-management/#critical-event-response","title":"Critical Event Response","text":"<pre><code>playbook_name: critical_event_response\ntrigger_conditions:\n  - severity: critical\n  - auto_response_enabled: true\nimmediate_actions:\n  - duration: 0-5_minutes\n    steps:\n      - notify_oncall_engineer: immediate\n      - create_incident_ticket: high_priority\n      - gather_system_state: comprehensive\n      - execute_containment_procedures: automated\n\nshort_term_actions:\n  - duration: 5-30_minutes\n    steps:\n      - assess_business_impact: ai_assisted\n      - implement_workarounds: temporary_solutions\n      - coordinate_response_team: escalation_procedures\n      - update_stakeholders: regular_intervals\n\nresolution_actions:\n  - duration: 30_minutes+\n    steps:\n      - execute_permanent_fix: tested_solutions\n      - validate_system_recovery: comprehensive_testing\n      - conduct_post_incident_review: lessons_learned\n      - update_documentation: knowledge_base\n</code></pre>"},{"location":"use-cases/event-management/#warning-event-response","title":"Warning Event Response","text":"<pre><code>playbook_name: warning_event_response\ntrigger_conditions:\n  - severity: warning\n  - investigation_required: true\ninvestigation_workflow:\n  - data_collection:\n      - system_metrics: last_24_hours\n      - event_history: correlated_events\n      - performance_data: trend_analysis\n  - analysis:\n      - root_cause_investigation: ai_assisted\n      - impact_assessment: business_context\n      - risk_evaluation: probability_matrix\n  - response_planning:\n      - remediation_options: cost_benefit_analysis\n      - implementation_timeline: resource_availability\n      - rollback_procedures: risk_mitigation\n</code></pre>"},{"location":"use-cases/event-management/#monitoring-and-alerting-configuration","title":"Monitoring and Alerting Configuration","text":""},{"location":"use-cases/event-management/#event-monitoring-setup","title":"Event Monitoring Setup","text":"<pre><code>monitoring_configuration:\n  event_sources:\n    - netapp_clusters: all_production_clusters\n    - storage_vms: all_active_svms\n    - aggregates: all_data_aggregates\n    - volumes: critical_volumes_only\n\n  collection_frequency:\n    - critical_events: real_time\n    - warning_events: 1_minute\n    - informational_events: 5_minutes\n\n  retention_policy:\n    - critical_events: 1_year\n    - warning_events: 6_months\n    - informational_events: 3_months\n</code></pre>"},{"location":"use-cases/event-management/#alert-notification-rules","title":"Alert Notification Rules","text":"<pre><code>notification_rules:\n  - rule_name: critical_hardware_failure\n    conditions:\n      - event_type: hardware_failure\n      - severity: critical\n    notifications:\n      - channel: pagerduty\n        recipients: oncall_engineer\n        escalation: immediate\n      - channel: slack\n        recipients: devops_team\n        escalation: immediate\n      - channel: email\n        recipients: management_team\n        escalation: 5_minutes\n\n  - rule_name: performance_degradation\n    conditions:\n      - event_type: performance_issue\n      - severity: warning\n      - duration: 10_minutes\n    notifications:\n      - channel: slack\n        recipients: devops_team\n        escalation: immediate\n      - channel: email\n        recipients: team_leads\n        escalation: 15_minutes\n</code></pre>"},{"location":"use-cases/event-management/#best-practices","title":"Best Practices","text":""},{"location":"use-cases/event-management/#1-event-management-strategy","title":"1. Event Management Strategy","text":"<ul> <li>Proactive Monitoring: Implement comprehensive event monitoring across all storage systems</li> <li>Intelligent Filtering: Use AI-powered classification to reduce alert fatigue</li> <li>Automated Response: Enable automated responses for well-defined event scenarios</li> <li>Continuous Improvement: Regularly review and update event handling procedures</li> </ul>"},{"location":"use-cases/event-management/#2-response-optimization","title":"2. Response Optimization","text":"<ul> <li>Clear Escalation Paths: Define clear escalation procedures for different event types</li> <li>Documentation: Maintain comprehensive runbooks for common event scenarios</li> <li>Training: Ensure team members are trained on event response procedures</li> <li>Post-Incident Reviews: Conduct thorough reviews to improve future responses</li> </ul>"},{"location":"use-cases/event-management/#3-ai-integration","title":"3. AI Integration","text":"<ul> <li>Model Training: Continuously train AI models with new event data</li> <li>Feedback Loops: Implement feedback mechanisms to improve AI accuracy</li> <li>Human Oversight: Maintain human oversight for critical automated decisions</li> <li>Transparency: Ensure AI decision processes are auditable and explainable</li> </ul>"},{"location":"use-cases/event-management/#success-metrics","title":"Success Metrics","text":"<ul> <li>Mean Time to Detection (MTTD): Average time to detect and classify events</li> <li>Mean Time to Acknowledgment (MTTA): Average time for human acknowledgment</li> <li>Mean Time to Resolution (MTTR): Average time to resolve events</li> <li>False Positive Rate: Percentage of incorrectly classified events</li> <li>Automation Success Rate: Percentage of events successfully handled automatically</li> <li>Escalation Rate: Percentage of events requiring escalation</li> <li>Customer Impact Reduction: Decrease in customer-affecting incidents</li> </ul>"},{"location":"use-cases/event-management/#troubleshooting-guide","title":"Troubleshooting Guide","text":""},{"location":"use-cases/event-management/#common-event-management-issues","title":"Common Event Management Issues","text":"<ol> <li>Alert Fatigue</li> <li>Review and tune event classification rules</li> <li>Implement intelligent event correlation</li> <li>Use AI-powered noise reduction</li> <li> <p>Optimize notification thresholds</p> </li> <li> <p>Missed Critical Events</p> </li> <li>Audit event source configurations</li> <li>Review filtering and routing rules</li> <li>Validate notification delivery mechanisms</li> <li> <p>Test escalation procedures</p> </li> <li> <p>Slow Response Times</p> </li> <li>Analyze response workflow efficiency</li> <li>Optimize automated response triggers</li> <li>Review team availability and coverage</li> <li>Improve documentation and training</li> </ol> <p>This comprehensive event management framework enables DevOps teams to maintain high availability and performance of NetApp storage systems through intelligent, automated, and proactive event handling.</p>"},{"location":"use-cases/expand_lun/","title":"Use Case: Expanding a LUN","text":"<p>This sequence diagram illustrates how to authenticate, find a LUN, and then expand its size.</p> <pre><code>sequenceDiagram\n    participant AS as Automation Script\n    participant API as NetApp ActiveIQ API\n\n    AS-&gt;&gt;+API: Authenticate (HTTP Basic Auth)\n    API--&gt;&gt;-AS: 200 OK (Authentication Successful)\n\n    Note over AS, API: All subsequent requests are authenticated\n\n    AS-&gt;&gt;+API: GET /storage-provider/luns (Find LUN)\n    API--&gt;&gt;-AS: 200 OK (List of LUNs)\n\n    AS-&gt;&gt;+API: PATCH /storage-provider/luns/{key} (Expand LUN)\n    API--&gt;&gt;-AS: 202 Accepted (Job Key)\n\n    loop Poll Job Status\n        AS-&gt;&gt;+API: GET /management-server/jobs/{job_key}\n        API--&gt;&gt;-AS: 200 OK (Job Status)\n    end</code></pre>"},{"location":"use-cases/expand_lun/#error-handling","title":"Error Handling","text":"<ul> <li>Authentication Failure (401 Unauthorized): If authentication fails, the script should log the error and terminate. Ensure that the API credentials are correct and have the necessary permissions.</li> <li>LUN Not Found (404 Not Found): If the LUN to be expanded is not found, the script should handle the error gracefully. This could involve retrying with different search criteria or logging the error.</li> <li>Insufficient Space (400 Bad Request): If there isn't enough free space in the containing aggregate to expand the LUN, the API will return a 400 error. The script should handle this by either finding additional space or notifying the administrator.</li> <li>Invalid Size (400 Bad Request): If the requested new size is invalid (e.g., smaller than the current size, exceeds maximum limits), the API will return a 400 error with details. The script should validate the size before making the request.</li> <li>LUN Online/Offline State: If the LUN is in an inappropriate state for expansion (e.g., offline), the operation may fail. The script should check the LUN state and potentially bring it online before attempting expansion.</li> <li>Job Failure: The expansion job may fail for various reasons (e.g., storage constraints, system issues). The script should monitor the job status and provide detailed error information if the job fails.</li> <li>Host Connectivity Issues: After expanding a LUN, the host may need to rescan or extend the filesystem. While this is outside the scope of the API, the script should provide guidance or automation for these post-expansion steps.</li> <li>Network Errors: Implement retry logic with exponential backoff for transient network errors.</li> </ul>"},{"location":"use-cases/infrastructure-management/","title":"Infrastructure Management Use Cases","text":"<p>Manage and optimize your NetApp storage infrastructure through AI-assisted operations and natural language commands.</p>"},{"location":"use-cases/infrastructure-management/#overview","title":"Overview","text":"<p>NetApp infrastructure management encompasses the complete lifecycle of storage systems, from initial deployment to ongoing operations and optimization. With AI assistant integration, administrators can manage complex infrastructure tasks through natural language interactions.</p>"},{"location":"use-cases/infrastructure-management/#core-management-capabilities","title":"Core Management Capabilities","text":""},{"location":"use-cases/infrastructure-management/#cluster-management","title":"\ud83c\udfd7\ufe0f Cluster Management","text":"<ul> <li>Cluster discovery and onboarding of new NetApp systems</li> <li>Health monitoring and maintenance of cluster operations</li> <li>Version management and upgrade planning</li> <li>Node configuration and optimization</li> </ul>"},{"location":"use-cases/infrastructure-management/#storage-virtual-machine-svm-management","title":"\ud83d\udda5\ufe0f Storage Virtual Machine (SVM) Management","text":"<ul> <li>SVM creation and configuration for multi-tenant environments</li> <li>Protocol configuration (NFS, CIFS, iSCSI, FC)</li> <li>Network interface management and optimization</li> <li>Security and access control configuration</li> </ul>"},{"location":"use-cases/infrastructure-management/#volume-and-storage-management","title":"\ud83d\udcc1 Volume and Storage Management","text":"<ul> <li>Volume provisioning and lifecycle management</li> <li>Storage efficiency optimization (deduplication, compression)</li> <li>Snapshot and backup management</li> <li>Quality of Service (QoS) configuration</li> </ul>"},{"location":"use-cases/infrastructure-management/#network-and-connectivity","title":"\ud83d\udd17 Network and Connectivity","text":"<ul> <li>Network interface configuration</li> <li>Inter-cluster networking setup</li> <li>Protocol optimization and tuning</li> <li>Connectivity troubleshooting</li> </ul>"},{"location":"use-cases/infrastructure-management/#natural-language-management-commands","title":"Natural Language Management Commands","text":""},{"location":"use-cases/infrastructure-management/#cluster-operations","title":"Cluster Operations","text":"<pre><code>\"Add a new cluster to monitoring\"\n\"Show me the version status of all clusters\"\n\"What clusters need firmware updates?\"\n\"Check the health of our production cluster\"\n\"Configure backup schedule for cluster management\"\n</code></pre>"},{"location":"use-cases/infrastructure-management/#svm-management","title":"SVM Management","text":"<pre><code>\"Create a new SVM for the development team\"\n\"Enable NFS protocol on the marketing SVM\"\n\"Show me all SVMs and their protocols\"\n\"Configure DNS settings for the production SVM\"\n\"What SVMs are offline or need attention?\"\n</code></pre>"},{"location":"use-cases/infrastructure-management/#storage-provisioning","title":"Storage Provisioning","text":"<pre><code>\"Create a 500GB volume for the database team\"\n\"Set up a new file share for project documents\"\n\"Configure a LUN for the VMware environment\"\n\"Enable compression on all test volumes\"\n\"Create a snapshot policy for production data\"\n</code></pre>"},{"location":"use-cases/infrastructure-management/#network-configuration","title":"Network Configuration","text":"<pre><code>\"Configure a new data LIF for NFS access\"\n\"Show me all network interfaces and their status\"\n\"Set up inter-cluster networking between sites\"\n\"Troubleshoot network connectivity issues\"\n\"Optimize network settings for performance\"\n</code></pre>"},{"location":"use-cases/infrastructure-management/#infrastructure-management-workflows","title":"Infrastructure Management Workflows","text":""},{"location":"use-cases/infrastructure-management/#1-new-environment-setup","title":"1. New Environment Setup","text":"<p>Scenario: Setting up storage for a new business unit</p> <p>Process: 1. Cluster Assessment: Validate cluster capacity and health 2. SVM Creation: Create dedicated SVM with appropriate protocols 3. Network Configuration: Set up data and management LIFs 4. Storage Provisioning: Create volumes and file shares 5. Security Configuration: Apply access controls and policies 6. Monitoring Setup: Configure alerts and monitoring</p> <p>Natural Language Flow: <pre><code>\"I need to set up storage for the new finance department\"\n\u2192 \"Check available capacity on our clusters\"\n\u2192 \"Create a new SVM called 'finance-svm' with CIFS protocol\"\n\u2192 \"Set up network interfaces for the finance network\"\n\u2192 \"Create volumes for finance applications and user data\"\n\u2192 \"Configure backup and snapshot policies\"\n</code></pre></p>"},{"location":"use-cases/infrastructure-management/#2-capacity-expansion","title":"2. Capacity Expansion","text":"<p>Scenario: Expanding storage capacity for growing data requirements</p> <p>Process: 1. Capacity Analysis: Assess current utilization and growth trends 2. Resource Planning: Identify optimal expansion strategy 3. Hardware Planning: Determine disk additions or new aggregates 4. Implementation: Execute expansion with minimal disruption 5. Validation: Verify capacity and performance post-expansion</p>"},{"location":"use-cases/infrastructure-management/#3-performance-optimization","title":"3. Performance Optimization","text":"<p>Scenario: Optimizing storage performance for critical workloads</p> <p>Process: 1. Performance Assessment: Analyze current performance metrics 2. Bottleneck Identification: Find performance constraints 3. Optimization Planning: Design performance improvements 4. Implementation: Apply optimizations and configurations 5. Monitoring: Track performance improvements</p>"},{"location":"use-cases/infrastructure-management/#4-disaster-recovery-setup","title":"4. Disaster Recovery Setup","text":"<p>Scenario: Implementing disaster recovery and data protection</p> <p>Process: 1. DR Planning: Design disaster recovery strategy 2. Replication Setup: Configure SnapMirror relationships 3. Network Configuration: Set up inter-site connectivity 4. Testing: Validate failover and recovery procedures 5. Documentation: Document DR procedures and schedules</p>"},{"location":"use-cases/infrastructure-management/#automated-infrastructure-operations","title":"Automated Infrastructure Operations","text":""},{"location":"use-cases/infrastructure-management/#daily-operations-automation","title":"Daily Operations Automation","text":"<pre><code># Automated daily infrastructure checks\ndef daily_infrastructure_check():\n    \"\"\"Perform automated daily infrastructure validation\"\"\"\n\n    # Check cluster health\n    clusters = get_all_clusters()\n    unhealthy_clusters = [c for c in clusters if c.health != 'healthy']\n\n    # Validate SVM status\n    svms = get_all_svms()\n    offline_svms = [s for s in svms if s.state != 'running']\n\n    # Check capacity thresholds\n    volumes = get_volumes_above_threshold(80)\n\n    # Network connectivity validation\n    network_issues = check_network_connectivity()\n\n    # Generate summary report\n    return {\n        'timestamp': datetime.now(),\n        'cluster_health': len(unhealthy_clusters) == 0,\n        'svm_status': len(offline_svms) == 0,\n        'capacity_warnings': len(volumes),\n        'network_status': len(network_issues) == 0,\n        'issues': {\n            'clusters': unhealthy_clusters,\n            'svms': offline_svms,\n            'capacity': volumes,\n            'network': network_issues\n        }\n    }\n</code></pre>"},{"location":"use-cases/infrastructure-management/#automated-provisioning","title":"Automated Provisioning","text":"<pre><code># Self-service storage provisioning\ndef provision_storage_for_team(team_name, requirements):\n    \"\"\"Automated storage provisioning workflow\"\"\"\n\n    # Validate requirements\n    validate_storage_requirements(requirements)\n\n    # Find optimal cluster placement\n    target_cluster = find_best_cluster(requirements.size, requirements.performance)\n\n    # Create SVM if needed\n    svm = get_or_create_team_svm(team_name, target_cluster)\n\n    # Provision volumes\n    volumes = create_volumes(svm, requirements.volumes)\n\n    # Configure network access\n    configure_network_access(svm, requirements.networks)\n\n    # Apply security policies\n    apply_security_policies(svm, team_name)\n\n    # Set up monitoring\n    configure_monitoring(svm, volumes)\n\n    return {\n        'svm': svm,\n        'volumes': volumes,\n        'status': 'provisioned',\n        'access_details': get_access_information(svm)\n    }\n</code></pre>"},{"location":"use-cases/infrastructure-management/#infrastructure-templates-and-standards","title":"Infrastructure Templates and Standards","text":""},{"location":"use-cases/infrastructure-management/#standard-svm-configurations","title":"Standard SVM Configurations","text":""},{"location":"use-cases/infrastructure-management/#development-environment","title":"Development Environment","text":"<pre><code>svm_template_dev:\n  protocols: [nfs, cifs]\n  security_style: mixed\n  language: c.utf_8\n  dns_enabled: true\n  snapshot_policy: default\n  export_policy: dev_policy\n  backup_schedule: weekly\n\nnetwork_config:\n  management_lif: true\n  data_lifs: 2\n  load_balancing: enabled\n</code></pre>"},{"location":"use-cases/infrastructure-management/#production-environment","title":"Production Environment","text":"<pre><code>svm_template_prod:\n  protocols: [nfs, cifs, iscsi]\n  security_style: ntfs\n  language: c.utf_8\n  dns_enabled: true\n  ldap_enabled: true\n  snapshot_policy: production\n  export_policy: prod_policy\n  backup_schedule: daily\n\nnetwork_config:\n  management_lif: true\n  data_lifs: 4\n  load_balancing: enabled\n  failover_enabled: true\n</code></pre>"},{"location":"use-cases/infrastructure-management/#volume-provisioning-standards","title":"Volume Provisioning Standards","text":""},{"location":"use-cases/infrastructure-management/#database-volumes","title":"Database Volumes","text":"<pre><code>database_volume_template:\n  size: 1TB\n  guarantee: volume\n  efficiency: enabled\n  compression: adaptive\n  snapshot_reserve: 10%\n  qos_policy: database_high_performance\n  tiering_policy: none\n</code></pre>"},{"location":"use-cases/infrastructure-management/#file-share-volumes","title":"File Share Volumes","text":"<pre><code>fileshare_volume_template:\n  size: 500GB\n  guarantee: none\n  efficiency: enabled\n  compression: background\n  snapshot_reserve: 20%\n  qos_policy: standard\n  tiering_policy: auto\n</code></pre>"},{"location":"use-cases/infrastructure-management/#change-management-and-compliance","title":"Change Management and Compliance","text":""},{"location":"use-cases/infrastructure-management/#configuration-management","title":"Configuration Management","text":"<ul> <li>Version Control: Track all configuration changes</li> <li>Approval Workflows: Require approval for production changes</li> <li>Rollback Procedures: Quick rollback for failed changes</li> <li>Audit Trails: Complete change history and compliance</li> </ul>"},{"location":"use-cases/infrastructure-management/#compliance-monitoring","title":"Compliance Monitoring","text":"<ul> <li>Security Policies: Enforce security configuration standards</li> <li>Best Practices: Validate against NetApp best practices</li> <li>Regulatory Requirements: Meet industry compliance standards</li> <li>Documentation: Maintain configuration documentation</li> </ul>"},{"location":"use-cases/infrastructure-management/#change-approval-process","title":"Change Approval Process","text":"<ol> <li>Change Request: Submit change through approved process</li> <li>Impact Assessment: Analyze potential impact and risks</li> <li>Approval: Get required approvals from stakeholders</li> <li>Implementation: Execute change with proper procedures</li> <li>Validation: Verify change success and performance</li> <li>Documentation: Update configuration documentation</li> </ol>"},{"location":"use-cases/infrastructure-management/#infrastructure-optimization","title":"Infrastructure Optimization","text":""},{"location":"use-cases/infrastructure-management/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Workload Analysis: Understand application requirements</li> <li>Resource Allocation: Optimize CPU, memory, and storage allocation</li> <li>Network Tuning: Optimize network configuration for performance</li> <li>Cache Optimization: Configure and tune storage caching</li> </ul>"},{"location":"use-cases/infrastructure-management/#capacity-optimization","title":"Capacity Optimization","text":"<ul> <li>Storage Efficiency: Maximize deduplication and compression</li> <li>Thin Provisioning: Optimize space utilization</li> <li>Data Tiering: Move data to appropriate storage tiers</li> <li>Lifecycle Management: Automate data lifecycle policies</li> </ul>"},{"location":"use-cases/infrastructure-management/#cost-optimization","title":"Cost Optimization","text":"<ul> <li>Resource Utilization: Identify underutilized resources</li> <li>Right-sizing: Match resources to actual requirements</li> <li>Efficiency Features: Maximize storage efficiency benefits</li> <li>Cloud Integration: Optimize hybrid cloud storage costs</li> </ul>"},{"location":"use-cases/infrastructure-management/#troubleshooting-and-support","title":"Troubleshooting and Support","text":""},{"location":"use-cases/infrastructure-management/#common-infrastructure-issues","title":"Common Infrastructure Issues","text":""},{"location":"use-cases/infrastructure-management/#connectivity-problems","title":"Connectivity Problems","text":"<pre><code>\"Why can't clients connect to the file server?\"\n\u2192 Check network interface status\n\u2192 Validate DNS configuration\n\u2192 Verify firewall and routing\n\u2192 Test client connectivity\n</code></pre>"},{"location":"use-cases/infrastructure-management/#performance-issues","title":"Performance Issues","text":"<pre><code>\"Storage performance is slow, what's wrong?\"\n\u2192 Analyze volume performance metrics\n\u2192 Check aggregate utilization\n\u2192 Review network performance\n\u2192 Identify workload patterns\n</code></pre>"},{"location":"use-cases/infrastructure-management/#capacity-issues","title":"Capacity Issues","text":"<pre><code>\"We're running out of space, what are our options?\"\n\u2192 Analyze current capacity utilization\n\u2192 Identify space-consuming volumes\n\u2192 Review efficiency opportunities\n\u2192 Plan capacity expansion\n</code></pre>"},{"location":"use-cases/infrastructure-management/#support-integration","title":"Support Integration","text":"<ul> <li>NetApp Support: Automatic case creation for hardware issues</li> <li>Knowledge Base: Integration with NetApp knowledge articles</li> <li>Community Support: Access to NetApp community resources</li> <li>Vendor Support: Escalation to vendor technical support</li> </ul>"},{"location":"use-cases/infrastructure-management/#best-practices","title":"Best Practices","text":""},{"location":"use-cases/infrastructure-management/#infrastructure-design","title":"Infrastructure Design","text":"<ol> <li>Plan for Growth: Design with future capacity and performance needs</li> <li>High Availability: Implement redundancy and failover capabilities</li> <li>Security First: Apply security best practices from the start</li> <li>Monitoring: Implement comprehensive monitoring and alerting</li> <li>Documentation: Maintain detailed infrastructure documentation</li> </ol>"},{"location":"use-cases/infrastructure-management/#operational-excellence","title":"Operational Excellence","text":"<ol> <li>Automation: Automate routine tasks and procedures</li> <li>Standardization: Use consistent configurations and templates</li> <li>Change Control: Implement proper change management processes</li> <li>Continuous Improvement: Regular review and optimization</li> <li>Training: Keep team skills current with technology updates</li> </ol> <p>This comprehensive approach to infrastructure management ensures reliable, scalable, and efficient NetApp storage operations through intelligent automation and AI-assisted management.</p>"},{"location":"use-cases/monitor_cluster_performance/","title":"Use Case: Monitoring Cluster Performance","text":"<p>This sequence diagram shows how to authenticate and then retrieve performance metrics for a specific cluster.</p> <pre><code>sequenceDiagram\n    participant AS as Automation Script\n    participant API as NetApp ActiveIQ API\n\n    AS-&gt;&gt;+API: Authenticate (HTTP Basic Auth)\n    API--&gt;&gt;-AS: 200 OK (Authentication Successful)\n\n    Note over AS, API: All subsequent requests are authenticated\n\n    AS-&gt;&gt;+API: GET /datacenter/cluster/clusters (Find Cluster)\n    API--&gt;&gt;-AS: 200 OK (List of Clusters)\n\n    AS-&gt;&gt;+API: GET /datacenter/cluster/clusters/{key}/metrics?interval=1h (Get Cluster Metrics)\n    API--&gt;&gt;-AS: 200 OK (Performance Metrics)</code></pre>"},{"location":"use-cases/monitor_cluster_performance/#error-handling","title":"Error Handling","text":"<ul> <li>Authentication Failure (401 Unauthorized): If authentication fails, the script should log the error and terminate. Ensure that the API credentials are correct and have the necessary permissions.</li> <li>Cluster Not Found (404 Not Found): If the specified cluster is not found, the script should handle the error gracefully. This could involve checking if the cluster name or key is correct, or if the cluster has been removed from management.</li> <li>Invalid Interval Parameter (400 Bad Request): If an invalid interval is specified (e.g., unsupported time range), the API will return a 400 error. The script should validate the interval parameter against supported values (1h, 12h, 1d, 2d, 3d, 15d, 1w, 1m, 2m, 3m, 6m).</li> <li>Metrics Not Available (404 Not Found): If performance metrics are not available for the specified cluster and time range, the API may return a 404 error. The script should handle this by trying a different time range or notifying that metrics are not available.</li> <li>Rate Limiting (429 Too Many Requests): If the script makes too many requests in a short period, the API may return a 429 error. Implement rate limiting and backoff strategies.</li> <li>Internal Server Error (500 Internal Server Error): If the API experiences internal issues while retrieving metrics, the script should implement retry logic with exponential backoff.</li> <li>Data Processing Errors: The script should validate the returned metrics data and handle cases where the data might be incomplete or malformed.</li> <li>Network Errors: Implement retry logic with exponential backoff for transient network errors.</li> <li>Monitoring Loop Failures: If this is part of a continuous monitoring loop, implement circuit breaker patterns to prevent cascading failures.</li> </ul>"},{"location":"use-cases/netapp_api_use_cases_overview/","title":"NetApp ActiveIQ API - Functional Use Cases Overview","text":"<p>This document provides an overview of the key functional use cases for automating NetApp storage operations using the ActiveIQ Unified Manager REST API.</p>"},{"location":"use-cases/netapp_api_use_cases_overview/#authentication","title":"Authentication","text":"<p>All NetApp ActiveIQ API requests require HTTP Basic Authentication. Each automation script must:</p> <ol> <li>Obtain valid credentials (username/password) with appropriate permissions</li> <li>Include the <code>Authorization</code> header in every API request</li> <li>Handle authentication failures gracefully</li> </ol>"},{"location":"use-cases/netapp_api_use_cases_overview/#use-cases","title":"Use Cases","text":""},{"location":"use-cases/netapp_api_use_cases_overview/#1-provisioning-a-new-nfs-file-share","title":"1. Provisioning a New NFS File Share","text":"<p>Objective: Automate the creation of new NFS file shares for clients.</p> <p>Key Steps:</p> <ul> <li>Discover available clusters and SVMs</li> <li>Select appropriate aggregates with sufficient space</li> <li>Create the file share with proper export policies</li> <li>Monitor the creation job until completion</li> </ul> <p>Common Errors: Authentication failures, insufficient space, invalid export policies, job failures.</p>"},{"location":"use-cases/netapp_api_use_cases_overview/#2-decommissioning-a-file-share","title":"2. Decommissioning a File Share","text":"<p>Objective: Safely remove file shares that are no longer needed.</p> <p>Key Steps:</p> <ul> <li>Locate the target file share</li> <li>Verify no active connections (optional)</li> <li>Delete the file share</li> <li>Monitor the deletion job</li> </ul> <p>Common Errors: File share not found, permission issues, active connections, deletion constraints.</p>"},{"location":"use-cases/netapp_api_use_cases_overview/#3-expanding-a-lun","title":"3. Expanding a LUN","text":"<p>Objective: Increase the size of existing LUNs to meet growing storage requirements.</p> <p>Key Steps:</p> <ul> <li>Locate the target LUN</li> <li>Validate the new size requirements</li> <li>Expand the LUN</li> <li>Monitor the expansion job</li> <li>(Optional) Provide guidance for host-side expansion</li> </ul> <p>Common Errors: LUN not found, insufficient space, invalid size parameters, LUN state issues.</p>"},{"location":"use-cases/netapp_api_use_cases_overview/#4-monitoring-cluster-performance","title":"4. Monitoring Cluster Performance","text":"<p>Objective: Continuously monitor cluster performance metrics for proactive management.</p> <p>Key Steps:</p> <ul> <li>Identify target clusters</li> <li>Retrieve performance metrics for specified time intervals</li> <li>Process and analyze the metrics data</li> <li>Generate alerts or reports as needed</li> </ul> <p>Common Errors: Cluster not found, invalid time intervals, metrics unavailable, rate limiting.</p>"},{"location":"use-cases/netapp_api_use_cases_overview/#5-annotating-an-event","title":"5. Annotating an Event","text":"<p>Objective: Add metadata annotations to events for enhanced tracking, categorization, and automation workflows.</p> <p>Key Steps:</p> <ul> <li>Search for events using specific criteria (severity, state, resource type)</li> <li>Select events that require annotation</li> <li>Add structured annotations using key-value pairs</li> <li>Verify the annotation was applied successfully</li> </ul> <p>Common Errors: Event not found, invalid annotation format, permission issues, concurrent modification conflicts.</p>"},{"location":"use-cases/netapp_api_use_cases_overview/#6-tagging-a-volume-with-name_tag","title":"6. Tagging a Volume with <code>name_tag</code>","text":"<p>Objective: Use <code>name_tag</code> to create consistently named volumes during LUN creation for better organization and searchability.</p> <p>Key Steps:</p> <ul> <li>Discover available SVMs for LUN creation</li> <li>Create a LUN with a specific <code>volume.name_tag</code> parameter</li> <li>Monitor the LUN creation job until completion</li> <li>Verify the volume was created with the expected name derived from the tag</li> </ul> <p>Common Errors: Invalid name_tag format, volume name conflicts, insufficient space, SVM not found, job failures.</p>"},{"location":"use-cases/netapp_api_use_cases_overview/#7-tagging-an-svm-via-event-annotation","title":"7. Tagging an SVM via Event Annotation","text":"<p>Objective: Add custom metadata tags to SVMs by creating associated events with annotations, enabling SVM categorization and management.</p> <p>Key Steps:</p> <ul> <li>Locate the target SVM using discovery endpoints</li> <li>Create a new event associated with the SVM</li> <li>Add structured annotations to the event (e.g., group, owner, project metadata)</li> <li>Monitor the event creation job until completion</li> <li>Verify the annotation was applied successfully</li> </ul> <p>Common Errors: SVM not found, invalid event payload, job failures, annotation format issues.</p>"},{"location":"use-cases/netapp_api_use_cases_overview/#8-attaching-metadata-to-any-object","title":"8. Attaching Metadata to any Object","text":"<p>Objective: Provide a universal mechanism for attaching custom metadata to any object within ActiveIQ Unified Manager using the console's event annotation capabilities.</p> <p>Key Steps:</p> <ul> <li>User selects an object in the ActiveIQ console</li> <li>User initiates an \"Add Annotation\" action</li> <li>Console creates an event associated with the object and adds the metadata as an annotation</li> <li>User can view all annotations for an object in the console</li> </ul> <p>Common Errors: Object not found, invalid metadata format, permission denied, job failures.</p>"},{"location":"use-cases/netapp_api_use_cases_overview/#9-searching-for-objects-by-metadata","title":"9. Searching for Objects by Metadata","text":"<p>Objective: Enable powerful search capabilities to find objects based on their attached metadata tags, supporting both API-based automation and console-based user interfaces.</p> <p>Key Steps:</p> <ul> <li>Search events using annotation filters (e.g., <code>owner:team_a</code>, <code>environment:production</code>)</li> <li>Parse event results to extract unique resource keys</li> <li>Retrieve full object details for each matching resource</li> <li>Present categorized search results grouped by object type</li> <li>Support advanced search patterns for compliance, cost tracking, and resource management</li> </ul> <p>Common Errors: No matching results, invalid search syntax, deleted resources, pagination handling.</p>"},{"location":"use-cases/netapp_api_use_cases_overview/#netapp-activeiq-administrative-philosophy","title":"NetApp ActiveIQ Administrative Philosophy","text":"<p>NetApp ActiveIQ Unified Manager follows an event-driven administrative approach that provides comprehensive metadata management capabilities through its built-in event system. This philosophy aligns with enterprise requirements for:</p>"},{"location":"use-cases/netapp_api_use_cases_overview/#audit-and-compliance","title":"Audit and Compliance","text":"<ul> <li>Complete Audit Trail: All metadata changes are recorded as events with timestamps and user attribution</li> <li>Regulatory Compliance: Events provide the documentation trail required for SOX, GDPR, and other compliance frameworks</li> <li>Change Management: Integration with ITSM systems through event-based workflows</li> </ul>"},{"location":"use-cases/netapp_api_use_cases_overview/#enterprise-integration","title":"Enterprise Integration","text":"<ul> <li>RBAC Integration: Leverages existing role-based access control for metadata management</li> <li>Workflow Automation: Events can trigger automated responses and notifications</li> <li>Reporting and Analytics: Metadata can be aggregated and analyzed through the event system</li> </ul>"},{"location":"use-cases/netapp_api_use_cases_overview/#operational-excellence","title":"Operational Excellence","text":"<ul> <li>Centralized Management: All metadata is managed through the same interface used for monitoring and administration</li> <li>Consistency: Standardized approach ensures uniform metadata handling across all object types</li> <li>Scalability: Event-based system scales with enterprise growth and complexity</li> </ul>"},{"location":"use-cases/netapp_api_use_cases_overview/#tagging-and-labeling-approaches","title":"Tagging and Labeling Approaches","text":"<p>NetApp ActiveIQ API provides multiple approaches for adding labels/tags to objects:</p>"},{"location":"use-cases/netapp_api_use_cases_overview/#event-annotations","title":"Event Annotations","text":"<ul> <li>Purpose: Add metadata to events for categorization, workflow integration, and reporting</li> <li>Format: Free-form string (recommended: key-value pairs like <code>priority:high,team:storage</code>)</li> <li>API Endpoint: <code>PATCH /management-server/events/{key}</code></li> <li>Use Cases:</li> <li>ITSM integration (ticket numbers, workflow IDs)</li> <li>Priority classification and routing</li> <li>Team assignment and responsibility tracking</li> <li>Compliance and audit metadata</li> </ul>"},{"location":"use-cases/netapp_api_use_cases_overview/#volume-name-tags","title":"Volume Name Tags","text":"<ul> <li>Purpose: Create consistent volume naming during LUN creation</li> <li>Format: String that becomes part of the volume name (e.g., <code>sample_volume</code> \u2192 <code>NSLM_sample_volume</code>)</li> <li>API Endpoint: <code>POST /storage-provider/luns</code> with <code>volume.name_tag</code> parameter</li> <li>Use Cases:</li> <li>Standardized naming conventions</li> <li>Environment identification (dev, test, prod)</li> <li>Application or project grouping</li> <li>Cost center or department tracking</li> </ul>"},{"location":"use-cases/netapp_api_use_cases_overview/#comparison","title":"Comparison","text":"Aspect Event Annotations Volume Name Tags Timing Applied after event creation Applied during LUN/volume creation Scope Events only Volumes (via LUN creation) Format Free-form string Naming convention string Persistence Stored as metadata Embedded in volume name Searchability Via API filters Via volume name searches Mutability Can be modified Fixed after creation (immutable)"},{"location":"use-cases/netapp_api_use_cases_overview/#general-error-handling-strategies","title":"General Error Handling Strategies","text":""},{"location":"use-cases/netapp_api_use_cases_overview/#network-and-connectivity","title":"Network and Connectivity","text":"<ul> <li>Timeout Errors: Implement exponential backoff retry logic</li> <li>Connection Errors: Verify network connectivity and API endpoint availability</li> <li>Rate Limiting (429): Implement request throttling and backoff strategies</li> </ul>"},{"location":"use-cases/netapp_api_use_cases_overview/#authentication-and-authorization","title":"Authentication and Authorization","text":"<ul> <li>401 Unauthorized: Verify credentials and re-authenticate if necessary</li> <li>403 Forbidden: Check user permissions and role assignments</li> </ul>"},{"location":"use-cases/netapp_api_use_cases_overview/#resource-management","title":"Resource Management","text":"<ul> <li>404 Not Found: Implement resource discovery and validation logic</li> <li>400 Bad Request: Parse error messages and provide user-friendly feedback</li> <li>409 Conflict: Handle resource state conflicts gracefully</li> </ul>"},{"location":"use-cases/netapp_api_use_cases_overview/#asynchronous-operations","title":"Asynchronous Operations","text":"<ul> <li>Job Monitoring: Implement polling logic with appropriate intervals</li> <li>Job Failures: Retrieve detailed error information from failed jobs</li> <li>Timeout Handling: Set reasonable timeouts for long-running operations</li> </ul>"},{"location":"use-cases/netapp_api_use_cases_overview/#best-practices","title":"Best Practices","text":""},{"location":"use-cases/netapp_api_use_cases_overview/#security","title":"Security","text":"<ul> <li>Store credentials securely (environment variables, secure vaults)</li> <li>Use service accounts with minimal required permissions</li> <li>Implement proper credential rotation</li> </ul>"},{"location":"use-cases/netapp_api_use_cases_overview/#reliability","title":"Reliability","text":"<ul> <li>Implement idempotent operations where possible</li> <li>Use circuit breaker patterns for external dependencies</li> <li>Log all operations for troubleshooting and audit purposes</li> </ul>"},{"location":"use-cases/netapp_api_use_cases_overview/#performance","title":"Performance","text":"<ul> <li>Cache frequently accessed data (clusters, SVMs, etc.)</li> <li>Implement connection pooling for high-frequency operations</li> <li>Use appropriate request batching where supported</li> </ul>"},{"location":"use-cases/netapp_api_use_cases_overview/#monitoring","title":"Monitoring","text":"<ul> <li>Track API response times and error rates</li> <li>Monitor job completion rates and failure patterns</li> <li>Implement health checks for the automation system</li> </ul>"},{"location":"use-cases/netapp_api_use_cases_overview/#implementation-tools","title":"Implementation Tools","text":"<p>The automation scripts can be implemented using various technologies:</p> <ul> <li>Python: Using <code>requests</code> library for HTTP operations</li> <li>PowerShell: Using <code>Invoke-RestMethod</code> cmdlets</li> <li>Bash/curl: For simple operations and testing</li> <li>Go: For high-performance, concurrent operations</li> <li>Terraform: For infrastructure-as-code approaches</li> </ul>"},{"location":"use-cases/netapp_api_use_cases_overview/#integration-patterns","title":"Integration Patterns","text":"<p>These use cases can be integrated into larger automation workflows:</p> <ul> <li>CI/CD Pipelines: Provision storage as part of application deployment</li> <li>Monitoring Systems: Integrate performance monitoring with alerting platforms</li> <li>ITSM Tools: Trigger storage operations from service request workflows</li> <li>Infrastructure as Code: Define storage resources in declarative formats</li> </ul>"},{"location":"use-cases/netapp_api_use_cases_overview/#next-steps","title":"Next Steps","text":"<p>For each use case, consider:</p> <ol> <li>Implementation: Develop the automation scripts using your preferred technology</li> <li>Testing: Validate the scripts in a test environment</li> <li>Monitoring: Implement logging and alerting for the automation</li> <li>Documentation: Create operational runbooks and troubleshooting guides</li> <li>Integration: Connect the automation to your existing workflows and systems</li> </ol>"},{"location":"use-cases/performance-analysis/","title":"Performance Analysis","text":""},{"location":"use-cases/performance-analysis/#overview","title":"Overview","text":"<p>Performance analysis is a critical DevOps use case that leverages the NetApp ActiveIQ MCP server through APIM to monitor, analyze, and optimize storage system performance. This use case demonstrates how DevOps teams can proactively identify performance bottlenecks and implement optimizations through automated workflows.</p>"},{"location":"use-cases/performance-analysis/#architecture-flow","title":"Architecture Flow","text":"<pre><code>sequenceDiagram\n    participant DevOps as DevOps GUI\n    participant APIM as API Management (APIM)\n    participant Temporal as Temporal Workflows\n    participant MCP as MCP Server (Optional)\n    participant NetApp as NetApp ActiveIQ APIs\n    participant AI as AI Assistant (Day-2)\n\n    DevOps-&gt;&gt;APIM: Request Performance Analysis\n    APIM-&gt;&gt;Temporal: Trigger Performance Workflow\n    Temporal-&gt;&gt;MCP: Optional: Enhanced Performance Context\n    Temporal-&gt;&gt;NetApp: Fetch Performance Metrics\n    NetApp--&gt;&gt;Temporal: Performance Data\n    Temporal-&gt;&gt;APIM: Analysis Results\n    APIM--&gt;&gt;DevOps: Performance Report\n\n    Note over AI: Day-2 Operations\n    AI-&gt;&gt;Temporal: Predictive Performance Insights\n    AI-&gt;&gt;DevOps: Performance Optimization Recommendations</code></pre>"},{"location":"use-cases/performance-analysis/#key-performance-metrics","title":"Key Performance Metrics","text":""},{"location":"use-cases/performance-analysis/#storage-performance-indicators","title":"Storage Performance Indicators","text":"<ul> <li>IOPS (Input/Output Operations Per Second)</li> <li>Read IOPS</li> <li>Write IOPS</li> <li> <p>Random vs Sequential patterns</p> </li> <li> <p>Latency Metrics</p> </li> <li>Average response time</li> <li>95<sup>th</sup> percentile latency</li> <li> <p>Peak latency periods</p> </li> <li> <p>Throughput Analysis</p> </li> <li>Data transfer rates</li> <li>Bandwidth utilization</li> <li>Network performance impact</li> </ul>"},{"location":"use-cases/performance-analysis/#resource-utilization","title":"Resource Utilization","text":"<ul> <li>CPU Usage</li> <li>Controller CPU utilization</li> <li>Node-level CPU metrics</li> <li> <p>Process-specific CPU consumption</p> </li> <li> <p>Memory Utilization</p> </li> <li>Buffer cache efficiency</li> <li>Memory allocation patterns</li> <li> <p>Free memory trends</p> </li> <li> <p>Disk Performance</p> </li> <li>Disk busy percentage</li> <li>Queue depth analysis</li> <li>Disk service time</li> </ul>"},{"location":"use-cases/performance-analysis/#apim-managed-workflows","title":"APIM-Managed Workflows","text":""},{"location":"use-cases/performance-analysis/#1-real-time-performance-monitoring","title":"1. Real-time Performance Monitoring","text":"<pre><code>workflow_name: performance_monitoring\ntrigger: scheduled\nfrequency: 5_minutes\nsteps:\n  - collect_metrics:\n      api_endpoint: /datacenter/storage/aggregates\n      metrics: [iops, latency, throughput]\n  - analyze_trends:\n      temporal_activity: performance_trend_analysis\n  - alert_thresholds:\n      cpu_threshold: 80%\n      latency_threshold: 10ms\n      iops_threshold: 90%_capacity\n</code></pre>"},{"location":"use-cases/performance-analysis/#2-performance-baseline-analysis","title":"2. Performance Baseline Analysis","text":"<pre><code>workflow_name: baseline_analysis\ntrigger: weekly\nsteps:\n  - historical_data_collection:\n      timeframe: 30_days\n      granularity: hourly\n  - baseline_calculation:\n      method: statistical_analysis\n      percentiles: [50, 75, 90, 95, 99]\n  - deviation_analysis:\n      alert_on_deviation: 20%\n</code></pre>"},{"location":"use-cases/performance-analysis/#3-predictive-performance-analysis","title":"3. Predictive Performance Analysis","text":"<pre><code>workflow_name: predictive_performance\ntrigger: daily\nai_integration: true\nsteps:\n  - data_preparation:\n      features: [iops, latency, cpu, memory, network]\n      window_size: 7_days\n  - ml_model_inference:\n      model_type: time_series_forecasting\n      prediction_horizon: 24_hours\n  - recommendation_generation:\n      optimization_suggestions: true\n      capacity_planning: true\n</code></pre>"},{"location":"use-cases/performance-analysis/#devops-integration-patterns","title":"DevOps Integration Patterns","text":""},{"location":"use-cases/performance-analysis/#performance-dashboard-integration","title":"Performance Dashboard Integration","text":"<pre><code># Example: Performance metrics integration\nfrom netapp_mcp_client import NetAppMCPClient\nfrom apim_client import APIMClient\n\nclass PerformanceAnalyzer:\n    def __init__(self):\n        self.apim = APIMClient()\n        self.mcp_client = NetAppMCPClient()\n\n    async def get_cluster_performance(self, cluster_id: str):\n        \"\"\"Fetch comprehensive cluster performance metrics\"\"\"\n        workflow_request = {\n            \"workflow\": \"cluster_performance_analysis\",\n            \"parameters\": {\n                \"cluster_id\": cluster_id,\n                \"metrics\": [\"iops\", \"latency\", \"throughput\", \"cpu\", \"memory\"],\n                \"timeframe\": \"1_hour\"\n            }\n        }\n\n        # Route through APIM for standardized access\n        response = await self.apim.execute_temporal_workflow(workflow_request)\n        return response.performance_data\n\n    async def analyze_performance_trends(self, svm_id: str):\n        \"\"\"Analyze performance trends for SVM\"\"\"\n        trend_data = await self.apim.get_performance_trends(\n            resource_type=\"svm\",\n            resource_id=svm_id,\n            analysis_period=\"7_days\"\n        )\n\n        return {\n            \"current_performance\": trend_data.current_metrics,\n            \"trend_analysis\": trend_data.trends,\n            \"predictions\": trend_data.ai_predictions,\n            \"recommendations\": trend_data.optimization_suggestions\n        }\n</code></pre>"},{"location":"use-cases/performance-analysis/#alerting-and-notification-workflows","title":"Alerting and Notification Workflows","text":"<pre><code>alert_rules:\n  - name: high_latency_alert\n    condition: average_latency &gt; 15ms\n    duration: 5_minutes\n    severity: warning\n    actions:\n      - temporal_workflow: performance_investigation\n      - notification: devops_team\n\n  - name: cpu_utilization_critical\n    condition: cpu_usage &gt; 90%\n    duration: 2_minutes\n    severity: critical\n    actions:\n      - temporal_workflow: emergency_performance_analysis\n      - ai_assistant: performance_optimization_recommendations\n      - escalation: on_call_engineer\n</code></pre>"},{"location":"use-cases/performance-analysis/#ai-enhanced-day-2-operations","title":"AI-Enhanced Day-2 Operations","text":""},{"location":"use-cases/performance-analysis/#intelligent-performance-optimization","title":"Intelligent Performance Optimization","text":"<p>The AI Assistant provides enhanced day-2 operations capabilities:</p> <ul> <li>Anomaly Detection: Automatically identify unusual performance patterns</li> <li>Root Cause Analysis: AI-powered investigation of performance issues</li> <li>Optimization Recommendations: Intelligent suggestions for performance tuning</li> <li>Capacity Planning: Predictive analysis for future capacity needs</li> </ul>"},{"location":"use-cases/performance-analysis/#performance-optimization-workflow","title":"Performance Optimization Workflow","text":"<pre><code>class AIPerformanceOptimizer:\n    async def optimize_performance(self, cluster_metrics):\n        \"\"\"AI-driven performance optimization\"\"\"\n\n        # Analyze current performance state\n        performance_state = await self.analyze_current_state(cluster_metrics)\n\n        # Generate optimization recommendations\n        optimizations = await self.ai_assistant.generate_optimizations(\n            performance_state=performance_state,\n            optimization_goals=[\"latency_reduction\", \"iops_improvement\", \"efficiency\"]\n        )\n\n        # Execute approved optimizations through Temporal workflows\n        for optimization in optimizations.approved_recommendations:\n            await self.apim.execute_temporal_workflow({\n                \"workflow\": \"performance_optimization\",\n                \"parameters\": optimization.parameters,\n                \"approval_required\": optimization.requires_approval\n            })\n\n        return optimizations\n</code></pre>"},{"location":"use-cases/performance-analysis/#best-practices","title":"Best Practices","text":""},{"location":"use-cases/performance-analysis/#1-performance-monitoring-strategy","title":"1. Performance Monitoring Strategy","text":"<ul> <li>Continuous Monitoring: Implement 24/7 performance monitoring</li> <li>Baseline Establishment: Maintain performance baselines for comparison</li> <li>Threshold Management: Define and regularly review alert thresholds</li> <li>Trend Analysis: Focus on performance trends rather than point-in-time metrics</li> </ul>"},{"location":"use-cases/performance-analysis/#2-optimization-approach","title":"2. Optimization Approach","text":"<ul> <li>Data-Driven Decisions: Base optimizations on comprehensive performance data</li> <li>Incremental Changes: Implement changes gradually to measure impact</li> <li>Testing Environment: Validate optimizations in non-production environments</li> <li>Rollback Procedures: Maintain ability to quickly revert changes</li> </ul>"},{"location":"use-cases/performance-analysis/#3-devops-integration","title":"3. DevOps Integration","text":"<ul> <li>Automated Workflows: Leverage Temporal workflows for consistent performance analysis</li> <li>API-First Approach: Use APIM for standardized access to performance data</li> <li>Documentation: Maintain comprehensive performance analysis documentation</li> <li>Team Collaboration: Enable cross-team visibility into performance metrics</li> </ul>"},{"location":"use-cases/performance-analysis/#troubleshooting-guide","title":"Troubleshooting Guide","text":""},{"location":"use-cases/performance-analysis/#common-performance-issues","title":"Common Performance Issues","text":"<ol> <li>High Latency</li> <li>Check network connectivity</li> <li>Analyze disk performance</li> <li>Review cache utilization</li> <li> <p>Examine workload patterns</p> </li> <li> <p>Low IOPS</p> </li> <li>Verify storage configuration</li> <li>Check for bottlenecks</li> <li>Analyze queue depths</li> <li> <p>Review application patterns</p> </li> <li> <p>CPU Bottlenecks</p> </li> <li>Analyze process utilization</li> <li>Check for resource contention</li> <li>Review workload distribution</li> <li>Consider scaling options</li> </ol>"},{"location":"use-cases/performance-analysis/#performance-analysis-tools","title":"Performance Analysis Tools","text":"<ul> <li>ActiveIQ Unified Manager: Primary performance monitoring platform</li> <li>Temporal Workflows: Orchestrated performance analysis processes</li> <li>APIM Dashboard: Centralized performance metrics visualization</li> <li>AI Assistant: Intelligent performance insights and recommendations</li> </ul>"},{"location":"use-cases/performance-analysis/#success-metrics","title":"Success Metrics","text":"<ul> <li>Mean Time to Detection (MTTD): Average time to identify performance issues</li> <li>Mean Time to Resolution (MTTR): Average time to resolve performance problems</li> <li>Performance SLA Compliance: Percentage of time within performance targets</li> <li>Optimization Success Rate: Percentage of successful performance improvements</li> <li>Predictive Accuracy: Accuracy of AI-powered performance predictions</li> </ul> <p>This performance analysis framework enables DevOps teams to maintain optimal storage system performance through automated monitoring, intelligent analysis, and proactive optimization.</p>"},{"location":"use-cases/provision_nfs_fileshare/","title":"Use Case: Provisioning a New NFS File Share","text":"<p>This sequence diagram illustrates the process of authenticating, discovering resources, provisioning a new NFS file share, and monitoring the creation job.</p> <pre><code>sequenceDiagram\n    participant AS as Automation Script\n    participant API as NetApp ActiveIQ API\n\n    AS-&gt;&gt;+API: Authenticate (HTTP Basic Auth)\n    API--&gt;&gt;-AS: 200 OK (Authentication Successful)\n\n    Note over AS, API: All subsequent requests are authenticated\n\n    AS-&gt;&gt;+API: GET /datacenter/cluster/clusters (Discover Clusters)\n    API--&gt;&gt;-AS: 200 OK (List of Clusters)\n\n    AS-&gt;&gt;+API: GET /storage-provider/svms?cluster.key={key} (Discover SVMs)\n    API--&gt;&gt;-AS: 200 OK (List of SVMs)\n\n    AS-&gt;&gt;+API: GET /datacenter/storage/aggregates?cluster.key={key} (Discover Aggregates)\n    API--&gt;&gt;-AS: 200 OK (List of Aggregates)\n\n    AS-&gt;&gt;+API: POST /storage-provider/file-shares (Create File Share)\n    API--&gt;&gt;-AS: 202 Accepted (Job Key)\n\n    loop Poll Job Status\n        AS-&gt;&gt;+API: GET /management-server/jobs/{job_key}\n        API--&gt;&gt;-AS: 200 OK (Job Status)\n    end\n\n    AS-&gt;&gt;+API: GET /storage-provider/file-shares/{fileshare_key} (Get File Share Details)\n    API--&gt;&gt;-AS: 200 OK (File Share Details)</code></pre>"},{"location":"use-cases/provision_nfs_fileshare/#error-handling","title":"Error Handling","text":"<ul> <li>Authentication Failure (401 Unauthorized): If authentication fails, the script should log the error and terminate. Ensure that the API credentials are correct and have the necessary permissions.</li> <li>Resource Not Found (404 Not Found): If a cluster, SVM, or aggregate is not found, the script should handle the error gracefully. This could involve trying a different resource or logging the error and exiting.</li> <li>Invalid Request (400 Bad Request): If the <code>POST</code> request to create the file share is invalid, the API will return a 400 error with a descriptive message. The script should parse the error message and provide feedback to the user.</li> <li>Job Failure: The job to create the file share may fail for various reasons (e.g., insufficient space, configuration error). The script should monitor the job status and, if it fails, retrieve the job's error message to diagnose the problem.</li> <li>Network Errors: Implement retry logic with exponential backoff for transient network errors (e.g., timeouts, connection errors).</li> </ul>"},{"location":"use-cases/search_object_by_metadata/","title":"Use Case: Searching for Objects by Metadata","text":"<p>This sequence diagram illustrates how to find all objects (e.g., SVMs, LUNs) that have been tagged with specific metadata using event annotations.</p> <pre><code>sequenceDiagram\n    participant AS as Automation Script\n    participant API as NetApp ActiveIQ API\n\n    AS-&gt;&gt;+API: Authenticate (HTTP Basic Auth)\n    API--&gt;&gt;-AS: 200 OK (Authentication Successful)\n\n    Note over AS, API: All subsequent requests are authenticated\n\n    AS-&gt;&gt;+API: GET /management-server/events?annotation=project:x,owner:team_a (1. Search Events by Annotation)\n    API--&gt;&gt;-AS: 200 OK (List of Matching Events)\n\n    Note right of AS: 2. Parse events and extract unique resource keys\n\n    loop For each unique resource key\n        AS-&gt;&gt;+API: GET /path/to/resource/{resource_key} (3. Retrieve Object Details)\n        API--&gt;&gt;-AS: 200 OK (Full Object Details)\n    end\n\n    Note right of AS: 4. Aggregate results to get a list of tagged objects\n</code></pre>"},{"location":"use-cases/search_object_by_metadata/#console-based-search-interface","title":"Console-Based Search Interface","text":"<pre><code>sequenceDiagram\n    participant User as User/Admin\n    participant Console as ActiveIQ Unified Manager\n    participant API as NetApp ActiveIQ API\n\n    User-&gt;&gt;+Console: 1. Access \"Search by Metadata\" interface\n    Console--&gt;&gt;-User: Display search form\n\n    User-&gt;&gt;+Console: 2. Enter search criteria (e.g., \"owner=team_a\")\n    Console-&gt;&gt;+API: 3. GET /management-server/events?annotation=owner:team_a\n    API--&gt;&gt;-Console: 200 OK (Matching Events)\n\n    Note right of Console: Parse events to extract resource keys&lt;br/&gt;and group by object type\n\n    loop For each object type (SVM, LUN, File Share)\n        Console-&gt;&gt;+API: GET /{object_type}s/{resource_key}\n        API--&gt;&gt;-Console: 200 OK (Object Details)\n    end\n\n    Console--&gt;&gt;User: 4. Display categorized search results\n\n    User-&gt;&gt;+Console: 5. Select object for details\n    Console--&gt;&gt;-User: Show object details with all associated metadata</code></pre>"},{"location":"use-cases/search_object_by_metadata/#search-patterns-and-examples","title":"Search Patterns and Examples","text":""},{"location":"use-cases/search_object_by_metadata/#basic-search-patterns","title":"Basic Search Patterns","text":"<ol> <li>Single Tag Search:</li> </ol> <pre><code>GET /management-server/events?annotation=environment:production\n</code></pre> <p>Find all objects tagged as production environment.</p> <ol> <li>Multiple Tag Search (AND logic):</li> </ol> <pre><code>GET /management-server/events?annotation=environment:production,owner:team_database\n</code></pre> <p>Find objects that are both production AND owned by database team.</p> <ol> <li>Owner-Based Search:</li> </ol> <pre><code>GET /management-server/events?annotation=owner:john.doe\n</code></pre> <p>Find all objects assigned to a specific owner.</p> <ol> <li>Project-Based Search:    <pre><code>GET /management-server/events?annotation=project:quarterly-reporting\n</code></pre>    Find all objects associated with a specific project.</li> </ol>"},{"location":"use-cases/search_object_by_metadata/#advanced-search-scenarios","title":"Advanced Search Scenarios","text":"<ol> <li>Cost Center Reporting:</li> </ol> <pre><code>GET /management-server/events?annotation=cost-center:IT-001\n</code></pre> <p>Find all objects for budget allocation and cost tracking.</p> <ol> <li>Compliance Audit:</li> </ol> <pre><code>GET /management-server/events?annotation=compliance:sox,criticality:high\n</code></pre> <p>Find high-criticality objects subject to SOX compliance.</p> <ol> <li>Environment Management:</li> </ol> <pre><code>GET /management-server/events?annotation=environment:development\nGET /management-server/events?annotation=environment:testing\nGET /management-server/events?annotation=environment:production\n</code></pre> <p>Separate searches for different environment types.</p> <ol> <li>Team Resource Allocation:    <pre><code>GET /management-server/events?annotation=team:storage-admins\n</code></pre>    Find all resources managed by a specific team.</li> </ol>"},{"location":"use-cases/search_object_by_metadata/#search-result-processing","title":"Search Result Processing","text":"<ol> <li>Group by Object Type: Categorize results as SVMs, LUNs, File Shares, etc.</li> <li>Aggregate Metadata: Collect all annotations for each found object</li> <li>Generate Reports: Create summary reports showing resource distribution</li> <li>Export Results: Generate CSV or Excel reports for further analysis</li> </ol>"},{"location":"use-cases/search_object_by_metadata/#error-handling","title":"Error Handling","text":"<ul> <li>Authentication Failure (401 Unauthorized): If authentication fails, the script should log the error and terminate.</li> <li>No Matching Events (200 OK with empty list): If no events match the search criteria, the API will return an empty list. The script should handle this gracefully, informing the user that no objects with the specified metadata were found.</li> <li>Invalid Search Syntax (400 Bad Request): If the annotation search string is malformed, the API will return a 400 error. The script should validate the search query before sending it.</li> <li>Resource Deleted (404 Not Found): It is possible that an event's annotation refers to an object that has since been deleted. The script should handle the 404 error when retrieving the object details and either log it or exclude it from the final result set.</li> <li>Network Errors: Implement retry logic with exponential backoff for transient network errors.</li> <li>Pagination: For large result sets, the script must handle pagination using the <code>_links.next</code> field in the API response to retrieve all matching events.</li> </ul>"},{"location":"use-cases/storage-monitoring/","title":"Storage Monitoring Use Cases","text":"<p>Monitor and analyze your NetApp storage infrastructure with AI-powered insights through natural language queries.</p>"},{"location":"use-cases/storage-monitoring/#overview","title":"Overview","text":"<p>NetApp ActiveIQ provides comprehensive storage monitoring capabilities that can be accessed through AI assistants. This enables storage administrators to ask natural language questions and receive instant insights about their storage environment.</p>"},{"location":"use-cases/storage-monitoring/#core-monitoring-capabilities","title":"Core Monitoring Capabilities","text":""},{"location":"use-cases/storage-monitoring/#capacity-monitoring","title":"\ud83d\udcca Capacity Monitoring","text":"<ul> <li>Real-time capacity utilization across volumes, aggregates, and clusters</li> <li>Growth trend analysis and capacity forecasting</li> <li>Threshold-based alerting for space consumption</li> <li>Storage efficiency metrics including deduplication and compression ratios</li> </ul>"},{"location":"use-cases/storage-monitoring/#performance-monitoring","title":"\ud83d\udcc8 Performance Monitoring","text":"<ul> <li>IOPS, latency, and throughput metrics for volumes and aggregates</li> <li>Performance trend analysis over time</li> <li>Bottleneck identification and performance optimization</li> <li>Workload characterization and resource utilization</li> </ul>"},{"location":"use-cases/storage-monitoring/#health-monitoring","title":"\ud83d\udd0d Health Monitoring","text":"<ul> <li>System health status for clusters, nodes, and storage components</li> <li>Hardware health including disk, controller, and network status</li> <li>Configuration compliance and best practice validation</li> <li>Risk assessment and proactive issue identification</li> </ul>"},{"location":"use-cases/storage-monitoring/#natural-language-queries","title":"Natural Language Queries","text":""},{"location":"use-cases/storage-monitoring/#capacity-questions","title":"Capacity Questions","text":"<pre><code>\"What volumes are running low on space?\"\n\"Show me storage utilization across all clusters\"\n\"Which aggregates have less than 20% free space?\"\n\"What's the storage growth rate for the production cluster?\"\n\"How much total capacity do we have available?\"\n</code></pre>"},{"location":"use-cases/storage-monitoring/#performance-questions","title":"Performance Questions","text":"<pre><code>\"What are the top 10 volumes by IOPS?\"\n\"Show me performance trends for the last 24 hours\"\n\"Which volumes have high latency?\"\n\"What's the average throughput for our NFS volumes?\"\n\"Are there any performance bottlenecks right now?\"\n</code></pre>"},{"location":"use-cases/storage-monitoring/#health-questions","title":"Health Questions","text":"<pre><code>\"What's the overall health status of our storage?\"\n\"Are there any failed disks or hardware issues?\"\n\"Show me all critical alerts\"\n\"What clusters need attention?\"\n\"Is everything running normally?\"\n</code></pre>"},{"location":"use-cases/storage-monitoring/#use-case-examples","title":"Use Case Examples","text":""},{"location":"use-cases/storage-monitoring/#1-daily-health-check","title":"1. Daily Health Check","text":"<p>Scenario: Storage administrator wants a quick overview of storage health</p> <p>Query: \"Give me a health summary of all our NetApp storage\"</p> <p>Information Provided: - Cluster health status - Critical alerts and events - Capacity utilization warnings - Hardware health issues - Performance outliers</p> <pre><code># AI Assistant translates to:\nhealth_data = {\n    \"clusters\": get_clusters(fields=[\"name\", \"state\", \"health\"]),\n    \"critical_events\": get_events(severity=\"critical\", state=\"new\"),\n    \"capacity_alerts\": get_volumes(utilization_threshold=85),\n    \"hardware_status\": get_nodes(fields=[\"health\", \"uptime\"])\n}\n</code></pre>"},{"location":"use-cases/storage-monitoring/#2-capacity-planning","title":"2. Capacity Planning","text":"<p>Scenario: Planning storage expansion and capacity management</p> <p>Query: \"What's our storage capacity outlook for the next 6 months?\"</p> <p>Information Provided: - Current capacity utilization - Growth trends and forecasting - Time to full projections - Recommendations for expansion</p>"},{"location":"use-cases/storage-monitoring/#3-performance-troubleshooting","title":"3. Performance Troubleshooting","text":"<p>Scenario: Users reporting slow performance, need to identify the cause</p> <p>Query: \"Why is our file server running slowly? Show me performance metrics\"</p> <p>Information Provided: - Volume performance metrics (IOPS, latency, throughput) - Aggregate performance data - Network and storage bottlenecks - Historical comparison</p>"},{"location":"use-cases/storage-monitoring/#4-storage-efficiency-analysis","title":"4. Storage Efficiency Analysis","text":"<p>Scenario: Evaluate storage optimization and efficiency</p> <p>Query: \"How well are our storage efficiency features working?\"</p> <p>Information Provided: - Deduplication savings - Compression ratios - Thin provisioning utilization - Space reclamation opportunities</p>"},{"location":"use-cases/storage-monitoring/#monitoring-dashboards","title":"Monitoring Dashboards","text":""},{"location":"use-cases/storage-monitoring/#executive-dashboard","title":"Executive Dashboard","text":"<ul> <li>Total Capacity: Used vs. Available across all systems</li> <li>Health Score: Overall infrastructure health percentage</li> <li>Critical Issues: Count of unresolved critical events</li> <li>Growth Rate: Monthly capacity growth trends</li> <li>Cost Optimization: Efficiency savings and recommendations</li> </ul>"},{"location":"use-cases/storage-monitoring/#operations-dashboard","title":"Operations Dashboard","text":"<ul> <li>Volume Utilization: Top volumes by capacity and growth</li> <li>Performance Metrics: IOPS, latency, and throughput trends</li> <li>Alert Status: Current alerts by severity and age</li> <li>Hardware Health: Node, disk, and network status</li> <li>Backup Status: Backup completion and failure rates</li> </ul>"},{"location":"use-cases/storage-monitoring/#technical-dashboard","title":"Technical Dashboard","text":"<ul> <li>Aggregate Performance: Detailed performance metrics</li> <li>Protocol Analysis: NFS, CIFS, iSCSI, FC performance</li> <li>Network Utilization: Inter-cluster and client traffic</li> <li>Storage Protocols: Protocol-specific performance and errors</li> </ul>"},{"location":"use-cases/storage-monitoring/#key-performance-indicators-kpis","title":"Key Performance Indicators (KPIs)","text":""},{"location":"use-cases/storage-monitoring/#capacity-kpis","title":"Capacity KPIs","text":"<ul> <li>Capacity Utilization: Percentage of total capacity used</li> <li>Growth Rate: Monthly/quarterly capacity growth</li> <li>Time to Full: Projected time until storage is full</li> <li>Efficiency Ratio: Space saved through deduplication/compression</li> </ul>"},{"location":"use-cases/storage-monitoring/#performance-kpis","title":"Performance KPIs","text":"<ul> <li>Average Latency: Response time for storage operations</li> <li>Peak IOPS: Maximum IOPS during business hours</li> <li>Throughput: Data transfer rates (MB/s)</li> <li>Cache Hit Ratio: Effectiveness of storage caching</li> </ul>"},{"location":"use-cases/storage-monitoring/#availability-kpis","title":"Availability KPIs","text":"<ul> <li>Uptime: System availability percentage</li> <li>MTBF: Mean Time Between Failures</li> <li>MTTR: Mean Time To Recovery</li> <li>Health Score: Overall system health rating</li> </ul>"},{"location":"use-cases/storage-monitoring/#automated-monitoring-workflows","title":"Automated Monitoring Workflows","text":""},{"location":"use-cases/storage-monitoring/#1-daily-health-report","title":"1. Daily Health Report","text":"<pre><code># Automated daily health check\ndaily_report = {\n    \"timestamp\": datetime.now(),\n    \"cluster_health\": check_all_clusters(),\n    \"capacity_warnings\": check_capacity_thresholds(),\n    \"critical_events\": get_new_critical_events(),\n    \"performance_outliers\": check_performance_baselines(),\n    \"recommendations\": generate_recommendations()\n}\n</code></pre>"},{"location":"use-cases/storage-monitoring/#2-capacity-threshold-alerts","title":"2. Capacity Threshold Alerts","text":"<pre><code># Monitor capacity thresholds\nfor volume in get_all_volumes():\n    if volume.utilization &gt; 90:\n        send_alert(f\"Volume {volume.name} is {volume.utilization}% full\")\n    elif volume.utilization &gt; 80:\n        send_warning(f\"Volume {volume.name} approaching capacity\")\n</code></pre>"},{"location":"use-cases/storage-monitoring/#3-performance-anomaly-detection","title":"3. Performance Anomaly Detection","text":"<pre><code># Detect performance anomalies\ncurrent_metrics = get_performance_metrics()\nbaseline_metrics = get_baseline_performance()\n\nif current_metrics.latency &gt; baseline_metrics.latency * 1.5:\n    investigate_performance_issue()\n</code></pre>"},{"location":"use-cases/storage-monitoring/#integration-with-external-systems","title":"Integration with External Systems","text":""},{"location":"use-cases/storage-monitoring/#itsm-integration","title":"ITSM Integration","text":"<ul> <li>ServiceNow: Automatic ticket creation for critical events</li> <li>Jira: Performance issue tracking and resolution</li> <li>PagerDuty: Alert escalation and on-call notifications</li> </ul>"},{"location":"use-cases/storage-monitoring/#monitoring-tools","title":"Monitoring Tools","text":"<ul> <li>Grafana: Custom dashboards and visualization</li> <li>Prometheus: Metrics collection and alerting</li> <li>Splunk: Log analysis and correlation</li> <li>Datadog: Infrastructure monitoring integration</li> </ul>"},{"location":"use-cases/storage-monitoring/#business-intelligence","title":"Business Intelligence","text":"<ul> <li>Tableau: Executive reporting and analytics</li> <li>Power BI: Capacity planning and trending</li> <li>Excel: Ad-hoc analysis and reporting</li> </ul>"},{"location":"use-cases/storage-monitoring/#alerting-and-notifications","title":"Alerting and Notifications","text":""},{"location":"use-cases/storage-monitoring/#critical-alerts","title":"Critical Alerts","text":"<ul> <li>Storage offline: Immediate notification to operations team</li> <li>Hardware failure: Automatic case creation with NetApp support</li> <li>Capacity full: Emergency expansion procedures triggered</li> <li>Performance degradation: Escalation to storage team</li> </ul>"},{"location":"use-cases/storage-monitoring/#warning-alerts","title":"Warning Alerts","text":"<ul> <li>Capacity thresholds: 80%, 85%, 90% utilization warnings</li> <li>Performance baselines: Deviation from normal performance</li> <li>Health degradation: Non-critical health issues</li> <li>Configuration drift: Changes from best practices</li> </ul>"},{"location":"use-cases/storage-monitoring/#informational-alerts","title":"Informational Alerts","text":"<ul> <li>Maintenance windows: Scheduled maintenance notifications</li> <li>Growth trends: Monthly capacity growth reports</li> <li>Efficiency reports: Storage optimization summaries</li> <li>Backup status: Daily backup completion reports</li> </ul>"},{"location":"use-cases/storage-monitoring/#best-practices","title":"Best Practices","text":""},{"location":"use-cases/storage-monitoring/#monitoring-strategy","title":"Monitoring Strategy","text":"<ol> <li>Set appropriate thresholds based on your environment</li> <li>Establish baselines for performance and capacity</li> <li>Regular health checks to identify issues early</li> <li>Automate routine monitoring tasks</li> <li>Integrate with existing tools and workflows</li> </ol>"},{"location":"use-cases/storage-monitoring/#data-retention","title":"Data Retention","text":"<ul> <li>Real-time data: Keep for 30 days</li> <li>Hourly aggregates: Keep for 6 months</li> <li>Daily summaries: Keep for 2 years</li> <li>Monthly reports: Keep for 5 years</li> </ul>"},{"location":"use-cases/storage-monitoring/#performance-optimization","title":"Performance Optimization","text":"<ol> <li>Monitor key metrics continuously</li> <li>Identify trends before they become problems</li> <li>Optimize workload placement based on performance data</li> <li>Right-size resources based on actual usage</li> <li>Plan capacity based on growth trends</li> </ol> <p>This comprehensive storage monitoring approach ensures optimal performance, availability, and capacity management of your NetApp storage infrastructure through intelligent, AI-assisted analysis.</p>"},{"location":"use-cases/svm-management/","title":"SVM Management","text":""},{"location":"use-cases/svm-management/#overview","title":"Overview","text":"<p>Storage Virtual Machine (SVM) Management is a fundamental DevOps use case that leverages the NetApp ActiveIQ MCP server through APIM to efficiently manage, monitor, and optimize Storage Virtual Machines. This use case demonstrates how DevOps teams can implement comprehensive SVM lifecycle management through automated workflows, intelligent monitoring, and AI-enhanced operational optimization.</p>"},{"location":"use-cases/svm-management/#architecture-flow","title":"Architecture Flow","text":"<pre><code>sequenceDiagram\n    participant DevOps as DevOps GUI\n    participant APIM as API Management (APIM)\n    participant Temporal as Temporal Workflows\n    participant MCP as MCP Server (Optional)\n    participant NetApp as NetApp ActiveIQ APIs\n    participant AI as AI Assistant (Day-2)\n    participant SVM as Storage Virtual Machines\n\n    DevOps-&gt;&gt;APIM: SVM Management Request\n    APIM-&gt;&gt;Temporal: Trigger SVM Workflow\n    Temporal-&gt;&gt;MCP: Optional: Enhanced SVM Context\n    Temporal-&gt;&gt;NetApp: Execute SVM Operations\n    NetApp-&gt;&gt;SVM: Perform SVM Actions\n    SVM--&gt;&gt;NetApp: Operation Status\n    NetApp--&gt;&gt;Temporal: SVM State &amp; Metrics\n    Temporal-&gt;&gt;APIM: Operation Results\n    APIM--&gt;&gt;DevOps: SVM Management Dashboard\n\n    Note over AI: Day-2 Operations\n    AI-&gt;&gt;Temporal: SVM Performance Analysis\n    AI-&gt;&gt;DevOps: Optimization Recommendations\n    AI-&gt;&gt;APIM: Predictive SVM Insights</code></pre>"},{"location":"use-cases/svm-management/#svm-management-categories","title":"SVM Management Categories","text":""},{"location":"use-cases/svm-management/#1-svm-lifecycle-operations","title":"1. SVM Lifecycle Operations","text":"<ul> <li>SVM Creation: Automated provisioning of new Storage Virtual Machines</li> <li>SVM Configuration: Network, protocol, and service configuration</li> <li>SVM Monitoring: Real-time performance and health monitoring</li> <li>SVM Optimization: Performance tuning and resource optimization</li> <li>SVM Decommissioning: Secure and controlled SVM removal</li> </ul>"},{"location":"use-cases/svm-management/#2-protocol-management","title":"2. Protocol Management","text":"<ul> <li>NFS Configuration: Network File System setup and optimization</li> <li>CIFS/SMB Setup: Windows file sharing protocol management</li> <li>iSCSI Management: Block-level storage protocol configuration</li> <li>FC Protocol: Fibre Channel configuration and monitoring</li> <li>NVMe Management: Non-Volatile Memory Express protocol support</li> </ul>"},{"location":"use-cases/svm-management/#3-security-and-access-control","title":"3. Security and Access Control","text":"<ul> <li>User Authentication: LDAP, Active Directory, and local user management</li> <li>Access Permissions: Role-based access control (RBAC)</li> <li>Network Security: Firewall rules and security policies</li> <li>Data Encryption: In-transit and at-rest encryption configuration</li> <li>Audit and Compliance: Security audit trails and compliance reporting</li> </ul>"},{"location":"use-cases/svm-management/#apim-managed-svm-workflows","title":"APIM-Managed SVM Workflows","text":""},{"location":"use-cases/svm-management/#1-svm-provisioning-automation","title":"1. SVM Provisioning Automation","text":"<pre><code>workflow_name: svm_provisioning\ntrigger: api_request\nvalidation_required: true\nsteps:\n  - pre_provisioning_validation:\n      check_cluster_resources: true\n      validate_network_config: true\n      verify_naming_standards: true\n      confirm_security_policies: true\n  - svm_creation:\n      create_svm: true\n      configure_protocols: [nfs, cifs, iscsi]\n      setup_network_interfaces: true\n      apply_security_policies: true\n  - post_provisioning_setup:\n      create_default_volumes: true\n      configure_data_protection: true\n      setup_monitoring: true\n      validate_connectivity: true\n  - documentation_update:\n      update_cmdb: true\n      create_operational_docs: true\n      notify_stakeholders: true\n</code></pre>"},{"location":"use-cases/svm-management/#2-svm-performance-monitoring","title":"2. SVM Performance Monitoring","text":"<pre><code>workflow_name: svm_performance_monitoring\ntrigger: scheduled\nfrequency: 5_minutes\nscope: all_active_svms\nsteps:\n  - metrics_collection:\n      performance_metrics: [iops, throughput, latency]\n      resource_utilization: [cpu, memory, network]\n      protocol_statistics: [nfs, cifs, iscsi, fc]\n      client_connections: true\n  - analysis_and_alerting:\n      threshold_evaluation: true\n      trend_analysis: true\n      anomaly_detection: ai_enabled\n      alert_generation: conditional\n  - optimization_recommendations:\n      performance_tuning: ai_suggested\n      resource_reallocation: predictive\n      protocol_optimization: intelligent\n</code></pre>"},{"location":"use-cases/svm-management/#3-svm-disaster-recovery","title":"3. SVM Disaster Recovery","text":"<pre><code>workflow_name: svm_disaster_recovery\ntrigger: manual_or_automated\ndr_scenarios: [planned_failover, unplanned_disaster, testing]\nsteps:\n  - dr_assessment:\n      evaluate_primary_svm: true\n      check_secondary_readiness: true\n      validate_snapmirror_relationships: true\n  - failover_execution:\n      break_snapmirror_relationships: conditional\n      promote_secondary_svm: true\n      update_dns_records: true\n      reconfigure_client_access: true\n  - validation_and_testing:\n      verify_data_integrity: true\n      test_protocol_access: true\n      validate_performance: true\n      confirm_client_connectivity: true\n</code></pre>"},{"location":"use-cases/svm-management/#devops-integration-patterns","title":"DevOps Integration Patterns","text":""},{"location":"use-cases/svm-management/#svm-management-interface","title":"SVM Management Interface","text":"<pre><code># Example: SVM management integration\nfrom netapp_mcp_client import NetAppMCPClient\nfrom apim_client import APIMClient\nfrom datetime import datetime\n\nclass SVMManager:\n    def __init__(self):\n        self.apim = APIMClient()\n        self.mcp_client = NetAppMCPClient()\n\n    async def create_svm(self, svm_config: dict):\n        \"\"\"Create a new SVM with specified configuration\"\"\"\n        provisioning_request = {\n            \"workflow\": \"svm_provisioning\",\n            \"parameters\": {\n                \"cluster_id\": svm_config[\"cluster_id\"],\n                \"svm_name\": svm_config[\"name\"],\n                \"protocols\": svm_config.get(\"protocols\", [\"nfs\", \"cifs\"]),\n                \"network_config\": svm_config[\"network\"],\n                \"security_policy\": svm_config.get(\"security_policy\", \"default\"),\n                \"data_protection\": svm_config.get(\"data_protection\", True)\n            }\n        }\n\n        response = await self.apim.execute_temporal_workflow(provisioning_request)\n        return response.svm_details\n\n    async def get_svm_performance(self, svm_id: str, timeframe_hours: int = 24):\n        \"\"\"Get comprehensive SVM performance metrics\"\"\"\n        performance_request = {\n            \"workflow\": \"svm_performance_analysis\",\n            \"parameters\": {\n                \"svm_id\": svm_id,\n                \"analysis_period\": f\"{timeframe_hours}_hours\",\n                \"metrics\": [\"iops\", \"throughput\", \"latency\", \"cpu\", \"memory\"],\n                \"include_trends\": True,\n                \"include_predictions\": True\n            }\n        }\n\n        response = await self.apim.execute_temporal_workflow(performance_request)\n        return response.performance_data\n\n    async def optimize_svm_configuration(self, svm_id: str):\n        \"\"\"Get AI-powered SVM optimization recommendations\"\"\"\n        optimization_request = {\n            \"workflow\": \"svm_optimization_analysis\",\n            \"parameters\": {\n                \"svm_id\": svm_id,\n                \"optimization_goals\": [\"performance\", \"efficiency\", \"security\"],\n                \"include_protocol_tuning\": True,\n                \"include_resource_optimization\": True,\n                \"risk_tolerance\": \"medium\"\n            }\n        }\n\n        response = await self.apim.execute_temporal_workflow(optimization_request)\n        return response.optimization_recommendations\n</code></pre>"},{"location":"use-cases/svm-management/#automated-svm-operations","title":"Automated SVM Operations","text":"<pre><code>class AutomatedSVMOperations:\n    async def setup_svm_automation(self):\n        \"\"\"Configure automated SVM management\"\"\"\n\n        # Automated SVM scaling\n        await self.apim.register_svm_handler({\n            \"trigger_type\": \"resource_utilization_high\",\n            \"threshold\": \"80%_cpu_or_memory\",\n            \"action\": \"svm_resource_scaling\",\n            \"auto_execute\": True,\n            \"approval_required\": False,\n            \"notification_channels\": [\"slack\", \"email\"]\n        })\n\n        # Automated protocol optimization\n        await self.apim.register_svm_handler({\n            \"trigger_type\": \"protocol_performance_degradation\",\n            \"threshold\": \"latency_increase_20%\",\n            \"action\": \"protocol_optimization_workflow\",\n            \"auto_execute\": False,\n            \"approval_required\": True,\n            \"approver_role\": \"storage_admin\"\n        })\n\n        # Automated failover for disaster recovery\n        await self.apim.register_svm_handler({\n            \"trigger_type\": \"svm_unavailability\",\n            \"detection_time\": \"5_minutes\",\n            \"action\": \"automated_svm_failover\",\n            \"auto_execute\": True,\n            \"approval_required\": True,\n            \"approver_role\": \"dr_manager\",\n            \"escalation_timeout\": \"10_minutes\"\n        })\n\n    async def execute_svm_migration(self, migration_plan):\n        \"\"\"Execute SVM migration workflow\"\"\"\n        migration_workflow = {\n            \"workflow\": \"svm_migration\",\n            \"parameters\": {\n                \"source_svm\": migration_plan[\"source_svm_id\"],\n                \"target_cluster\": migration_plan[\"target_cluster_id\"],\n                \"migration_type\": migration_plan[\"type\"],  # online, offline, cutover\n                \"data_transfer_method\": migration_plan.get(\"transfer_method\", \"snapmirror\"),\n                \"validation_checks\": True,\n                \"rollback_plan\": migration_plan.get(\"rollback_plan\")\n            }\n        }\n\n        return await self.apim.execute_temporal_workflow(migration_workflow)\n</code></pre>"},{"location":"use-cases/svm-management/#ai-enhanced-day-2-operations","title":"AI-Enhanced Day-2 Operations","text":""},{"location":"use-cases/svm-management/#intelligent-svm-optimization","title":"Intelligent SVM Optimization","text":"<p>The AI Assistant provides advanced SVM management capabilities:</p> <ul> <li>Performance Optimization: Analyze SVM performance patterns and suggest optimizations</li> <li>Resource Allocation: Intelligent resource allocation based on workload patterns</li> <li>Protocol Tuning: AI-driven protocol configuration optimization</li> <li>Predictive Scaling: Predict resource needs and proactively scale SVMs</li> </ul>"},{"location":"use-cases/svm-management/#ai-svm-analytics-pipeline","title":"AI SVM Analytics Pipeline","text":"<pre><code>class AISVMAnalytics:\n    async def optimize_svm_performance(self, svm_metrics):\n        \"\"\"AI-driven SVM performance optimization\"\"\"\n\n        # Analyze current SVM performance\n        performance_analysis = await self.ai_assistant.analyze_svm_performance(\n            svm_metrics=svm_metrics,\n            historical_data=\"30_days\",\n            include_protocol_analysis=True\n        )\n\n        # Generate optimization recommendations\n        optimizations = await self.ai_assistant.generate_svm_optimizations(\n            performance_analysis=performance_analysis,\n            workload_patterns=await self.get_workload_patterns(),\n            resource_constraints=await self.get_resource_constraints()\n        )\n\n        # Execute approved optimizations\n        for optimization in optimizations.approved_recommendations:\n            await self.apim.execute_temporal_workflow({\n                \"workflow\": optimization.workflow,\n                \"parameters\": optimization.parameters,\n                \"ai_confidence\": optimization.confidence_score\n            })\n\n        return optimizations\n\n    async def predict_svm_scaling_needs(self, svm_metrics):\n        \"\"\"Predict SVM scaling requirements\"\"\"\n        scaling_prediction = await self.ai_assistant.predict_scaling_needs(\n            current_metrics=svm_metrics,\n            growth_patterns=await self.get_growth_patterns(),\n            business_forecasts=await self.get_business_forecasts()\n        )\n\n        # Proactive scaling for high-confidence predictions\n        for prediction in scaling_prediction.high_confidence_predictions:\n            if prediction.confidence_score &gt; 0.85:\n                await self.apim.execute_temporal_workflow({\n                    \"workflow\": \"proactive_svm_scaling\",\n                    \"parameters\": {\n                        \"svm_id\": prediction.svm_id,\n                        \"scaling_action\": prediction.recommended_action,\n                        \"timing\": prediction.optimal_timing\n                    }\n                })\n\n        return scaling_prediction\n</code></pre>"},{"location":"use-cases/svm-management/#predictive-svm-management","title":"Predictive SVM Management","text":"<pre><code>predictive_svm_workflows:\n  - name: svm_performance_forecasting\n    trigger: daily\n    ai_model: time_series_prediction\n    features:\n      - historical_performance_metrics\n      - workload_patterns\n      - resource_utilization_trends\n    predictions:\n      - performance_bottlenecks\n      - resource_exhaustion_points\n      - optimization_opportunities\n\n  - name: svm_capacity_planning\n    trigger: weekly\n    ai_model: capacity_forecasting\n    inputs:\n      - svm_growth_history\n      - business_projections\n      - seasonal_patterns\n    outputs:\n      - capacity_requirements\n      - scaling_timeline\n      - resource_allocation_plan\n\n  - name: svm_health_prediction\n    trigger: continuous\n    ai_model: anomaly_detection\n    monitoring:\n      - performance_metrics\n      - error_rates\n      - resource_utilization\n    actions:\n      - predictive_maintenance\n      - proactive_optimization\n      - early_warning_alerts\n</code></pre>"},{"location":"use-cases/svm-management/#svm-security-and-compliance","title":"SVM Security and Compliance","text":""},{"location":"use-cases/svm-management/#security-configuration-management","title":"Security Configuration Management","text":"<pre><code>svm_security_policies:\n  authentication:\n    - method: ldap\n      server: corporate_ldap\n      encryption: tls\n    - method: active_directory\n      domain: corporate_domain\n      secure_channel: true\n\n  access_control:\n    - rbac_enabled: true\n    - default_permissions: restrictive\n    - audit_logging: comprehensive\n    - session_timeout: 30_minutes\n\n  network_security:\n    - firewall_rules: enabled\n    - allowed_protocols: [nfs, cifs, iscsi]\n    - encryption_in_transit: mandatory\n    - ip_whitelisting: enabled\n\n  data_protection:\n    - encryption_at_rest: aes_256\n    - key_management: external_kmip\n    - backup_encryption: enabled\n    - compliance_reporting: automated\n</code></pre>"},{"location":"use-cases/svm-management/#compliance-monitoring","title":"Compliance Monitoring","text":"<pre><code>class SVMComplianceManager:\n    async def audit_svm_compliance(self, compliance_framework: str):\n        \"\"\"Audit SVM compliance with specified framework\"\"\"\n        audit_request = {\n            \"workflow\": \"svm_compliance_audit\",\n            \"parameters\": {\n                \"compliance_framework\": compliance_framework,\n                \"audit_scope\": \"all_active_svms\",\n                \"include_remediation\": True,\n                \"generate_report\": True\n            }\n        }\n\n        response = await self.apim.execute_temporal_workflow(audit_request)\n        return response.compliance_report\n\n    async def enforce_security_policies(self, policy_updates):\n        \"\"\"Enforce updated security policies across SVMs\"\"\"\n        enforcement_request = {\n            \"workflow\": \"security_policy_enforcement\",\n            \"parameters\": {\n                \"policy_updates\": policy_updates,\n                \"enforcement_scope\": \"all_svms\",\n                \"validation_required\": True,\n                \"rollback_on_failure\": True\n            }\n        }\n\n        return await self.apim.execute_temporal_workflow(enforcement_request)\n</code></pre>"},{"location":"use-cases/svm-management/#svm-protocol-management","title":"SVM Protocol Management","text":""},{"location":"use-cases/svm-management/#protocol-configuration-and-optimization","title":"Protocol Configuration and Optimization","text":"<pre><code>protocol_management:\n  nfs:\n    - version: [v3, v4, v4.1]\n    - security: [sys, krb5, krb5i, krb5p]\n    - performance_tuning: auto_tuned\n    - caching: enabled\n\n  cifs:\n    - smb_versions: [2.1, 3.0, 3.1.1]\n    - authentication: [ntlm, kerberos]\n    - encryption: smb3_encryption\n    - oplocks: enabled\n\n  iscsi:\n    - authentication: chap\n    - multipathing: alua\n    - target_discovery: isns\n    - performance_optimization: enabled\n\n  nvme:\n    - transport: [tcp, rdma]\n    - namespace_management: automatic\n    - multipathing: ana\n    - performance_monitoring: continuous\n</code></pre>"},{"location":"use-cases/svm-management/#protocol-performance-monitoring","title":"Protocol Performance Monitoring","text":"<pre><code>class ProtocolPerformanceMonitor:\n    async def monitor_protocol_performance(self, svm_id: str):\n        \"\"\"Monitor protocol-specific performance metrics\"\"\"\n        monitoring_request = {\n            \"workflow\": \"protocol_performance_monitoring\",\n            \"parameters\": {\n                \"svm_id\": svm_id,\n                \"protocols\": [\"nfs\", \"cifs\", \"iscsi\", \"nvme\"],\n                \"metrics\": [\"iops\", \"throughput\", \"latency\", \"errors\"],\n                \"aggregation_period\": \"5_minutes\",\n                \"alert_thresholds\": True\n            }\n        }\n\n        response = await self.apim.execute_temporal_workflow(monitoring_request)\n        return response.protocol_metrics\n\n    async def optimize_protocol_configuration(self, optimization_targets):\n        \"\"\"Optimize protocol configurations based on workload analysis\"\"\"\n        optimization_request = {\n            \"workflow\": \"protocol_optimization\",\n            \"parameters\": {\n                \"optimization_targets\": optimization_targets,\n                \"performance_goals\": [\"latency_reduction\", \"throughput_increase\"],\n                \"ai_recommendations\": True,\n                \"validation_testing\": True\n            }\n        }\n\n        return await self.apim.execute_temporal_workflow(optimization_request)\n</code></pre>"},{"location":"use-cases/svm-management/#svm-best-practices","title":"SVM Best Practices","text":""},{"location":"use-cases/svm-management/#1-svm-design-principles","title":"1. SVM Design Principles","text":"<ul> <li>Single Protocol per SVM: Optimize performance by dedicating SVMs to specific protocols</li> <li>Resource Isolation: Ensure proper resource isolation between workloads</li> <li>Network Segmentation: Implement appropriate network segmentation for security</li> <li>Naming Conventions: Establish consistent SVM naming standards</li> </ul>"},{"location":"use-cases/svm-management/#2-performance-optimization","title":"2. Performance Optimization","text":"<ul> <li>Protocol Tuning: Optimize protocol-specific settings for workload requirements</li> <li>Resource Allocation: Properly size CPU, memory, and network resources</li> <li>Load Balancing: Distribute workloads across multiple SVMs when appropriate</li> <li>Monitoring and Alerting: Implement comprehensive monitoring and alerting</li> </ul>"},{"location":"use-cases/svm-management/#3-security-and-compliance","title":"3. Security and Compliance","text":"<ul> <li>Access Control: Implement strict role-based access control</li> <li>Encryption: Enable encryption for data in transit and at rest</li> <li>Audit Logging: Maintain comprehensive audit logs</li> <li>Regular Security Reviews: Conduct regular security assessments</li> </ul>"},{"location":"use-cases/svm-management/#troubleshooting-guide","title":"Troubleshooting Guide","text":""},{"location":"use-cases/svm-management/#common-svm-issues","title":"Common SVM Issues","text":"<ol> <li>Performance Degradation</li> <li>Check resource utilization (CPU, memory, network)</li> <li>Analyze protocol-specific metrics</li> <li>Review client connection patterns</li> <li> <p>Investigate storage backend performance</p> </li> <li> <p>Connectivity Issues</p> </li> <li>Verify network configuration</li> <li>Check firewall rules and security policies</li> <li>Validate DNS resolution</li> <li> <p>Test client authentication</p> </li> <li> <p>Protocol-Specific Problems</p> </li> <li>NFS: Check export policies and mount options</li> <li>CIFS: Verify domain authentication and share permissions</li> <li>iSCSI: Validate initiator configuration and CHAP settings</li> <li>NVMe: Check namespace configuration and multipathing</li> </ol>"},{"location":"use-cases/svm-management/#performance-optimization-techniques","title":"Performance Optimization Techniques","text":"<ul> <li>CPU Optimization: Adjust CPU allocation based on workload patterns</li> <li>Memory Tuning: Optimize memory allocation for caching and buffering</li> <li>Network Optimization: Configure network interfaces for optimal throughput</li> <li>Protocol Tuning: Optimize protocol-specific parameters</li> </ul>"},{"location":"use-cases/svm-management/#success-metrics","title":"Success Metrics","text":"<ul> <li>SVM Availability: Percentage uptime of Storage Virtual Machines</li> <li>Performance SLA Compliance: Meeting performance service level agreements</li> <li>Resource Utilization: Optimal use of allocated resources</li> <li>Security Compliance: Adherence to security policies and standards</li> <li>Protocol Performance: Protocol-specific performance metrics</li> <li>Automation Success Rate: Percentage of successful automated operations</li> <li>Mean Time to Resolution: Average time to resolve SVM issues</li> </ul> <p>This comprehensive SVM management framework enables DevOps teams to efficiently manage Storage Virtual Machines through automated provisioning, intelligent monitoring, AI-enhanced optimization, and robust security controls, ensuring optimal performance and compliance.</p>"},{"location":"use-cases/tag_svm_with_event_annotation/","title":"Use Case: Tagging an SVM via Event Annotation","text":"<p>This sequence diagram illustrates how to tag an SVM by creating a new event and adding a custom annotation to it.</p> <pre><code>sequenceDiagram\n    participant AS as Automation Script\n    participant API as NetApp ActiveIQ API\n\n    AS-&gt;&gt;+API: Authenticate (HTTP Basic Auth)\n    API--&gt;&gt;-AS: 200 OK (Authentication Successful)\n\n    Note over AS, API: All subsequent requests are authenticated\n\n    AS-&gt;&gt;+API: GET /datacenter/svm/svms (Find SVM)\n    API--&gt;&gt;-AS: 200 OK (List of SVMs)\n\n    Note right of AS: Prepare event payload&lt;br/&gt;with SVM key and&lt;br/&gt;custom annotation\n\n    AS-&gt;&gt;+API: POST /management-server/events (Create Event with Annotation)\n    API--&gt;&gt;-AS: 202 Accepted (Job Key)\n\n    loop Poll Job Status\n        AS-&gt;&gt;+API: GET /management-server/jobs/{job_key}\n        API--&gt;&gt;-AS: 200 OK (Job Status)\n    end\n\n    AS-&gt;&gt;+API: GET /management-server/events/{event_key} (Verify Annotation)\n    API--&gt;&gt;-AS: 200 OK (Event with SVM and Annotation)</code></pre>"},{"location":"use-cases/tag_svm_with_event_annotation/#error-handling","title":"Error Handling","text":"<ul> <li>Authentication Failure (401 Unauthorized): If authentication fails, the script should log the error and terminate. Ensure that the API credentials are correct and have the necessary permissions.</li> <li>SVM Not Found (404 Not Found): If the SVM to be tagged does not exist, the script should handle the error gracefully.</li> <li>Invalid Event Payload (400 Bad Request): If the event creation request is invalid (e.g., missing required fields), the API will return a 400 error. The script should validate the payload before sending it.</li> <li>Job Failure: The event creation job may fail. The script should monitor the job status and provide detailed error information if it fails.</li> <li>Network Errors: Implement retry logic with exponential backoff for transient network errors.</li> </ul>"},{"location":"use-cases/tag_volume_with_nametag/","title":"Use Case: Tagging a Volume with <code>name_tag</code>","text":"<p>This sequence diagram illustrates how to authenticate and use a <code>name_tag</code> to create a consistently named volume during LUN creation.</p> <pre><code>sequenceDiagram\n    participant AS as Automation Script\n    participant API as NetApp ActiveIQ API\n\n    AS-&gt;&gt;+API: Authenticate (HTTP Basic Auth)\n    API--&gt;&gt;-AS: 200 OK (Authentication Successful)\n\n    Note over AS, API: All subsequent requests are authenticated\n\n    AS-&gt;&gt;+API: GET /storage-provider/svms (Discover SVMs)\n    API--&gt;&gt;-AS: 200 OK (List of SVMs)\n\n    AS-&gt;&gt;+API: POST /storage-provider/luns (Create LUN with `volume.name_tag`)\n    API--&gt;&gt;-AS: 202 Accepted (Job Key)\n\n    loop Poll Job Status\n        AS-&gt;&gt;+API: GET /management-server/jobs/{job_key}\n        API--&gt;&gt;-AS: 200 OK (Job Status)\n    end\n\n    AS-&gt;&gt;+API: GET /storage-provider/luns/{lun_key} (Verify LUN and Volume Name)\n    API--&gt;&gt;-AS: 200 OK (LUN Details)</code></pre>"},{"location":"use-cases/tag_volume_with_nametag/#error-handling","title":"Error Handling","text":"<ul> <li>Authentication Failure (401 Unauthorized): If authentication fails, the script should log the error and terminate. Ensure that the API credentials are correct and have the necessary permissions.</li> <li>Invalid <code>name_tag</code> (400 Bad Request): If the <code>name_tag</code> contains invalid characters or does not meet naming requirements, the API will return a 400 error. The script should validate the <code>name_tag</code> before making the request.</li> <li>Volume Name Conflict (400 Bad Request): If the volume name derived from the <code>name_tag</code> (e.g., <code>NSLM_&lt;name_tag&gt;</code>) already exists, the LUN creation will fail. The script should include logic to check for existing volumes or use a unique <code>name_tag</code>.</li> <li>Resource Not Found (404 Not Found): If the specified SVM for LUN creation does not exist, the API will return a 404 error. The script should validate the SVM key before creating the LUN.</li> <li>Insufficient Space (400 Bad Request): If the selected aggregate does not have enough space, the LUN creation job will fail. The script should check for available capacity before initiating the request.</li> <li>Job Failure: The LUN creation job may fail for other reasons (e.g., system constraints, licensing issues). The script should monitor the job status and retrieve detailed error information if the job fails.</li> <li>Network Errors: Implement retry logic with exponential backoff for transient network errors.</li> </ul>"},{"location":"use-cases/volume-operations/","title":"Volume Operations","text":""},{"location":"use-cases/volume-operations/#overview","title":"Overview","text":"<p>Volume Operations is a critical DevOps use case that leverages the NetApp ActiveIQ MCP server through APIM to efficiently manage, monitor, and optimize storage volumes. This use case demonstrates how DevOps teams can implement comprehensive volume lifecycle management through automated workflows, intelligent monitoring, and AI-enhanced operational optimization.</p>"},{"location":"use-cases/volume-operations/#architecture-flow","title":"Architecture Flow","text":"<pre><code>sequenceDiagram\n    participant DevOps as DevOps GUI\n    participant APIM as API Management (APIM)\n    participant Temporal as Temporal Workflows\n    participant MCP as MCP Server (Optional)\n    participant NetApp as NetApp ActiveIQ APIs\n    participant AI as AI Assistant (Day-2)\n    participant Storage as Storage Volumes\n\n    DevOps-&gt;&gt;APIM: Volume Operation Request\n    APIM-&gt;&gt;Temporal: Trigger Volume Workflow\n    Temporal-&gt;&gt;MCP: Optional: Enhanced Volume Context\n    Temporal-&gt;&gt;NetApp: Execute Volume Operations\n    NetApp-&gt;&gt;Storage: Perform Volume Actions\n    Storage--&gt;&gt;NetApp: Operation Status\n    NetApp--&gt;&gt;Temporal: Volume State &amp; Metrics\n    Temporal-&gt;&gt;APIM: Operation Results\n    APIM--&gt;&gt;DevOps: Volume Management Dashboard\n\n    Note over AI: Day-2 Operations\n    AI-&gt;&gt;Temporal: Volume Performance Analysis\n    AI-&gt;&gt;DevOps: Optimization Recommendations\n    AI-&gt;&gt;APIM: Predictive Volume Insights</code></pre>"},{"location":"use-cases/volume-operations/#volume-operations-categories","title":"Volume Operations Categories","text":""},{"location":"use-cases/volume-operations/#1-volume-lifecycle-management","title":"1. Volume Lifecycle Management","text":"<ul> <li>Volume Creation: Automated provisioning of new storage volumes</li> <li>Volume Configuration: Size, performance, and policy configuration</li> <li>Volume Monitoring: Real-time performance and health monitoring</li> <li>Volume Optimization: Performance tuning and efficiency optimization</li> <li>Volume Decommissioning: Secure and controlled volume removal</li> </ul>"},{"location":"use-cases/volume-operations/#2-volume-performance-operations","title":"2. Volume Performance Operations","text":"<ul> <li>Performance Monitoring: IOPS, throughput, and latency tracking</li> <li>Quality of Service (QoS): Performance guarantee management</li> <li>Tiering Management: Automated data tiering optimization</li> <li>Caching Optimization: Read and write cache management</li> <li>Workload Analysis: Application workload pattern analysis</li> </ul>"},{"location":"use-cases/volume-operations/#3-volume-data-protection","title":"3. Volume Data Protection","text":"<ul> <li>Snapshot Management: Point-in-time copy creation and management</li> <li>Replication Setup: SnapMirror and SnapVault configuration</li> <li>Backup Scheduling: Automated backup policy management</li> <li>Recovery Operations: Volume restore and recovery procedures</li> <li>Data Deduplication: Storage efficiency optimization</li> </ul>"},{"location":"use-cases/volume-operations/#apim-managed-volume-workflows","title":"APIM-Managed Volume Workflows","text":""},{"location":"use-cases/volume-operations/#1-volume-provisioning-automation","title":"1. Volume Provisioning Automation","text":"<pre><code>workflow_name: volume_provisioning\ntrigger: api_request\nvalidation_required: true\nsteps:\n  - pre_provisioning_validation:\n      check_aggregate_space: true\n      validate_naming_conventions: true\n      verify_security_policies: true\n      confirm_performance_requirements: true\n  - volume_creation:\n      create_volume: true\n      set_size_and_guarantee: true\n      configure_security_style: true\n      apply_qos_policies: true\n  - post_provisioning_setup:\n      create_initial_snapshot: true\n      configure_data_protection: true\n      setup_monitoring_alerts: true\n      validate_access_permissions: true\n  - documentation_and_notification:\n      update_inventory: true\n      create_operational_documentation: true\n      notify_stakeholders: true\n</code></pre>"},{"location":"use-cases/volume-operations/#2-volume-performance-optimization","title":"2. Volume Performance Optimization","text":"<pre><code>workflow_name: volume_performance_optimization\ntrigger: scheduled_or_threshold\nfrequency: hourly\nscope: all_active_volumes\nsteps:\n  - performance_analysis:\n      collect_iops_metrics: true\n      analyze_latency_patterns: true\n      evaluate_throughput_efficiency: true\n      assess_cache_hit_ratios: true\n  - optimization_recommendations:\n      qos_policy_adjustments: ai_recommended\n      tiering_optimization: intelligent\n      cache_configuration: adaptive\n      aggregate_placement: optimal\n  - automated_optimizations:\n      execute_approved_changes: true\n      monitor_impact: continuous\n      rollback_on_degradation: automatic\n</code></pre>"},{"location":"use-cases/volume-operations/#3-volume-data-protection-workflows","title":"3. Volume Data Protection Workflows","text":"<pre><code>workflow_name: volume_data_protection\ntrigger: policy_based\nprotection_levels: [basic, standard, enterprise]\nsteps:\n  - snapshot_management:\n      create_scheduled_snapshots: true\n      manage_snapshot_retention: policy_based\n      monitor_snapshot_space: true\n  - replication_setup:\n      configure_snapmirror: conditional\n      setup_snapvault: backup_required\n      validate_replication_health: continuous\n  - backup_orchestration:\n      coordinate_backup_schedules: true\n      monitor_backup_completion: true\n      validate_backup_integrity: true\n  - recovery_testing:\n      automated_recovery_tests: monthly\n      validate_rpo_rto_compliance: true\n      update_recovery_procedures: true\n</code></pre>"},{"location":"use-cases/volume-operations/#devops-integration-patterns","title":"DevOps Integration Patterns","text":""},{"location":"use-cases/volume-operations/#volume-management-interface","title":"Volume Management Interface","text":"<pre><code># Example: Volume operations integration\nfrom netapp_mcp_client import NetAppMCPClient\nfrom apim_client import APIMClient\nfrom datetime import datetime, timedelta\n\nclass VolumeManager:\n    def __init__(self):\n        self.apim = APIMClient()\n        self.mcp_client = NetAppMCPClient()\n\n    async def create_volume(self, volume_config: dict):\n        \"\"\"Create a new volume with specified configuration\"\"\"\n        provisioning_request = {\n            \"workflow\": \"volume_provisioning\",\n            \"parameters\": {\n                \"svm_id\": volume_config[\"svm_id\"],\n                \"volume_name\": volume_config[\"name\"],\n                \"size\": volume_config[\"size\"],\n                \"aggregate\": volume_config.get(\"aggregate\", \"auto_select\"),\n                \"security_style\": volume_config.get(\"security_style\", \"unix\"),\n                \"qos_policy\": volume_config.get(\"qos_policy\", \"default\"),\n                \"data_protection\": volume_config.get(\"data_protection\", True)\n            }\n        }\n\n        response = await self.apim.execute_temporal_workflow(provisioning_request)\n        return response.volume_details\n\n    async def get_volume_performance(self, volume_id: str, timeframe_hours: int = 24):\n        \"\"\"Get comprehensive volume performance metrics\"\"\"\n        performance_request = {\n            \"workflow\": \"volume_performance_analysis\",\n            \"parameters\": {\n                \"volume_id\": volume_id,\n                \"analysis_period\": f\"{timeframe_hours}_hours\",\n                \"metrics\": [\"iops\", \"throughput\", \"latency\", \"cache_hits\"],\n                \"include_trends\": True,\n                \"include_predictions\": True,\n                \"workload_analysis\": True\n            }\n        }\n\n        response = await self.apim.execute_temporal_workflow(performance_request)\n        return response.performance_data\n\n    async def resize_volume(self, volume_id: str, new_size: str, resize_type: str = \"auto\"):\n        \"\"\"Resize volume with optional auto-shrink capability\"\"\"\n        resize_request = {\n            \"workflow\": \"volume_resize\",\n            \"parameters\": {\n                \"volume_id\": volume_id,\n                \"new_size\": new_size,\n                \"resize_type\": resize_type,  # manual, auto, auto_grow\n                \"validate_space\": True,\n                \"backup_before_resize\": True\n            }\n        }\n\n        return await self.apim.execute_temporal_workflow(resize_request)\n</code></pre>"},{"location":"use-cases/volume-operations/#automated-volume-operations","title":"Automated Volume Operations","text":"<pre><code>class AutomatedVolumeOperations:\n    async def setup_volume_automation(self):\n        \"\"\"Configure automated volume management\"\"\"\n\n        # Automated volume resizing\n        await self.apim.register_volume_handler({\n            \"trigger_type\": \"volume_space_threshold\",\n            \"threshold\": \"85%_full\",\n            \"action\": \"auto_volume_resize\",\n            \"resize_increment\": \"20%\",\n            \"max_size\": \"1TB\",\n            \"approval_required\": False,\n            \"notification_channels\": [\"slack\", \"email\"]\n        })\n\n        # Automated snapshot management\n        await self.apim.register_volume_handler({\n            \"trigger_type\": \"snapshot_schedule\",\n            \"schedule\": \"hourly_daily_weekly\",\n            \"action\": \"create_snapshots\",\n            \"retention_policy\": \"7_daily_4_weekly_12_monthly\",\n            \"auto_execute\": True,\n            \"cleanup_old_snapshots\": True\n        })\n\n        # Automated performance optimization\n        await self.apim.register_volume_handler({\n            \"trigger_type\": \"performance_degradation\",\n            \"threshold\": \"latency_increase_30%\",\n            \"action\": \"volume_performance_optimization\",\n            \"auto_execute\": False,\n            \"approval_required\": True,\n            \"approver_role\": \"storage_admin\"\n        })\n\n    async def execute_volume_migration(self, migration_plan):\n        \"\"\"Execute volume migration workflow\"\"\"\n        migration_workflow = {\n            \"workflow\": \"volume_migration\",\n            \"parameters\": {\n                \"source_volume\": migration_plan[\"source_volume_id\"],\n                \"target_aggregate\": migration_plan[\"target_aggregate\"],\n                \"migration_method\": migration_plan.get(\"method\", \"vol_move\"),\n                \"cutover_window\": migration_plan.get(\"cutover_window\", \"maintenance\"),\n                \"validation_checks\": True,\n                \"rollback_plan\": migration_plan.get(\"rollback_plan\")\n            }\n        }\n\n        return await self.apim.execute_temporal_workflow(migration_workflow)\n</code></pre>"},{"location":"use-cases/volume-operations/#ai-enhanced-day-2-operations","title":"AI-Enhanced Day-2 Operations","text":""},{"location":"use-cases/volume-operations/#intelligent-volume-optimization","title":"Intelligent Volume Optimization","text":"<p>The AI Assistant provides advanced volume management capabilities:</p> <ul> <li>Performance Prediction: Predict volume performance bottlenecks before they occur</li> <li>Capacity Forecasting: Intelligent capacity growth prediction and planning</li> <li>Workload Analysis: Analyze application workload patterns for optimization</li> <li>Automated Tiering: AI-driven data placement across storage tiers</li> </ul>"},{"location":"use-cases/volume-operations/#ai-volume-analytics-pipeline","title":"AI Volume Analytics Pipeline","text":"<pre><code>class AIVolumeAnalytics:\n    async def optimize_volume_performance(self, volume_metrics):\n        \"\"\"AI-driven volume performance optimization\"\"\"\n\n        # Analyze current volume performance\n        performance_analysis = await self.ai_assistant.analyze_volume_performance(\n            volume_metrics=volume_metrics,\n            historical_data=\"30_days\",\n            include_workload_patterns=True\n        )\n\n        # Generate optimization recommendations\n        optimizations = await self.ai_assistant.generate_volume_optimizations(\n            performance_analysis=performance_analysis,\n            application_requirements=await self.get_application_requirements(),\n            storage_constraints=await self.get_storage_constraints()\n        )\n\n        # Execute approved optimizations\n        for optimization in optimizations.approved_recommendations:\n            await self.apim.execute_temporal_workflow({\n                \"workflow\": optimization.workflow,\n                \"parameters\": optimization.parameters,\n                \"ai_confidence\": optimization.confidence_score\n            })\n\n        return optimizations\n\n    async def predict_volume_capacity_needs(self, volume_metrics):\n        \"\"\"Predict volume capacity requirements\"\"\"\n        capacity_prediction = await self.ai_assistant.predict_capacity_needs(\n            current_metrics=volume_metrics,\n            growth_patterns=await self.get_growth_patterns(),\n            business_forecasts=await self.get_business_forecasts()\n        )\n\n        # Proactive capacity management for high-confidence predictions\n        for prediction in capacity_prediction.high_confidence_predictions:\n            if prediction.confidence_score &gt; 0.85:\n                await self.apim.execute_temporal_workflow({\n                    \"workflow\": \"proactive_volume_expansion\",\n                    \"parameters\": {\n                        \"volume_id\": prediction.volume_id,\n                        \"expansion_size\": prediction.recommended_size,\n                        \"timing\": prediction.optimal_timing\n                    }\n                })\n\n        return capacity_prediction\n</code></pre>"},{"location":"use-cases/volume-operations/#predictive-volume-management","title":"Predictive Volume Management","text":"<pre><code>predictive_volume_workflows:\n  - name: volume_performance_forecasting\n    trigger: daily\n    ai_model: time_series_analysis\n    features:\n      - historical_performance_data\n      - application_workload_patterns\n      - storage_utilization_trends\n    predictions:\n      - performance_bottlenecks\n      - capacity_exhaustion_timeline\n      - optimization_opportunities\n\n  - name: volume_workload_analysis\n    trigger: continuous\n    ai_model: workload_classification\n    analysis:\n      - io_pattern_recognition\n      - application_behavior_modeling\n      - performance_correlation_analysis\n    outputs:\n      - workload_classification\n      - optimization_recommendations\n      - tiering_suggestions\n\n  - name: volume_anomaly_detection\n    trigger: real_time\n    ai_model: anomaly_detection\n    monitoring:\n      - performance_metrics\n      - usage_patterns\n      - error_rates\n    actions:\n      - immediate_alerting\n      - automatic_diagnostics\n      - preventive_measures\n</code></pre>"},{"location":"use-cases/volume-operations/#volume-performance-management","title":"Volume Performance Management","text":""},{"location":"use-cases/volume-operations/#quality-of-service-qos-configuration","title":"Quality of Service (QoS) Configuration","text":"<pre><code>qos_policies:\n  high_performance:\n    min_iops: 1000\n    max_iops: 10000\n    min_throughput: 100_mbps\n    max_throughput: 1_gbps\n    latency_target: 2_ms\n\n  standard_performance:\n    min_iops: 100\n    max_iops: 5000\n    min_throughput: 10_mbps\n    max_throughput: 500_mbps\n    latency_target: 10_ms\n\n  basic_performance:\n    min_iops: 10\n    max_iops: 1000\n    min_throughput: 1_mbps\n    max_throughput: 100_mbps\n    latency_target: 50_ms\n</code></pre>"},{"location":"use-cases/volume-operations/#performance-monitoring-and-alerting","title":"Performance Monitoring and Alerting","text":"<pre><code>class VolumePerformanceMonitor:\n    async def monitor_volume_performance(self, volume_id: str):\n        \"\"\"Monitor comprehensive volume performance metrics\"\"\"\n        monitoring_request = {\n            \"workflow\": \"volume_performance_monitoring\",\n            \"parameters\": {\n                \"volume_id\": volume_id,\n                \"metrics\": [\"iops\", \"throughput\", \"latency\", \"queue_depth\"],\n                \"sampling_frequency\": \"1_minute\",\n                \"alert_thresholds\": {\n                    \"latency_warning\": \"10_ms\",\n                    \"latency_critical\": \"50_ms\",\n                    \"iops_utilization\": \"90%\",\n                    \"throughput_utilization\": \"85%\"\n                }\n            }\n        }\n\n        response = await self.apim.execute_temporal_workflow(monitoring_request)\n        return response.performance_metrics\n\n    async def analyze_workload_patterns(self, volume_id: str, analysis_period: str = \"7_days\"):\n        \"\"\"Analyze volume workload patterns for optimization\"\"\"\n        analysis_request = {\n            \"workflow\": \"workload_pattern_analysis\",\n            \"parameters\": {\n                \"volume_id\": volume_id,\n                \"analysis_period\": analysis_period,\n                \"pattern_types\": [\"io_size\", \"access_patterns\", \"temporal_patterns\"],\n                \"correlation_analysis\": True,\n                \"optimization_suggestions\": True\n            }\n        }\n\n        return await self.apim.execute_temporal_workflow(analysis_request)\n</code></pre>"},{"location":"use-cases/volume-operations/#volume-data-protection-and-backup","title":"Volume Data Protection and Backup","text":""},{"location":"use-cases/volume-operations/#snapshot-management","title":"Snapshot Management","text":"<pre><code>snapshot_policies:\n  frequent_snapshots:\n    schedule: every_4_hours\n    retention: 24_snapshots\n    prefix: \"frequent\"\n\n  daily_snapshots:\n    schedule: daily_at_midnight\n    retention: 30_snapshots\n    prefix: \"daily\"\n\n  weekly_snapshots:\n    schedule: weekly_sunday\n    retention: 12_snapshots\n    prefix: \"weekly\"\n\n  monthly_snapshots:\n    schedule: monthly_first_sunday\n    retention: 12_snapshots\n    prefix: \"monthly\"\n</code></pre>"},{"location":"use-cases/volume-operations/#backup-and-recovery-operations","title":"Backup and Recovery Operations","text":"<pre><code>class VolumeBackupManager:\n    async def create_snapshot(self, volume_id: str, snapshot_name: str = None):\n        \"\"\"Create manual snapshot of volume\"\"\"\n        snapshot_request = {\n            \"workflow\": \"create_volume_snapshot\",\n            \"parameters\": {\n                \"volume_id\": volume_id,\n                \"snapshot_name\": snapshot_name or f\"manual_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n                \"validate_consistency\": True,\n                \"update_catalog\": True\n            }\n        }\n\n        return await self.apim.execute_temporal_workflow(snapshot_request)\n\n    async def restore_from_snapshot(self, volume_id: str, snapshot_name: str, restore_type: str = \"in_place\"):\n        \"\"\"Restore volume from snapshot\"\"\"\n        restore_request = {\n            \"workflow\": \"volume_snapshot_restore\",\n            \"parameters\": {\n                \"volume_id\": volume_id,\n                \"snapshot_name\": snapshot_name,\n                \"restore_type\": restore_type,  # in_place, clone, new_volume\n                \"validation_checks\": True,\n                \"backup_current_state\": True\n            }\n        }\n\n        return await self.apim.execute_temporal_workflow(restore_request)\n\n    async def setup_replication(self, source_volume: str, destination_cluster: str, policy: str = \"MirrorAndVault\"):\n        \"\"\"Setup SnapMirror replication for volume\"\"\"\n        replication_request = {\n            \"workflow\": \"setup_volume_replication\",\n            \"parameters\": {\n                \"source_volume\": source_volume,\n                \"destination_cluster\": destination_cluster,\n                \"policy\": policy,\n                \"schedule\": \"hourly\",\n                \"initialize_immediately\": True\n            }\n        }\n\n        return await self.apim.execute_temporal_workflow(replication_request)\n</code></pre>"},{"location":"use-cases/volume-operations/#volume-efficiency-and-optimization","title":"Volume Efficiency and Optimization","text":""},{"location":"use-cases/volume-operations/#storage-efficiency-features","title":"Storage Efficiency Features","text":"<pre><code>efficiency_policies:\n  maximum_efficiency:\n    deduplication: enabled\n    compression: enabled\n    compaction: enabled\n    temperature_sensitive_storage: enabled\n    inline_efficiency: true\n\n  balanced_efficiency:\n    deduplication: enabled\n    compression: enabled\n    compaction: disabled\n    temperature_sensitive_storage: disabled\n    inline_efficiency: false\n\n  performance_optimized:\n    deduplication: background_only\n    compression: disabled\n    compaction: disabled\n    temperature_sensitive_storage: disabled\n    inline_efficiency: false\n</code></pre>"},{"location":"use-cases/volume-operations/#automated-efficiency-management","title":"Automated Efficiency Management","text":"<pre><code>class VolumeEfficiencyManager:\n    async def optimize_volume_efficiency(self, volume_id: str):\n        \"\"\"Optimize volume storage efficiency\"\"\"\n        efficiency_request = {\n            \"workflow\": \"volume_efficiency_optimization\",\n            \"parameters\": {\n                \"volume_id\": volume_id,\n                \"efficiency_features\": [\"deduplication\", \"compression\", \"compaction\"],\n                \"performance_impact_threshold\": \"5%_latency_increase\",\n                \"space_savings_target\": \"20%\",\n                \"schedule_optimization\": \"low_usage_hours\"\n            }\n        }\n\n        return await self.apim.execute_temporal_workflow(efficiency_request)\n\n    async def analyze_efficiency_opportunities(self, cluster_id: str):\n        \"\"\"Analyze storage efficiency opportunities across volumes\"\"\"\n        analysis_request = {\n            \"workflow\": \"cluster_efficiency_analysis\",\n            \"parameters\": {\n                \"cluster_id\": cluster_id,\n                \"analysis_scope\": \"all_volumes\",\n                \"efficiency_metrics\": [\"dedup_ratio\", \"compression_ratio\", \"space_savings\"],\n                \"recommendations\": True,\n                \"cost_benefit_analysis\": True\n            }\n        }\n\n        return await self.apim.execute_temporal_workflow(analysis_request)\n</code></pre>"},{"location":"use-cases/volume-operations/#volume-best-practices","title":"Volume Best Practices","text":""},{"location":"use-cases/volume-operations/#1-volume-design-principles","title":"1. Volume Design Principles","text":"<ul> <li>Right-sizing: Provision volumes with appropriate initial sizes</li> <li>Performance Requirements: Match volume configuration to application needs</li> <li>Data Protection: Implement appropriate backup and replication strategies</li> <li>Efficiency Optimization: Enable storage efficiency features when appropriate</li> </ul>"},{"location":"use-cases/volume-operations/#2-performance-optimization","title":"2. Performance Optimization","text":"<ul> <li>QoS Policies: Implement Quality of Service policies for performance guarantees</li> <li>Aggregate Placement: Optimize volume placement across aggregates</li> <li>Workload Separation: Separate different workload types onto different volumes</li> <li>Cache Optimization: Leverage read and write caching effectively</li> </ul>"},{"location":"use-cases/volume-operations/#3-operational-excellence","title":"3. Operational Excellence","text":"<ul> <li>Monitoring and Alerting: Implement comprehensive volume monitoring</li> <li>Automation: Automate routine volume management tasks</li> <li>Documentation: Maintain accurate volume inventory and documentation</li> <li>Regular Reviews: Conduct regular volume performance and efficiency reviews</li> </ul>"},{"location":"use-cases/volume-operations/#troubleshooting-guide","title":"Troubleshooting Guide","text":""},{"location":"use-cases/volume-operations/#common-volume-issues","title":"Common Volume Issues","text":"<ol> <li>Performance Problems</li> <li>Check QoS policy limits and utilization</li> <li>Analyze workload patterns and access methods</li> <li>Review aggregate performance and capacity</li> <li> <p>Investigate network and client-side issues</p> </li> <li> <p>Capacity Issues</p> </li> <li>Monitor volume space utilization and growth</li> <li>Check snapshot space consumption</li> <li>Analyze storage efficiency opportunities</li> <li> <p>Review capacity planning and forecasts</p> </li> <li> <p>Data Protection Issues</p> </li> <li>Verify snapshot schedules and retention</li> <li>Check replication health and lag times</li> <li>Validate backup completion and integrity</li> <li>Test recovery procedures regularly</li> </ol>"},{"location":"use-cases/volume-operations/#volume-optimization-techniques","title":"Volume Optimization Techniques","text":"<ul> <li>Performance Tuning: Adjust QoS policies and aggregate placement</li> <li>Efficiency Optimization: Enable and tune deduplication and compression</li> <li>Capacity Management: Implement auto-grow and space monitoring</li> <li>Workload Optimization: Analyze and optimize application access patterns</li> </ul>"},{"location":"use-cases/volume-operations/#success-metrics","title":"Success Metrics","text":"<ul> <li>Volume Availability: Percentage uptime of storage volumes</li> <li>Performance SLA Compliance: Meeting volume performance requirements</li> <li>Storage Efficiency: Deduplication and compression space savings</li> <li>Backup Success Rate: Percentage of successful backup operations</li> <li>Recovery Time Objectives: Meeting RTO requirements for volume recovery</li> <li>Automation Success Rate: Percentage of successful automated volume operations</li> <li>Cost Efficiency: Cost per GB of storage with optimization benefits</li> </ul> <p>This comprehensive volume operations framework enables DevOps teams to efficiently manage storage volumes through automated provisioning, intelligent monitoring, AI-enhanced optimization, and robust data protection, ensuring optimal performance, efficiency, and reliability.</p>"}]}